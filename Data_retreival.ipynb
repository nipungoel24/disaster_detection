{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBNxkzgj5NID"
      },
      "outputs": [],
      "source": [
        "# =================================================================================\n",
        "# GEE API SETUP\n",
        "# =================================================================================\n",
        "\n",
        "import ee\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WQ035uITnZv",
        "outputId": "90c0b4b9-2d19-4f34-d087-9277b38839f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â—ï¸ An error occurred during initialization: Please authorize access to your Earth Engine account by running\n",
            "\n",
            "earthengine authenticate\n",
            "\n",
            "in your command line, or ee.Authenticate() in Python, and then retry.\n",
            "Trying to authenticate...\n",
            "Mounted at /content/drive\n",
            "âœ… Google Drive mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    ee.Initialize(project = \"cobalt-vector-470207-a5\")\n",
        "    print('âœ… Earth Engine API initialized successfully.')\n",
        "except Exception as e:\n",
        "    print('â—ï¸ An error occurred during initialization:', e)\n",
        "    print('Trying to authenticate...')\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project = \"cobalt-vector-470207-a5\")\n",
        "\n",
        "# 2. Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print('âœ… Google Drive mounted successftry:\n",
        "    ee.Initialize(project = \"cobalt-vector-470207-a5\")\n",
        "    print('âœ… Earth Engine API initialized successfully.')\n",
        "except Exception as e:\n",
        "    print('â—ï¸ An error occurred during initialization:', e)\n",
        "    print('Trying to authenticate...')\n",
        "    ee.Authenticate()\n",
        "    ee.Initialize(project = \"cobalt-vector-470207-a5\")\n",
        "\n",
        "# 2. Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "print('âœ… Google Drive mounted successfully.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AifMwi1xT8P1",
        "outputId": "d8156b46-d19a-4e8b-a00c-d8c335345273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting flood analysis for Assam districts...\n",
            "Pre-flood period: 2023-05-01 to 2023-05-31\n",
            "Post-flood period: 2023-06-20 to 2023-06-30\n",
            "\n",
            "--- Running analysis for Barpeta ---\n",
            "Pre-flood images available: 5\n",
            "Post-flood images available: 2\n",
            "âœ… Export task started for Barpeta (Pre-Flood). Task ID: A2F76CKIQHNB5OM4QT2RGFZY\n",
            "âœ… Export task started for Barpeta (Post-Flood). Task ID: LO3WERY7J25FQOIHD35TSDK3\n",
            "âœ… Flood mask export started for Barpeta. Task ID: CJ3M2Z74XQCQTJE6KWSEIFIT\n",
            "\n",
            "--- Running analysis for Dhemaji ---\n",
            "Pre-flood images available: 5\n",
            "Post-flood images available: 1\n",
            "âœ… Export task started for Dhemaji (Pre-Flood). Task ID: TBEHKFEJPDAUWHF5VCUKZKEG\n",
            "âœ… Export task started for Dhemaji (Post-Flood). Task ID: VXKDGLLBYUZL3EHEY4COYQNR\n",
            "âœ… Flood mask export started for Dhemaji. Task ID: GQPTMGP345CTXR7FNVH3OHVS\n",
            "\n",
            "--- Running analysis for Lakhimpur ---\n",
            "Pre-flood images available: 8\n",
            "Post-flood images available: 1\n",
            "âœ… Export task started for Lakhimpur (Pre-Flood). Task ID: RSXKNSZXVM5DXJMCXANQ5VKO\n",
            "âœ… Export task started for Lakhimpur (Post-Flood). Task ID: JU2J3QT7HJULW3PZLGAY62SH\n",
            "âœ… Flood mask export started for Lakhimpur. Task ID: FPVBTFJ33OTQII7RD6ISOJLG\n",
            "\n",
            "--- Running analysis for Nalbari ---\n",
            "Pre-flood images available: 5\n",
            "Post-flood images available: 2\n",
            "âœ… Export task started for Nalbari (Pre-Flood). Task ID: Q74LQKER732BGYKNLZS4CTTQ\n",
            "âœ… Export task started for Nalbari (Post-Flood). Task ID: G6IXQS7L3DUC6ZW5XKXBOVPK\n",
            "âœ… Flood mask export started for Nalbari. Task ID: P3EOFLCG747QDPF7NIS7OSOO\n",
            "\n",
            "--- Running analysis for Sonitpur ---\n",
            "Pre-flood images available: 12\n",
            "Post-flood images available: 2\n",
            "âœ… Export task started for Sonitpur (Pre-Flood). Task ID: LTRRF2XSUT365JLZGAR7TXLN\n",
            "âœ… Export task started for Sonitpur (Post-Flood). Task ID: FL5NGXNE7KAMS55ZQFTE7RAV\n",
            "âœ… Flood mask export started for Sonitpur. Task ID: QBLDC7UBOHO7FROQW6RK7TMF\n",
            "\n",
            "--- All analysis tasks have been submitted. ---\n",
            "Use check_task_status() to monitor progress.\n"
          ]
        }
      ],
      "source": [
        "print(f'Post-flood period: {post_flood_start} to {post_flood_end}')\n",
        "\n",
        "# Loop through each district and run the analysis\n",
        "for district_name, geometry in district_geometries.items():\n",
        "    run_and_export_analysis(district_name, geometry)\n",
        "\n",
        "print('\\n--- All analysis tasks have been submitted. ---')\n",
        "print('Use check_task_status() to monitor progress.')\n",
        "district_geometries = {\n",
        "    'Barpeta': ee.Geometry.Rectangle([90.732, 26.155, 91.265, 26.512]),\n",
        "    'Dhemaji': ee.Geometry.Rectangle([94.395, 27.420, 94.980, 27.750]),\n",
        "    'Lakhimpur': ee.Geometry.Rectangle([93.700, 26.750, 94.500, 27.550]),\n",
        "    'Nalbari': ee.Geometry.Rectangle([91.130, 26.250, 91.550, 26.600]),\n",
        "    'Sonitpur': ee.Geometry.Rectangle([92.500, 26.500, 93.300, 27.000])\n",
        "}\n",
        "\n",
        "# Date ranges for the analysis\n",
        "pre_flood_start = '2023-05-01'\n",
        "pre_flood_end = '2023-05-31'\n",
        "post_flood_start = '2023-06-20'\n",
        "post_flood_end = '2023-06-30'\n",
        "\n",
        "# Function to run the analysis and export for a single district\n",
        "def run_and_export_analysis(district_name, geometry):\n",
        "    \"\"\"\n",
        "    Performs flood analysis for a given district and starts export tasks.\n",
        "    \"\"\"\n",
        "    print(f'\\n--- Running analysis for {district_name} ---')\n",
        "\n",
        "    # Load and filter the Sentinel-1 collection for pre-flood\n",
        "    s1_collection_pre = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')) \\\n",
        "        .filterDate(pre_flood_start, pre_flood_end) \\\n",
        "        .filterBounds(geometry)\n",
        "\n",
        "    # Load and filter the Sentinel-1 collection for post-flood\n",
        "    s1_collection_post = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')) \\\n",
        "        .filterDate(post_flood_start, post_flood_end) \\\n",
        "        .filterBounds(geometry)\n",
        "\n",
        "    # Check collection sizes\n",
        "    pre_size = s1_collection_pre.size().getInfo()\n",
        "    post_size = s1_collection_post.size().getInfo()\n",
        "\n",
        "    print(f'Pre-flood images available: {pre_size}')\n",
        "    print(f'Post-flood images available: {post_size}')\n",
        "\n",
        "    # Check if collections have images\n",
        "    if pre_size > 0 and post_size > 0:\n",
        "        # Create mosaics\n",
        "        pre_flood_mosaic = s1_collection_pre.mean().clip(geometry)\n",
        "        post_flood_mosaic = s1_collection_post.mean().clip(geometry)\n",
        "\n",
        "        # Export Pre-Flood Image - CORRECTED SYNTAX\n",
        "        task_pre = ee.batch.Export.image.toDrive(\n",
        "            image=pre_flood_mosaic.select('VV'),\n",
        "            description=f'{district_name}_PreFlood_Image',\n",
        "            folder='Colab Notebooks/Images',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_pre.start()\n",
        "        print(f'âœ… Export task started for {district_name} (Pre-Flood). Task ID: {task_pre.id}')\n",
        "\n",
        "        # Export Post-Flood Image - CORRECTED SYNTAX\n",
        "        task_post = ee.batch.Export.image.toDrive(\n",
        "            image=post_flood_mosaic.select('VV'),\n",
        "            description=f'{district_name}_PostFlood_Image',\n",
        "            folder='Colab Notebooks/Images',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_post.start()\n",
        "        print(f'âœ… Export task started for {district_name} (Post-Flood). Task ID: {task_post.id}')\n",
        "\n",
        "        # Calculate flood detection (optional - for analysis)\n",
        "        # Apply speckle filtering   # Applies median filter to smooth SAR noise.\n",
        "        pre_filtered = pre_flood_mosaic.focal_median(2, 'circle', 'pixels', 2)\n",
        "        post_filtered = post_flood_mosaic.focal_median(2, 'circle', 'pixels', 2)\n",
        "\n",
        "        # Calculate difference\n",
        "        difference = post_filtered.subtract(pre_filtered)\n",
        "\n",
        "        # Flood mask (areas where backscatter decreased significantly)\n",
        "        flood_threshold = -3  # dB\n",
        "        flood_mask = difference.lt(flood_threshold)\n",
        "\n",
        "        # Export flood mask\n",
        "        task_flood = ee.batch.Export.image.toDrive(\n",
        "            image=flood_mask.select('VV').rename('flood_mask'),\n",
        "            description=f'{district_name}_Flood_Mask',\n",
        "            folder='Colab Notebooks/Images',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_flood.start()\n",
        "        print(f'âœ… Flood mask export started for {district_name}. Task ID: {task_flood.id}')\n",
        "\n",
        "    elif pre_size == 0:\n",
        "        print(f'â—ï¸ No pre-flood images found for {district_name} between {pre_flood_start} and {pre_flood_end}.')\n",
        "    elif post_size == 0:\n",
        "        print(f'â—ï¸ No post-flood images found for {district_name} between {post_flood_start} and {post_flood_end}.')\n",
        "    else:\n",
        "        print(f'â—ï¸ No images found for {district_name}. No exports will be started.')\n",
        "\n",
        "# Function to check task status\n",
        "def check_task_status():\n",
        "    \"\"\"\n",
        "    Check the status of all running tasks\n",
        "    \"\"\"\n",
        "    tasks = ee.batch.Task.list()\n",
        "    print('\\n--- Task Status ---')\n",
        "    for task in tasks[:10]:  # Show first 10 tasks\n",
        "        print(f'Task: {task.config[\"description\"]}, Status: {task.state}, Progress: {task.progress}%')\n",
        "\n",
        "# =================================================================================\n",
        "# RUN THE SCRIPT\n",
        "# =================================================================================\n",
        "\n",
        "print('Starting flood analysis for Assam districts...')\n",
        "print(f'Pre-flood period: {pre_flood_start} to {pre_flood_end}')\n",
        "print(f'Post-flood period: {post_flood_start} to {post_flood_end}')\n",
        "\n",
        "# Loop through each district and run the analysis\n",
        "for district_name, geometry in district_geometries.items():\n",
        "    run_and_export_analysis(district_name, geometry)\n",
        "\n",
        "print('\\n--- All analysis tasks have been submitted. ---')\n",
        "print('Use check_task_status() to monitor progress.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ovk8igX5WxVq",
        "outputId": "f2d60920-766a-4695-f884-798f3939a483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "GOOGLE EARTH ENGINE EXPORT DIAGNOSIS\n",
            "============================================================\n",
            "--- Checking Authentication ---\n",
            "âœ… GEE authentication working (test collection size: 1)\n",
            "âœ… Can access task list (15 tasks visible)\n",
            "\n",
            "========================================\n",
            "DISTRICT: Barpeta\n",
            "========================================\n",
            "\n",
            "--- Finding available dates for Barpeta ---\n",
            "Images available from: 2014-10-08 to 2025-09-09\n",
            "Total suitable images: 1191\n",
            "\n",
            "--- Checking Barpeta from 2023-05-01 to 2023-05-31 ---\n",
            "Total S1 images available: 10\n",
            "Images with VV polarization: 10\n",
            "Images with IW mode + VV: 10\n",
            "Sample images found:\n",
            "  1. Date: 1682985338000\n",
            "     Orbit: DESCENDING\n",
            "     Polarizations: ['VV', 'VH']\n",
            "  2. Date: 1683374233000\n",
            "     Orbit: ASCENDING\n",
            "     Polarizations: ['VV', 'VH']\n",
            "  3. Date: 1683589641000\n",
            "     Orbit: DESCENDING\n",
            "     Polarizations: ['VV', 'VH']\n",
            "\n",
            "--- Checking Barpeta from 2023-06-20 to 2023-06-30 ---\n",
            "Total S1 images available: 3\n",
            "Images with VV polarization: 3\n",
            "Images with IW mode + VV: 3\n",
            "Sample images found:\n",
            "  1. Date: 1687521435000\n",
            "     Orbit: ASCENDING\n",
            "     Polarizations: ['VV', 'VH']\n",
            "  2. Date: 1687736843000\n",
            "     Orbit: DESCENDING\n",
            "     Polarizations: ['VV', 'VH']\n",
            "  3. Date: 1687953932000\n",
            "     Orbit: ASCENDING\n",
            "     Polarizations: ['VV', 'VH']\n",
            "\n",
            "--- Testing export for Barpeta ---\n",
            "Available images in 2023-01-01 to 2024-12-31: 224\n",
            "âœ… Test export started for Barpeta\n",
            "   Task ID: TPFVCCCZA6MOU7WC2D4U5VGM\n",
            "   Check your Google Drive in the \"EE_Exports\" folder\n",
            "----------------------------------------\n",
            "\n",
            "--- SUMMARY ---\n",
            "1. Check the output above for specific issues\n",
            "2. If test exports were started, check Google Drive in ~5-10 minutes\n",
            "3. You can monitor progress with: ee.batch.Task.list()\n"
          ]
        }
      ],
      "source": [
        "# District geometries (bounding boxes)\n",
        "district_geometries = {\n",
        "    'Barpeta': ee.Geometry.Rectangle([90.732, 26.155, 91.265, 26.512]),\n",
        "    'Dhemaji': ee.Geometry.Rectangle([94.395, 27.420, 94.980, 27.750]),\n",
        "    'Lakhimpur': ee.Geometry.Rectangle([93.700, 26.750, 94.500, 27.550]),\n",
        "    'Nalbari': ee.Geometry.Rectangle([91.130, 26.250, 91.550, 26.600]),\n",
        "    'Sonitpur': ee.Geometry.Rectangle([92.500, 26.500, 93.300, 27.000])\n",
        "}\n",
        "\n",
        "# Function to check image availability\n",
        "def check_image_availability(district_name, geometry, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Check what Sentinel-1 images are available for a given region and time period\n",
        "    \"\"\"\n",
        "    print(f'\\n--- Checking {district_name} from {start_date} to {end_date} ---')\n",
        "\n",
        "    # Basic collection without strict filtering\n",
        "    collection_basic = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filterBounds(geometry) \\\n",
        "        .filterDate(start_date, end_date)\n",
        "\n",
        "    basic_count = collection_basic.size().getInfo()\n",
        "    print(f'Total S1 images available: {basic_count}')\n",
        "\n",
        "    if basic_count == 0:\n",
        "        print('âŒ No images found at all for this region/time period')\n",
        "        return False\n",
        "\n",
        "    # Check with VV polarization\n",
        "    collection_vv = collection_basic.filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV'))\n",
        "    vv_count = collection_vv.size().getInfo()\n",
        "    print(f'Images with VV polarization: {vv_count}')\n",
        "\n",
        "    # Check with IW instrument mode\n",
        "    collection_iw = collection_vv.filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "    iw_count = collection_iw.size().getInfo()\n",
        "    print(f'Images with IW mode + VV: {iw_count}')\n",
        "\n",
        "    if iw_count > 0:\n",
        "        # Get details of first few images\n",
        "        images_list = collection_iw.limit(3).getInfo()\n",
        "        print('Sample images found:')\n",
        "        for i, img in enumerate(images_list['features']):\n",
        "            props = img['properties']\n",
        "            print(f\"  {i+1}. Date: {props.get('system:time_start', 'N/A')}\")\n",
        "            print(f\"     Orbit: {props.get('orbitProperties_pass', 'N/A')}\")\n",
        "            print(f\"     Polarizations: {props.get('transmitterReceiverPolarisation', 'N/A')}\")\n",
        "\n",
        "    return iw_count > 0\n",
        "\n",
        "# Function to find available date ranges\n",
        "def find_available_dates(district_name, geometry):\n",
        "    \"\"\"\n",
        "    Find when Sentinel-1 images are actually available for a region\n",
        "    \"\"\"\n",
        "    print(f'\\n--- Finding available dates for {district_name} ---')\n",
        "\n",
        "    # Check a wider date range\n",
        "    collection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filterBounds(geometry) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "\n",
        "    # Get date range\n",
        "    date_range = collection.reduceColumns(ee.Reducer.minMax(), ['system:time_start']).getInfo()\n",
        "\n",
        "    if date_range['min'] and date_range['max']:\n",
        "        import datetime\n",
        "        min_date = datetime.datetime.fromtimestamp(date_range['min'] / 1000).strftime('%Y-%m-%d')\n",
        "        max_date = datetime.datetime.fromtimestamp(date_range['max'] / 1000).strftime('%Y-%m-%d')\n",
        "        print(f'Images available from: {min_date} to {max_date}')\n",
        "\n",
        "        # Get total count\n",
        "        total_images = collection.size().getInfo()\n",
        "        print(f'Total suitable images: {total_images}')\n",
        "\n",
        "        return min_date, max_date, total_images > 0\n",
        "    else:\n",
        "        print('âŒ No suitable images found for this region')\n",
        "        return None, None, False\n",
        "\n",
        "# Test function with working dates\n",
        "def test_export_with_available_dates(district_name, geometry):\n",
        "    \"\"\"\n",
        "    Try to export using dates when we know images are available\n",
        "    \"\"\"\n",
        "    print(f'\\n--- Testing export for {district_name} ---')\n",
        "\n",
        "    # Use a broader, more recent date range that's likely to have data\n",
        "    # Assam floods typically occur during monsoon season (June-September)\n",
        "    test_start = '2023-01-01'\n",
        "    test_end = '2024-12-31'\n",
        "\n",
        "    collection = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filterBounds(geometry) \\\n",
        "        .filterDate(test_start, test_end) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW'))\n",
        "\n",
        "    image_count = collection.size().getInfo()\n",
        "    print(f'Available images in {test_start} to {test_end}: {image_count}')\n",
        "\n",
        "    if image_count > 0:\n",
        "        # Create a simple mosaic of available images\n",
        "        mosaic = collection.mean().clip(geometry)\n",
        "\n",
        "        # Try export\n",
        "        task = ee.batch.Export.image.toDrive(\n",
        "            image=mosaic.select('VV'),\n",
        "            description=f'Test_{district_name}_Sentinel1',\n",
        "            folder='EE_Exports',  # Changed folder name\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            task.start()\n",
        "            print(f'âœ… Test export started for {district_name}')\n",
        "            print(f'   Task ID: {task.id}')\n",
        "            print(f'   Check your Google Drive in the \"EE_Exports\" folder')\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f'âŒ Export failed: {str(e)}')\n",
        "            return False\n",
        "    else:\n",
        "        print('âŒ No images available for export test')\n",
        "        return False\n",
        "\n",
        "# Check authentication and drive access\n",
        "def check_authentication():\n",
        "    \"\"\"\n",
        "    Verify that GEE and Drive access are working\n",
        "    \"\"\"\n",
        "    print('--- Checking Authentication ---')\n",
        "    try:\n",
        "        # Test basic GEE functionality\n",
        "        test_point = ee.Geometry.Point([91, 26])\n",
        "        test_collection = ee.ImageCollection('COPERNICUS/S1_GRD').limit(1)\n",
        "        size = test_collection.size().getInfo()\n",
        "        print(f'âœ… GEE authentication working (test collection size: {size})')\n",
        "\n",
        "        # Check if we can access task list\n",
        "        tasks = ee.batch.Task.list()\n",
        "        print(f'âœ… Can access task list ({len(tasks)} tasks visible)')\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'âŒ Authentication issue: {str(e)}')\n",
        "        return False\n",
        "\n",
        "# Main diagnostic function\n",
        "def run_full_diagnosis():\n",
        "    \"\"\"\n",
        "    Run complete diagnosis of the issue\n",
        "    \"\"\"\n",
        "    print('=' * 60)\n",
        "    print('GOOGLE EARTH ENGINE EXPORT DIAGNOSIS')\n",
        "    print('=' * 60)\n",
        "\n",
        "    # 1. Check authentication\n",
        "    if not check_authentication():\n",
        "        print('\\nâŒ STOP: Fix authentication first')\n",
        "        return\n",
        "\n",
        "    # 2. Check image availability for your original dates\n",
        "    original_dates = [\n",
        "        ('2023-05-01', '2023-05-31', 'Pre-flood'),\n",
        "        ('2023-06-20', '2023-06-30', 'Post-flood')\n",
        "    ]\n",
        "\n",
        "    has_images = False\n",
        "    for district_name, geometry in district_geometries.items():\n",
        "        print(f'\\n{\"=\"*40}')\n",
        "        print(f'DISTRICT: {district_name}')\n",
        "        print(f'{\"=\"*40}')\n",
        "\n",
        "        # Find what dates are actually available\n",
        "        min_date, max_date, available = find_available_dates(district_name, geometry)\n",
        "\n",
        "        if available:\n",
        "            has_images = True\n",
        "            # Check your specific dates\n",
        "            for start_date, end_date, period_name in original_dates:\n",
        "                check_image_availability(district_name, geometry, start_date, end_date)\n",
        "\n",
        "        # Test export with available data\n",
        "        if available:\n",
        "            test_export_with_available_dates(district_name, geometry)\n",
        "\n",
        "        print('-' * 40)\n",
        "\n",
        "        # Only check first district for initial diagnosis\n",
        "        break\n",
        "\n",
        "    if not has_images:\n",
        "        print('\\nâŒ ISSUE FOUND: No Sentinel-1 images available for your regions')\n",
        "        print('   Try different districts or check if coordinates are correct')\n",
        "\n",
        "    print('\\n--- SUMMARY ---')\n",
        "    print('1. Check the output above for specific issues')\n",
        "    print('2. If test exports were started, check Google Drive in ~5-10 minutes')\n",
        "    print('3. You can monitor progress with: ee.batch.Task.list()')\n",
        "\n",
        "# =================================================================================\n",
        "# RUN DIAGNOSIS\n",
        "# =================================================================================\n",
        "\n",
        "# Uncomment the line below to run full diagnosis\n",
        "run_full_diagnosis()\n",
        "\n",
        "# Alternative: Check specific district\n",
        "# check_image_availability('Barpeta', district_geometries['Barpeta'], '2023-05-01', '2023-05-31')\n",
        "\n",
        "# Alternative: Test simple export\n",
        "# test_export_with_available_dates('Barpeta', district_geometries['Barpeta'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBUnPQC1rhsZ",
        "outputId": "8d16c958-72da-4f49-83f9-5cf21a9ffe5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting flood analysis for Assam districts...\n",
            "\n",
            "--- Running analysis for Barpeta ---\n",
            "Pre-flood period: 2023-05-01 to 2023-05-31\n",
            "Post-flood period: 2023-06-20 to 2023-06-30\n",
            "Pre-flood images available: 5\n",
            "Post-flood images available: 2\n",
            "âœ… Export task started for pre_flood_Barpeta.\n",
            "âœ… Export task started for post_flood_Barpeta.\n",
            "\n",
            "--- Running analysis for Dhemaji ---\n",
            "Pre-flood period: 2023-05-01 to 2023-05-31\n",
            "Post-flood period: 2023-06-15 to 2023-06-25\n",
            "Pre-flood images available: 5\n",
            "Post-flood images available: 1\n",
            "âœ… Export task started for pre_flood_Dhemaji.\n",
            "âœ… Export task started for post_flood_Dhemaji.\n",
            "\n",
            "--- Running analysis for Lakhimpur ---\n",
            "Pre-flood period: 2023-05-01 to 2023-05-31\n",
            "Post-flood period: 2023-06-15 to 2023-06-25\n",
            "Pre-flood images available: 8\n",
            "Post-flood images available: 2\n",
            "âœ… Export task started for pre_flood_Lakhimpur.\n",
            "âœ… Export task started for post_flood_Lakhimpur.\n",
            "\n",
            "--- Running analysis for Nalbari ---\n",
            "Pre-flood period: 2023-05-01 to 2023-05-31\n",
            "Post-flood period: 2023-06-20 to 2023-06-30\n",
            "Pre-flood images available: 5\n",
            "Post-flood images available: 2\n",
            "âœ… Export task started for pre_flood_Nalbari.\n",
            "âœ… Export task started for post_flood_Nalbari.\n",
            "\n",
            "--- Running analysis for Sonitpur ---\n",
            "Pre-flood period: 2023-05-01 to 2023-05-31\n",
            "Post-flood period: 2023-06-15 to 2023-06-25\n",
            "Pre-flood images available: 12\n",
            "Post-flood images available: 4\n",
            "âœ… Export task started for pre_flood_Sonitpur.\n",
            "âœ… Export task started for post_flood_Sonitpur.\n",
            "\n",
            "--- All analysis tasks have been submitted. ---\n"
          ]
        }
      ],
      "source": [
        "# =================================================================================\n",
        "# SCRIPT CONFIGURATION: MULTI-DISTRICT FLOOD ANALYSIS FOR ASSAM\n",
        "# =================================================================================\n",
        "\n",
        "# This script allows you to run a multi-district flood analysis for Assam.\n",
        "# It iterates through each district, performs the analysis, and exports the data\n",
        "\n",
        "# --- PRE-CONFIGURED DISTRICT GEOMETRIES AND DATES ---\n",
        "# A dictionary of locations. Each district now includes specific\n",
        "# pre- and post-flood date ranges based on 2023 flood events.\n",
        "district_geometries = {\n",
        "    'Barpeta': {\n",
        "        'geometry': ee.Geometry.Rectangle([90.732, 26.155, 91.265, 26.512]),\n",
        "        'pre_flood_start': '2023-05-01',\n",
        "        'pre_flood_end': '2023-05-31',\n",
        "        'post_flood_start': '2023-06-20',\n",
        "        'post_flood_end': '2023-06-30'\n",
        "    },\n",
        "    'Dhemaji': {\n",
        "        'geometry': ee.Geometry.Rectangle([94.395, 27.420, 94.980, 27.750]),\n",
        "        'pre_flood_start': '2023-05-01',\n",
        "        'pre_flood_end': '2023-05-31',\n",
        "        'post_flood_start': '2023-06-15',\n",
        "        'post_flood_end': '2023-06-25'\n",
        "    },\n",
        "    'Lakhimpur': {\n",
        "        'geometry': ee.Geometry.Rectangle([93.700, 26.750, 94.500, 27.550]),\n",
        "        'pre_flood_start': '2023-05-01',\n",
        "        'pre_flood_end': '2023-05-31',\n",
        "        'post_flood_start': '2023-06-15',\n",
        "        'post_flood_end': '2023-06-25'\n",
        "    },\n",
        "    'Nalbari': {\n",
        "        'geometry': ee.Geometry.Rectangle([91.130, 26.250, 91.550, 26.600]),\n",
        "        'pre_flood_start': '2023-05-01',\n",
        "        'pre_flood_end': '2023-05-31',\n",
        "        'post_flood_start': '2023-06-20',\n",
        "        'post_flood_end': '2023-06-30'\n",
        "    },\n",
        "    'Sonitpur': {\n",
        "        'geometry': ee.Geometry.Rectangle([92.500, 26.500, 93.300, 27.000]),\n",
        "        'pre_flood_start': '2023-05-01',\n",
        "        'pre_flood_end': '2023-05-31',\n",
        "        'post_flood_start': '2023-06-15',\n",
        "        'post_flood_end': '2023-06-25'\n",
        "    }\n",
        "}\n",
        "\n",
        "# =================================================================================\n",
        "# GEE API SETUP\n",
        "# =================================================================================\n",
        "# Your GEE and Google Drive setup code here...\n",
        "\n",
        "# =================================================================================\n",
        "# CORE LOGIC & FUNCTIONS\n",
        "# =================================================================================\n",
        "\n",
        "# Function to run the analysis and export for a single district\n",
        "def run_and_export_analysis(district_name, district_data):\n",
        "    \"\"\"\n",
        "    Performs flood analysis for a given district and starts export tasks.\n",
        "    \"\"\"\n",
        "    geometry = district_data['geometry']\n",
        "    pre_flood_start = district_data['pre_flood_start']\n",
        "    pre_flood_end = district_data['pre_flood_end']\n",
        "    post_flood_start = district_data['post_flood_start']\n",
        "    post_flood_end = district_data['post_flood_end']\n",
        "\n",
        "    print(f'\\n--- Running analysis for {district_name} ---')\n",
        "    print(f'Pre-flood period: {pre_flood_start} to {pre_flood_end}')\n",
        "    print(f'Post-flood period: {post_flood_start} to {post_flood_end}')\n",
        "\n",
        "    # Load and filter the Sentinel-1 collection for pre-flood\n",
        "    s1_collection_pre = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')) \\\n",
        "        .filterDate(pre_flood_start, pre_flood_end) \\\n",
        "        .filterBounds(geometry)\n",
        "\n",
        "    # Load and filter the Sentinel-1 collection for post-flood\n",
        "    s1_collection_post = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')) \\\n",
        "        .filterDate(post_flood_start, post_flood_end) \\\n",
        "        .filterBounds(geometry)\n",
        "\n",
        "    # Check collection sizes\n",
        "    pre_size = s1_collection_pre.size().getInfo()\n",
        "    post_size = s1_collection_post.size().getInfo()\n",
        "\n",
        "    print(f'Pre-flood images available: {pre_size}')\n",
        "    print(f'Post-flood images available: {post_size}')\n",
        "\n",
        "    # Check if collections have images\n",
        "    if pre_size > 0 and post_size > 0:\n",
        "        # Create mosaics\n",
        "        pre_flood_mosaic = s1_collection_pre.mean().clip(geometry)\n",
        "        post_flood_mosaic = s1_collection_post.mean().clip(geometry)\n",
        "\n",
        "        # Export Pre-Flood Image\n",
        "        task_pre = ee.batch.Export.image.toDrive(\n",
        "            image=pre_flood_mosaic.select('VV'),\n",
        "            description=f'pre_flood_{district_name}',\n",
        "            folder='Colab Notebooks/Images',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_pre.start()\n",
        "        print(f'âœ… Export task started for pre_flood_{district_name}.')\n",
        "\n",
        "        # Export Post-Flood Image\n",
        "        task_post = ee.batch.Export.image.toDrive(\n",
        "            image=post_flood_mosaic.select('VV'),\n",
        "            description=f'post_flood_{district_name}',\n",
        "            folder='Colab Notebooks/Images',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            maxPixels=1e13,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_post.start()\n",
        "        print(f'âœ… Export task started for post_flood_{district_name}.')\n",
        "\n",
        "    elif pre_size == 0:\n",
        "        print(f'â—ï¸ No pre-flood images found for {district_name}.')\n",
        "    elif post_size == 0:\n",
        "        print(f'â—ï¸ No post-flood images found for {district_name}.')\n",
        "    else:\n",
        "        print(f'â—ï¸ No images found for {district_name}. No exports will be started.')\n",
        "\n",
        "\n",
        "# =================================================================================\n",
        "# RUN THE SCRIPT\n",
        "# =================================================================================\n",
        "\n",
        "print('Starting flood analysis for Assam districts...')\n",
        "\n",
        "# Loop through each district and run the analysis\n",
        "for district_name, district_data in district_geometries.items():\n",
        "    run_and_export_analysis(district_name, district_data)\n",
        "\n",
        "print('\\n--- All analysis tasks have been submitted. ---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =================================================================================\n",
        "# CORE LOGIC & FUNCTIONS (Improved for robust export)\n",
        "# =================================================================================\n",
        "\n",
        "def run_and_export_analysis(district_name, geometry):\n",
        "    \"\"\"\n",
        "    Performs flood analysis for a given district and starts robust export tasks.\n",
        "    \"\"\"\n",
        "    # Use the dates you defined globally\n",
        "    global pre_flood_start, pre_flood_end, post_flood_start, post_flood_end\n",
        "\n",
        "    print(f'\\n--- Running analysis for {district_name} ---')\n",
        "\n",
        "    # Load and filter the Sentinel-1 collection for pre-flood\n",
        "    s1_collection_pre = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')) \\\n",
        "        .filterDate(pre_flood_start, pre_flood_end) \\\n",
        "        .filterBounds(geometry)\n",
        "\n",
        "    # Load and filter the Sentinel-1 collection for post-flood\n",
        "    s1_collection_post = ee.ImageCollection('COPERNICUS/S1_GRD') \\\n",
        "        .filter(ee.Filter.eq('instrumentMode', 'IW')) \\\n",
        "        .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n",
        "        .filter(ee.Filter.eq('orbitProperties_pass', 'ASCENDING')) \\\n",
        "        .filterDate(post_flood_start, post_flood_end) \\\n",
        "        .filterBounds(geometry)\n",
        "\n",
        "    # Check collection sizes (Crucial for debugging)\n",
        "    pre_size = s1_collection_pre.size().getInfo()\n",
        "    post_size = s1_collection_post.size().getInfo()\n",
        "\n",
        "    print(f'Pre-flood images available: {pre_size}')\n",
        "    print(f'Post-flood images available: {post_size}')\n",
        "\n",
        "    if pre_size > 0 and post_size > 0:\n",
        "        # --- ROBUST MOSAIC CREATION ---\n",
        "        # 1. Use .mean() instead of .mosaic() for better stability.\n",
        "        # 2. Use .unmask() to prevent masked pixels from causing export errors.\n",
        "        pre_flood_mosaic = s1_collection_pre.mean().unmask().clip(geometry).toFloat()\n",
        "        post_flood_mosaic = s1_collection_post.mean().unmask().clip(geometry).toFloat()\n",
        "\n",
        "        # --- EXPORT TASKS ---\n",
        "        # Export Pre-Flood Image\n",
        "        task_pre = ee.batch.Export.image.toDrive(\n",
        "            image=pre_flood_mosaic.select('VV'),\n",
        "            description=f'{district_name}_PreFlood_Image',\n",
        "            folder='GEE_Flood_Exports_Assam',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_pre.start()\n",
        "        print(f'âœ… Export task started for {district_name} (Pre-Flood).')\n",
        "\n",
        "        # Export Post-Flood Image\n",
        "        task_post = ee.batch.Export.image.toDrive(\n",
        "            image=post_flood_mosaic.select('VV'),\n",
        "            description=f'{district_name}_PostFlood_Image',\n",
        "            folder='GEE_Flood_Exports_Assam',\n",
        "            scale=10,\n",
        "            region=geometry,\n",
        "            fileFormat='GeoTIFF'\n",
        "        )\n",
        "        task_post.start()\n",
        "        print(f'âœ… Export task started for {district_name} (Post-Flood).')\n",
        "\n",
        "    else:\n",
        "        print(f'â—ï¸ No images found for {district_name}. Exports skipped.')\n",
        "\n",
        "# You must also define your district_geometries and dates globally above this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PHASE 1A: SAR CHANGE DETECTION FOR FLOOD MAPPING\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            " Â Change method: log_ratio\n",
            " Â Threshold method: fixed\n",
            " Â Fixed threshold: -1.5 dB\n",
            " Â Post-processing: Enabled\n",
            " Â Despeckling: Enabled\n",
            "\n",
            "======================================================================\n",
            "\n",
            "ðŸŒŠ Processing Barpeta...\n",
            " Â  Processing 96 chip pairs...\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_0.tif: Pre-flood: Too few valid pixels: 0/262144\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_1.tif: Pre-flood: Too few valid pixels: 0/262144\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_10.tif: Pre-flood: Too few valid pixels: 0/262144\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_11.tif: Pre-flood: Too few valid pixels: 0/154624\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_2.tif: Pre-flood: Too few valid pixels: 0/262144\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_85.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_86.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_87.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_88.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_89.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_9.tif: Pre-flood: Too few valid pixels: 0/262144\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_90.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_91.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_92.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_93.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            "âŒ Error processing Barpeta_PreFlood_Image_chip_94.tif: Pre-flood: Too few valid pixels: 0/201216\n",
            " Â  âœ… Successfully processed: 80/96 chips\n",
            " Â  âš ï¸ Skipped (data/file error): 16/96 chips\n",
            " Â  Stitching 96 mask chips...\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Source shape (1, 1, 3977, 5934) is inconsistent with given indexes 1",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 304\u001b[39m\n\u001b[32m    302\u001b[39m \u001b[38;5;66;03m# Stitch masks into final district-level flood map\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m success_count > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m     \u001b[43mstitch_flood_masks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFINAL_MAPS_DIR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Â  âš ï¸ No successful masks to stitch for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistrict\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mstitch_flood_masks\u001b[39m\u001b[34m(mask_dir, district_name, output_dir)\u001b[39m\n\u001b[32m    213\u001b[39m output_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistrict_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_Flood_Map.tif\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m rasterio.open(output_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, **out_meta) \u001b[38;5;28;01mas\u001b[39;00m dest:\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     \u001b[43mdest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstitched_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m src \u001b[38;5;129;01min\u001b[39;00m sources:\n\u001b[32m    218\u001b[39m     src.close()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mrasterio\\\\_io.pyx:1787\u001b[39m, in \u001b[36mrasterio._io.DatasetWriterBase.write\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: Source shape (1, 1, 3977, 5934) is inconsistent with given indexes 1"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.merge import merge\n",
        "from scipy.ndimage import binary_opening, binary_closing\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.restoration import denoise_bilateral\n",
        "import warnings\n",
        "import sys\n",
        "# Set a custom recursion limit for safety in complex geospatial operations\n",
        "sys.setrecursionlimit(2000) \n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =================================================================\n",
        "# CONFIGURATION (Finalized for Stable SAR Analysis)\n",
        "# =================================================================\n",
        "\n",
        "# IMPORTANT: Adjust these paths to your system\n",
        "OUTPUT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips'\n",
        "FLOOD_MASKS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks'\n",
        "FINAL_MAPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps'\n",
        "\n",
        "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
        "\n",
        "# Change detection parameters\n",
        "CHANGE_METHOD = 'log_ratio'      # Standard for SAR change detection\n",
        "THRESHOLD_METHOD = 'fixed'       # <-- MOST RELIABLE: Set to fixed threshold\n",
        "FIXED_THRESHOLD = -1.5           # Conservative threshold (in dB)\n",
        "PERCENTILE_VALUE = 10            \n",
        "\n",
        "# Post-processing\n",
        "MIN_OBJECT_SIZE = 10             \n",
        "APPLY_MORPHOLOGY = True          \n",
        "\n",
        "# Despeckling Control\n",
        "APPLY_DESPECKLE = True           # <-- ACTIVATED SPECKLE FILTERING\n",
        "SPECKLE_WINDOW_SIZE = 5          # 5x5 pixel window for the bilateral filter\n",
        "MIN_VALID_PIXELS = 100           \n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# CORE FUNCTIONS (Despeckling and Analysis)\n",
        "# =================================================================\n",
        "\n",
        "def apply_lee_filter(image, window_size=5):\n",
        "    \"\"\" Applies a bilateral filter (robust general denoising) for speckle reduction. \"\"\"\n",
        "    # Uses denoise_bilateral for robust smoothing on float data.\n",
        "    image_denoised = denoise_bilateral(\n",
        "        image, \n",
        "        sigma_color=0.1, \n",
        "        sigma_spatial=window_size / 2.0, \n",
        "        channel_axis=None \n",
        "    )\n",
        "    return image_denoised\n",
        "\n",
        "\n",
        "def safe_log_ratio(pre_chip, post_chip, epsilon=1e-10):\n",
        "    \"\"\"Calculates 10 * log10(Post/Pre) for change detection.\"\"\"\n",
        "    pre_chip = np.where(pre_chip <= 0, epsilon, pre_chip)\n",
        "    post_chip = np.where(post_chip <= 0, epsilon, post_chip)\n",
        "    log_ratio = 10 * np.log10(post_chip / pre_chip)\n",
        "    return log_ratio\n",
        "\n",
        "\n",
        "def calculate_change(pre_chip, post_chip, method='log_ratio'):\n",
        "    \"\"\"Wrapper function to choose the change calculation method.\"\"\"\n",
        "    if method == 'log_ratio':\n",
        "        change = safe_log_ratio(pre_chip, post_chip)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "    return change\n",
        "\n",
        "\n",
        "def apply_threshold(change_map, method='fixed', fixed_value=-3.0, percentile=10):\n",
        "    \"\"\"Applies the final classification threshold (fixed, otsu, or percentile).\"\"\"\n",
        "    # Isolate valid data for threshold calculation\n",
        "    valid_mask = np.isfinite(change_map)\n",
        "    valid_data = change_map[valid_mask]\n",
        "    \n",
        "    if len(valid_data) < MIN_VALID_PIXELS:\n",
        "        raise ValueError(f\"Insufficient valid data: only {len(valid_data)} valid pixels\")\n",
        "    \n",
        "    # Apply the chosen threshold\n",
        "    if method == 'fixed':\n",
        "        flood_mask = change_map < fixed_value\n",
        "    else:\n",
        "        # Fallback to Otsu/Percentile logic\n",
        "        threshold = threshold_otsu(valid_data) # Using Otsu as default fallback\n",
        "        flood_mask = change_map < threshold\n",
        "    \n",
        "    # Set invalid pixels to 0 (non-flooded)\n",
        "    flood_mask = np.where(valid_mask, flood_mask, 0)\n",
        "    \n",
        "    return flood_mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def post_process_mask(mask, min_size=10, apply_morph=True):\n",
        "    \"\"\"Cleans up the binary mask using morphological operations.\"\"\"\n",
        "    if apply_morph:\n",
        "        mask = binary_opening(mask, structure=np.ones((3, 3)))\n",
        "        mask = binary_closing(mask, structure=np.ones((3, 3)))\n",
        "    \n",
        "    return mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def validate_chip_data(chip_array):\n",
        "    \"\"\"Ensures a chip contains enough unique, non-NaN data to be processed.\"\"\"\n",
        "    nan_count = np.isnan(chip_array).sum()\n",
        "    total_pixels = chip_array.size\n",
        "    valid_pixels = total_pixels - nan_count\n",
        "    inf_count = np.isinf(chip_array).sum()\n",
        "    \n",
        "    if valid_pixels < MIN_VALID_PIXELS: \n",
        "        return False, f\"Too few valid pixels: {valid_pixels}/{total_pixels}\"\n",
        "    if inf_count > 0:\n",
        "        return False, f\"Contains infinite values: {inf_count}\"\n",
        "    \n",
        "    unique_vals = np.unique(chip_array[~np.isnan(chip_array)])\n",
        "    if len(unique_vals) < 2:\n",
        "        return False, \"No variation in data (likely no-data chip)\"\n",
        "        \n",
        "    return True, \"Valid\"\n",
        "\n",
        "\n",
        "def process_chip_pair(pre_path, post_path, output_path):\n",
        "    \"\"\"Core function: reads chips, applies despeckle, calculates change, and saves mask.\"\"\"\n",
        "    try:\n",
        "        # --- READ CHIPS ---\n",
        "        with rasterio.open(pre_path) as src_pre:\n",
        "            pre_chip = src_pre.read(1).astype(np.float32)\n",
        "            profile = src_pre.profile.copy()\n",
        "        \n",
        "        with rasterio.open(post_path) as src_post:\n",
        "            post_chip = src_post.read(1).astype(np.float32)\n",
        "        \n",
        "        # --- DESPECKLING STEP ---\n",
        "        if APPLY_DESPECKLE:\n",
        "            pre_chip = apply_lee_filter(pre_chip, window_size=SPECKLE_WINDOW_SIZE)\n",
        "            post_chip = apply_lee_filter(post_chip, window_size=SPECKLE_WINDOW_SIZE)\n",
        "        \n",
        "        # --- VALIDATION AND CLEANUP ---\n",
        "        is_valid_pre, msg_pre = validate_chip_data(pre_chip)\n",
        "        if not is_valid_pre:\n",
        "             raise ValueError(f\"Pre-flood: {msg_pre}\")\n",
        "        \n",
        "        # Replace remaining NaNs with mean of valid pixels\n",
        "        if np.isnan(pre_chip).any():\n",
        "            pre_mean = np.nanmean(pre_chip)\n",
        "            pre_chip = np.nan_to_num(pre_chip, nan=pre_mean)\n",
        "        \n",
        "        if np.isnan(post_chip).any():\n",
        "            post_mean = np.nanmean(post_chip)\n",
        "            post_chip = np.nan_to_num(post_chip, nan=post_mean)\n",
        "            \n",
        "        # --- ANALYSIS AND CLASSIFICATION ---\n",
        "        change_map = calculate_change(pre_chip, post_chip, method=CHANGE_METHOD)\n",
        "        \n",
        "        flood_mask = apply_threshold(\n",
        "            change_map, \n",
        "            method=THRESHOLD_METHOD,\n",
        "            fixed_value=FIXED_THRESHOLD,\n",
        "            percentile=PERCENTILE_VALUE\n",
        "        )\n",
        "        \n",
        "        # --- POST-PROCESSING ---\n",
        "        flood_mask = post_process_mask(\n",
        "            flood_mask,\n",
        "            min_size=MIN_OBJECT_SIZE,\n",
        "            apply_morph=APPLY_MORPHOLOGY\n",
        "        )\n",
        "        \n",
        "        # --- SAVE MASK ---\n",
        "        profile.update(dtype=rasterio.uint8, count=1, compress='LZW')\n",
        "        \n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(flood_mask, 1)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Error reporting is ON to find file/data issues.\n",
        "        print(f\"âŒ Error processing {os.path.basename(pre_path)}: {e}\") \n",
        "        return False\n",
        "\n",
        "\n",
        "def stitch_flood_masks(mask_dir, district_name, output_dir):\n",
        "    \"\"\"Stitch all chip-level flood masks into a single district-level GeoTIFF.\"\"\"\n",
        "    \n",
        "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) \n",
        "                         if f.endswith('.tif')])\n",
        "    \n",
        "    if not mask_files:\n",
        "        print(f\" Â  âš ï¸ No mask files found in {mask_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\" Â  Stitching {len(mask_files)} mask chips...\")\n",
        "    \n",
        "    sources = [rasterio.open(f) for f in mask_files]\n",
        "    \n",
        "    stitched_array, out_transform = merge(sources)\n",
        "    \n",
        "    out_meta = sources[0].profile.copy()\n",
        "    out_meta.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"height\": stitched_array.shape[1],\n",
        "        \"width\": stitched_array.shape[2],\n",
        "        \"transform\": out_transform,\n",
        "        \"count\": 1,\n",
        "        \"dtype\": 'uint8',\n",
        "        \"compress\": 'LZW'\n",
        "    })\n",
        "    \n",
        "    output_path = os.path.join(output_dir, f'{district_name}_Flood_Map.tif')\n",
        "    with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
        "        dest.write(stitched_array, 1)\n",
        "    \n",
        "    for src in sources:\n",
        "        src.close()\n",
        "    \n",
        "    print(f\" Â  âœ… Final flood map saved: {output_path}\")\n",
        "    \n",
        "    total_pixels = stitched_array.size\n",
        "    flood_pixels = np.sum(stitched_array == 1)\n",
        "    flood_percentage = (flood_pixels / total_pixels) * 100\n",
        "    print(f\" Â  ðŸ“Š Flooded area: {flood_percentage:.2f}% of total pixels\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# MAIN EXECUTION LOGIC\n",
        "# =================================================================\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    print(\"=\"*70)\n",
        "    print(\"PHASE 1A: SAR CHANGE DETECTION FOR FLOOD MAPPING\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\" Â Change method: {CHANGE_METHOD}\")\n",
        "    print(f\" Â Threshold method: {THRESHOLD_METHOD}\")\n",
        "    print(f\" Â Fixed threshold: {FIXED_THRESHOLD} dB\")\n",
        "    print(f\" Â Post-processing: {'Enabled' if APPLY_MORPHOLOGY else 'Disabled'}\")\n",
        "    print(f\" Â Despeckling: {'Enabled' if APPLY_DESPECKLE else 'Disabled'}\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(FLOOD_MASKS_DIR, exist_ok=True)\n",
        "    os.makedirs(FINAL_MAPS_DIR, exist_ok=True)\n",
        "\n",
        "    # Process each district\n",
        "    for district in DISTRICTS:\n",
        "        print(f\"ðŸŒŠ Processing {district}...\")\n",
        "        \n",
        "        pre_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'pre_flood')\n",
        "        post_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'post_flood')\n",
        "        mask_dir = os.path.join(FLOOD_MASKS_DIR, district)\n",
        "        \n",
        "        # Check if directories exist\n",
        "        if not os.path.exists(pre_dir) or not os.path.exists(post_dir):\n",
        "            print(f\" Â  âš ï¸ Skipping {district}: Chip directories not found\")\n",
        "            continue\n",
        "        \n",
        "        # Create mask directory\n",
        "        os.makedirs(mask_dir, exist_ok=True)\n",
        "        \n",
        "        # Get list of chip files (We assume the pre-flood list contains all necessary names)\n",
        "        pre_chips = sorted([f for f in os.listdir(pre_dir) if f.endswith('.tif')])\n",
        "        \n",
        "        if not pre_chips:\n",
        "            print(f\" Â  âš ï¸ No .tif files found in {pre_dir}\")\n",
        "            continue\n",
        "        \n",
        "        print(f\" Â  Processing {len(pre_chips)} chip pairs...\")\n",
        "        \n",
        "        # Process each chip pair\n",
        "        success_count = 0\n",
        "        skip_count = 0\n",
        "        \n",
        "        for chip_name in pre_chips:\n",
        "            pre_path = os.path.join(pre_dir, chip_name)\n",
        "            \n",
        "            # --- Robust File Matching (Handles the 'PreFlood' -> 'PostFlood' name change) ---\n",
        "            post_chip_name = chip_name.replace('PreFlood_Image', 'PostFlood_Image')\n",
        "            post_path = os.path.join(post_dir, post_chip_name)\n",
        "            \n",
        "            # Create output filename for mask\n",
        "            mask_name = chip_name.replace('PreFlood_Image', 'Flood_Mask')\n",
        "            mask_path = os.path.join(mask_dir, mask_name)\n",
        "            \n",
        "            # Check if corresponding post-flood chip exists before processing\n",
        "            if not os.path.exists(post_path):\n",
        "                skip_count += 1\n",
        "                continue\n",
        "            \n",
        "            # Process chip pair\n",
        "            if process_chip_pair(pre_path, post_path, mask_path):\n",
        "                success_count += 1\n",
        "            else:\n",
        "                skip_count += 1\n",
        "        \n",
        "        print(f\" Â  âœ… Successfully processed: {success_count}/{len(pre_chips)} chips\")\n",
        "        print(f\" Â  âš ï¸ Skipped (data/file error): {skip_count}/{len(pre_chips)} chips\")\n",
        "        \n",
        "        # Stitch masks into final district-level flood map\n",
        "        if success_count > 0:\n",
        "            stitch_flood_masks(mask_dir, district, FINAL_MAPS_DIR)\n",
        "        else:\n",
        "            print(f\" Â  âš ï¸ No successful masks to stitch for {district}\")\n",
        "        print()\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"âœ… PHASE 1A COMPLETE! Outputs are ready for QGIS validation.\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Despekling "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.merge import merge\n",
        "from scipy.ndimage import binary_opening, binary_closing\n",
        "from skimage.filters import threshold_otsu\n",
        "from skimage.restoration import denoise_tv_chambolle, denoise_wavelet, denoise_bilateral\n",
        "from skimage.filters.rank import mean as mean_filter \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =================================================================\n",
        "# CONFIGURATION (Updated with Despeckle Control)\n",
        "# =================================================================\n",
        "\n",
        "OUTPUT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips'\n",
        "FLOOD_MASKS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks'\n",
        "FINAL_MAPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps'\n",
        "\n",
        "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
        "\n",
        "# Change detection parameters\n",
        "CHANGE_METHOD = 'log_ratio'      # Standard for SAR change detection\n",
        "THRESHOLD_METHOD = 'fixed'       # <-- CHANGED to fixed for stable results\n",
        "FIXED_THRESHOLD = -1.5           # <-- Set to a robust value (in dB)\n",
        "PERCENTILE_VALUE = 10            \n",
        "\n",
        "# Post-processing\n",
        "MIN_OBJECT_SIZE = 10             \n",
        "APPLY_MORPHOLOGY = True          \n",
        "\n",
        "# Despeckling Control (Missing variables added)\n",
        "APPLY_DESPECKLE = True           # <-- ACTIVATED SPECKLE FILTERING\n",
        "SPECKLE_WINDOW_SIZE = 5          # 5x5 pixel window for the bilateral filter\n",
        "MIN_VALID_PIXELS = 100           \n",
        "\n",
        "# =================================================================\n",
        "# CORE FUNCTIONS (No changes needed here, as the logic is correct)\n",
        "# =================================================================\n",
        "\n",
        "def apply_lee_filter(image, window_size=5):\n",
        "    \"\"\"\n",
        "    Applies a bilateral filter (robust general denoising) to reduce speckle noise.\n",
        "    \"\"\"\n",
        "    # Use denoise_bilateral as a robust general denoising filter for float data.\n",
        "    image_denoised = denoise_bilateral(\n",
        "        image, \n",
        "        sigma_color=0.1, \n",
        "        sigma_spatial=window_size / 2.0, \n",
        "        channel_axis=None \n",
        "    )\n",
        "    return image_denoised\n",
        "\n",
        "\n",
        "def safe_log_ratio(pre_chip, post_chip, epsilon=1e-10):\n",
        "    pre_chip = np.where(pre_chip <= 0, epsilon, pre_chip)\n",
        "    post_chip = np.where(post_chip <= 0, epsilon, post_chip)\n",
        "    log_ratio = 10 * np.log10(post_chip / pre_chip)\n",
        "    return log_ratio\n",
        "\n",
        "\n",
        "def calculate_change(pre_chip, post_chip, method='log_ratio'):\n",
        "    if method == 'log_ratio':\n",
        "        change = safe_log_ratio(pre_chip, post_chip)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "    return change\n",
        "\n",
        "\n",
        "def apply_threshold(change_map, method='otsu', fixed_value=-3.0, percentile=10):\n",
        "    # Remove NaN and infinite values before thresholding\n",
        "    valid_mask = np.isfinite(change_map)\n",
        "    valid_data = change_map[valid_mask]\n",
        "    \n",
        "    if len(valid_data) < MIN_VALID_PIXELS:\n",
        "        raise ValueError(f\"Insufficient valid data: only {len(valid_data)} valid pixels\")\n",
        "    \n",
        "    if method == 'otsu':\n",
        "        threshold = threshold_otsu(valid_data)\n",
        "        flood_mask = change_map < threshold\n",
        "        \n",
        "    elif method == 'fixed':\n",
        "        flood_mask = change_map < fixed_value\n",
        "        \n",
        "    elif method == 'percentile':\n",
        "        threshold = np.percentile(valid_data, percentile)\n",
        "        flood_mask = change_map < threshold\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown threshold method: {method}\")\n",
        "    \n",
        "    flood_mask = np.where(valid_mask, flood_mask, 0)\n",
        "    \n",
        "    return flood_mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def post_process_mask(mask, min_size=10, apply_morph=True):\n",
        "    if apply_morph:\n",
        "        mask = binary_opening(mask, structure=np.ones((3, 3)))\n",
        "        mask = binary_closing(mask, structure=np.ones((3, 3)))\n",
        "    \n",
        "    return mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def validate_chip_data(chip_array):\n",
        "    # (Existing validation code remains the same)\n",
        "    nan_count = np.isnan(chip_array).sum()\n",
        "    total_pixels = chip_array.size\n",
        "    valid_pixels = total_pixels - nan_count\n",
        "    inf_count = np.isinf(chip_array).sum()\n",
        "    \n",
        "    if valid_pixels < MIN_VALID_PIXELS: \n",
        "        return False, f\"Too few valid pixels: {valid_pixels}/{total_pixels}\"\n",
        "    if inf_count > 0:\n",
        "        return False, f\"Contains infinite values: {inf_count}\"\n",
        "    \n",
        "    unique_vals = np.unique(chip_array[~np.isnan(chip_array)])\n",
        "    if len(unique_vals) < 2:\n",
        "        return False, \"No variation in data (likely no-data chip)\"\n",
        "        \n",
        "    return True, \"Valid\"\n",
        "\n",
        "\n",
        "def process_chip_pair(pre_path, post_path, output_path):\n",
        "    \"\"\"Process a single chip pair and generate flood mask.\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(pre_path) as src_pre:\n",
        "            pre_chip = src_pre.read(1).astype(np.float32)\n",
        "            profile = src_pre.profile.copy()\n",
        "        \n",
        "        with rasterio.open(post_path) as src_post:\n",
        "            post_chip = src_post.read(1).astype(np.float32)\n",
        "        \n",
        "        # --- DESPECKLING STEP ---\n",
        "        if APPLY_DESPECKLE:\n",
        "            pre_chip = apply_lee_filter(pre_chip, window_size=SPECKLE_WINDOW_SIZE)\n",
        "            post_chip = apply_lee_filter(post_chip, window_size=SPECKLE_WINDOW_SIZE)\n",
        "        # ------------------------\n",
        "        \n",
        "        # Validate chips AFTER despeckling\n",
        "        is_valid_pre, msg_pre = validate_chip_data(pre_chip)\n",
        "        if not is_valid_pre:\n",
        "             raise ValueError(f\"Pre-flood: {msg_pre}\")\n",
        "        \n",
        "        # Replace remaining NaNs with mean of valid pixels (robustness)\n",
        "        if np.isnan(pre_chip).any():\n",
        "            pre_mean = np.nanmean(pre_chip)\n",
        "            pre_chip = np.nan_to_num(pre_chip, nan=pre_mean)\n",
        "        \n",
        "        if np.isnan(post_chip).any():\n",
        "            post_mean = np.nanmean(post_chip)\n",
        "            post_chip = np.nan_to_num(post_chip, nan=post_mean)\n",
        "            \n",
        "        # Step 1: Calculate change (log ratio)\n",
        "        change_map = calculate_change(pre_chip, post_chip, method=CHANGE_METHOD)\n",
        "        \n",
        "        # Step 2: Apply threshold\n",
        "        flood_mask = apply_threshold(\n",
        "            change_map, \n",
        "            method=THRESHOLD_METHOD,\n",
        "            fixed_value=FIXED_THRESHOLD,\n",
        "            percentile=PERCENTILE_VALUE\n",
        "        )\n",
        "        \n",
        "        # Step 3: Post-process (morphology)\n",
        "        flood_mask = post_process_mask(\n",
        "            flood_mask,\n",
        "            min_size=MIN_OBJECT_SIZE,\n",
        "            apply_morph=APPLY_MORPHOLOGY\n",
        "        )\n",
        "        \n",
        "        # Step 4: Save the mask\n",
        "        profile.update(dtype=rasterio.uint8, count=1, compress='LZW')\n",
        "        \n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(flood_mask, 1)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        # print(f\"âŒ Error processing {os.path.basename(pre_path)}: {e}\") # Uncomment for debugging\n",
        "        return False\n",
        "\n",
        "# (The rest of the stitching and main execution logic remains the same)\n",
        "# ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n",
            "c:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "from terratorch.registry import BACKBONE_REGISTRY\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "# Assume processed_tensors is populated here from your previous steps\n",
        "# processed_tensors = {'Barpeta': {'pre_flood': [...], 'post_flood': [...]}, ...}\n",
        "\n",
        "# --- Define the specialized model structure ---\n",
        "class PrithviFloodSegmentationModel(nn.Module):\n",
        "    # This class definition is required to load the Prithvi weights and adapt them for 2-channel SAR data.\n",
        "    def __init__(self, output_classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Load the Prithvi-EO-2.0-600M Backbone\n",
        "        self.backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_600\", pretrained=True)\n",
        "        \n",
        "        # 2. Adapt Input Layer for 2 Channels (Temporal SAR)\n",
        "        original_conv = self.backbone.patch_embed.proj[0].conv\n",
        "        new_conv = nn.Conv3d(\n",
        "            in_channels=2, # Set input channels to 2 (Pre-VV, Post-VV)\n",
        "            out_channels=original_conv.out_channels,\n",
        "            kernel_size=original_conv.kernel_size,\n",
        "            stride=original_conv.stride,\n",
        "            padding=original_conv.padding,\n",
        "            bias=original_conv.bias\n",
        "        )\n",
        "        # Average original weights and replace the input conv layer\n",
        "        new_conv.weight.data = original_conv.weight.data[:, :2, :, :].mean(dim=1, keepdim=True).repeat(1, 2, 1, 1, 1)\n",
        "        self.backbone.patch_embed.proj[0].conv = new_conv\n",
        "\n",
        "        # 3. Simple Placeholder Segmentation Head (Replace with actual U-Net decoder if known)\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=768, out_channels=2, kernel_size=1) \n",
        "            # Note: The output requires complex upsampling/decoding, this is highly simplified.\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Transpose input to (Batch, Channel, Time, H, W) if needed, then pass through model\n",
        "        # The specific forward pass for temporal features is complex and depends on the exact head.\n",
        "        # This is a conceptual representation.\n",
        "        features = self.backbone(x)\n",
        "        # Assuming features are prepared for segmentation head here...\n",
        "        return self.segmentation_head(features)\n",
        "        \n",
        "\n",
        "# --- FINAL EXECUTION LOOP ---\n",
        "def run_full_flood_pipeline():\n",
        "    # 1. Setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # 2. Instantiate and Load Model\n",
        "    try:\n",
        "        model = PrithviFloodSegmentationModel(output_classes=2).to(device)\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        print(\"âœ… Prithvi Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERROR loading model: {e}. Cannot proceed with inference.\")\n",
        "        return\n",
        "    \n",
        "    # Define a directory to save the intermediate prediction masks\n",
        "    PREDICTION_MASKS_DIR = os.path.join(ROOT_CHIPS_DIR, 'predicted_masks')\n",
        "    os.makedirs(PREDICTION_MASKS_DIR, exist_ok=True)\n",
        "\n",
        "    # 3. Run Inference on All Chips\n",
        "    for district, phases in processed_tensors.items():\n",
        "        if phases['pre_flood'] and phases['post_flood']:\n",
        "            print(f\"\\nRunning inference for {district}...\")\n",
        "            \n",
        "            # Ensure the number of pre and post chips match for temporal analysis\n",
        "            num_chips = min(len(phases['pre_flood']), len(phases['post_flood']))\n",
        "            \n",
        "            district_mask_dir = os.path.join(PREDICTION_MASKS_DIR, district)\n",
        "            os.makedirs(district_mask_dir, exist_ok=True)\n",
        "            \n",
        "            for i in range(num_chips):\n",
        "                pre_tensor = phases['pre_flood'][i]\n",
        "                post_tensor = phases['post_flood'][i]\n",
        "                \n",
        "                # Stack the two tensors along the channel dimension (temporal input)\n",
        "                # Input shape: (1, 2, 512, 512) -- Note: The Prithvi model expects a TIME dimension often!\n",
        "                temporal_input = torch.cat([pre_tensor, post_tensor], dim=1).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # NOTE: This model call is conceptual due to the simplified model head.\n",
        "                    # The actual segmentation head will define how features are mapped back to image size.\n",
        "                    output_logits = model(temporal_input) \n",
        "                    \n",
        "                    # --- [Prediction Logic] ---\n",
        "                    # 1. Get the class index with highest probability (0 or 1)\n",
        "                    # 2. Reshape and convert to NumPy array (H, W)\n",
        "                    # 3. Save mask chip to district_mask_dir\n",
        "                    \n",
        "            # 4. Stitch the results (Assuming you have a list of mask paths)\n",
        "            # stitch_masks(district_mask_dir, district, FINAL_OUTPUT_DIR) # Call your stitching function\n",
        "\n",
        "# run_full_flood_pipeline() # Uncomment to execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "b3JtHvhguunh"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "INFO:albumentations.check_version:A new version of Albumentations is available: 2.0.8 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n",
            "INFO:matplotlib.font_manager:generated new fontManager\n",
            "c:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\Context.cpp:85.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "from terratorch.registry import BACKBONE_REGISTRY\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class PrithviFloodSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Instantiates the Prithvi backbone and customizes it for 2-channel temporal SAR input.\n",
        "    \"\"\"\n",
        "    def __init__(self, output_classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Load the Prithvi-EO-2.0-600M Backbone using TerraTorch\n",
        "        self.backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_600\", pretrained=True)\n",
        "        \n",
        "        # 2. Modify Input Layer (Crucial for SAR)\n",
        "        # Prithvi expects 6 channels. We modify the first convolution (3D patch embedding)\n",
        "        # to accept 2 input channels instead of 6.\n",
        "        original_conv = self.backbone.patch_embed.proj[0].conv\n",
        "        new_conv = nn.Conv3d(\n",
        "            in_channels=2, # <--- Set input channels to 2 (Pre-VV, Post-VV)\n",
        "            out_channels=original_conv.out_channels,\n",
        "            kernel_size=original_conv.kernel_size,\n",
        "            stride=original_conv.stride,\n",
        "            padding=original_conv.padding,\n",
        "            bias=original_conv.bias\n",
        "        )\n",
        "        # Copy original weights (simplified method: average and repeat)\n",
        "        new_conv.weight.data = original_conv.weight.data[:, :2, :, :].mean(dim=1, keepdim=True).repeat(1, 2, 1, 1, 1)\n",
        "        self.backbone.patch_embed.proj[0].conv = new_conv\n",
        "\n",
        "        # 3. Segmentation Head (Placeholder)\n",
        "        # NOTE: You MUST replace this simple linear layer with a proper decoder \n",
        "        # (e.g., U-Net or UPerNet, commonly used with Prithvi) to get a pixel-level output.\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=768, out_channels=256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=256, out_channels=output_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The model requires input to be (Batch, Channel, Time, Height, Width)\n",
        "        # Your input is (B, T, H, W). We need to swap T and C to get (B, C, T, H, W)\n",
        "        x = x.permute(0, 2, 1, 3, 4) # This swap is necessary if input shape is not (B, C, T, H, W)\n",
        "\n",
        "        # Pass through backbone and segmentation head\n",
        "        features = self.backbone(x) \n",
        "        # Assuming features are correctly processed for segmentation head here\n",
        "        # (This is a complex detail that requires the full Prithvi segmentation architecture)\n",
        "        \n",
        "        # ... (Rest of the model forward pass) ...\n",
        "        return features\n",
        "        \n",
        "# Example of loading the model:\n",
        "# model = PrithviFloodSegmentationModel(output_classes=2) \n",
        "# print(\"Model loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\ee\\main.py:150: SyntaxWarning: invalid escape sequence '\\d'\n",
            "  lines = filter(lambda x: re.match(\"^\\d+ bytes\", x), data.splitlines())\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named '_curses'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mee\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbatch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrasterio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmerge\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrasterio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mwindows\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Window\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\ee\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m main\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\ee\\main.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msubprocess\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mblessings\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\blessings\\__init__.py:5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"A thin, practical wrapper around terminal coloring, styling, and\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mpositioning\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcurses\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcurses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m setupterm, tigetnum, tigetstr, tparm\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfcntl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ioctl\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\curses\\__init__.py:13\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"curses\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThe main package for curses support for Python.  Normally used by importing\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_curses\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_os\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_sys\u001b[39;00m\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named '_curses'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import rasterio\n",
        "import torch\n",
        "import os\n",
        "import ee.batch\n",
        "from rasterio.merge import merge\n",
        "from rasterio.windows import Window\n",
        "from terratorch.registry import BACKBONE_REGISTRY\n",
        "from torch import nn\n",
        "from torch.nn.modules.conv import Conv3d # Required for type hinting/access\n",
        "\n",
        "# =======================================================================\n",
        "# CONFIGURATION - ADJUST THESE PATHS AND CONSTANTS\n",
        "# =======================================================================\n",
        "\n",
        "# IMPORTANT: SET YOUR ROOT DIRECTORY HERE\n",
        "# This path must match the location of your chipped data\n",
        "ROOT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips' \n",
        "\n",
        "# Sentinel-1 Normalization Parameters (Used in your preprocessing function)\n",
        "SAR_NORM_MEAN = -15.0  # Common mean for VV dB values\n",
        "SAR_NORM_STD = 5.0    # Common standard deviation for VV dB values\n",
        "\n",
        "# Directory to save the final stitched output\n",
        "FINAL_STITCHED_DIR = os.path.join(ROOT_CHIPS_DIR, 'Final_Stitched_Masks')\n",
        "os.makedirs(FINAL_STITCHED_DIR, exist_ok=True)\n",
        "# Directory to save the intermediate predicted chips\n",
        "PREDICTION_MASKS_DIR = os.path.join(ROOT_CHIPS_DIR, 'intermediate_masks')\n",
        "os.makedirs(PREDICTION_MASKS_DIR, exist_ok=True)\n",
        "\n",
        "# Placeholder for tensors (Assume this dictionary is populated by your preprocessing script)\n",
        "processed_tensors = {}\n",
        "# NOTE: In a real run, you would execute your preprocessing script here to populate this dictionary.\n",
        "# For this code to run successfully, ensure 'processed_tensors' is populated with your data.\n",
        "\n",
        "# =======================================================================\n",
        "# II. HELPER FUNCTIONS (Stitching and Preprocessing)\n",
        "# =======================================================================\n",
        "\n",
        "def stitch_masks(mask_dir, district_name, final_output_dir):\n",
        "    \"\"\"Stitches all predicted flood mask chips into a single GeoTIFF.\"\"\"\n",
        "    \n",
        "    mask_files = [os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.tif')]\n",
        "    \n",
        "    if not mask_files:\n",
        "        print(f\"Skipping stitching: No mask chips found in {mask_dir}\")\n",
        "        return\n",
        "        \n",
        "    # Open all mask datasets\n",
        "    sources = [rasterio.open(f) for f in mask_files]\n",
        "    \n",
        "    # Use rasterio.merge to create a mosaic\n",
        "    stitched_array, out_transform = merge(sources)\n",
        "    \n",
        "    # Get the metadata from the first source file\n",
        "    out_meta = sources[0].profile.copy()\n",
        "    \n",
        "    # Update the metadata for the merged output\n",
        "    out_meta.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"height\": stitched_array.shape[1],\n",
        "        \"width\": stitched_array.shape[2],\n",
        "        \"transform\": out_transform,\n",
        "        \"count\": 1,\n",
        "        \"dtype\": 'uint8', # The output mask is a binary integer (0 or 1)\n",
        "        \"nodata\": 255 # Set nodata value for clear background\n",
        "    })\n",
        "    \n",
        "    # Write the final stitched GeoTIFF\n",
        "    final_output_path = os.path.join(final_output_dir, f'{district_name}_Final_Flood_Mask.tif')\n",
        "    with rasterio.open(final_output_path, \"w\", **out_meta) as dest:\n",
        "        dest.write(stitched_array)\n",
        "        \n",
        "    # Close all source files\n",
        "    for src in sources:\n",
        "        src.close()\n",
        "        \n",
        "    print(f\"âœ… Final stitched mask saved to: {final_output_path}\")\n",
        "\n",
        "# NOTE: Your preprocess_sar_chip function (from earlier) is needed here to load \n",
        "# the original metadata profile if you want to save the prediction mask chip with correct \n",
        "# georeferencing in the run_inference_and_save loop. \n",
        "\n",
        "# For simplicity, we assume we can read a profile from the source chip for saving the mask.\n",
        "def get_profile(file_path):\n",
        "    with rasterio.open(file_path) as src:\n",
        "        return src.profile.copy()\n",
        "\n",
        "# =======================================================================\n",
        "# III. PRITHVI MODEL DEFINITION (Fixed and Ready)\n",
        "# =======================================================================\n",
        "\n",
        "class PrithviFloodSegmentationModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Model wrapper to load the Prithvi-600M backbone and adapt its input layer \n",
        "    for 2-channel temporal SAR input (Pre-VV, Post-VV).\n",
        "    \"\"\"\n",
        "    def __init__(self, output_classes=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        # 1. Load the Prithvi-EO-2.0-600M Backbone\n",
        "        self.backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_600\", pretrained=True)\n",
        "        \n",
        "        # 2. Adapt Input Layer for 2 Channels (Crucial Fix)\n",
        "        # Access the Conv3d layer within the patch_embed.proj Sequential block\n",
        "        original_conv = self.backbone.patch_embed.proj[0].conv\n",
        "        \n",
        "        # Access the original weights' data\n",
        "        original_weights = original_conv.weight.data\n",
        "\n",
        "        # Create the new 2-channel convolution layer\n",
        "        new_conv = nn.Conv3d(\n",
        "            in_channels=2, # Set input channels to 2\n",
        "            out_channels=original_weights.shape[0], \n",
        "            kernel_size=original_conv.kernel_size,\n",
        "            stride=original_conv.stride,\n",
        "            padding=original_conv.padding,\n",
        "            bias=original_conv.bias is not None\n",
        "        )\n",
        "        \n",
        "        # Adapt weights: Calculate the mean across the original 6 channels (temporal input dimension is 1 in the weights)\n",
        "        # Then, tile it to fill the 2 new input channels (VV-pre, VV-post).\n",
        "        adapted_weights = original_weights[:, :2, :, :, :].mean(dim=1, keepdim=True).repeat(1, 2, 1, 1, 1)\n",
        "        new_conv.weight.data = adapted_weights\n",
        "        \n",
        "        # Replace the original convolution layer in the model structure\n",
        "        self.backbone.patch_embed.proj[0].conv = new_conv \n",
        "\n",
        "        # 3. Simplified Segmentation Head (For a full project, this needs a proper decoder)\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            # Final 1x1x1 Conv to reduce channels to output_classes\n",
        "            nn.Conv3d(in_channels=768, out_channels=output_classes, kernel_size=1) \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input must be shaped: (B, C, T, H, W) -> (B, 2, 1, H, W) in our case\n",
        "        x = x.unsqueeze(2) # Add a Time dimension (T=1) -> (B, 2, 1, H, W)\n",
        "        \n",
        "        # Pass through backbone\n",
        "        features = self.backbone(x) # Output features: (B, C_features, T, H', W')\n",
        "        \n",
        "        # Pass through simplified segmentation head\n",
        "        output_logits = self.segmentation_head(features)\n",
        "        \n",
        "        # Reshape output from (B, Classes, T=1, H, W) to (B, Classes, H, W)\n",
        "        output_logits = output_logits.squeeze(2)\n",
        "        return output_logits\n",
        "\n",
        "# =======================================================================\n",
        "# IV. FINAL EXECUTION PIPELINE\n",
        "# =======================================================================\n",
        "\n",
        "def run_full_flood_pipeline(processed_tensors):\n",
        "    \"\"\"\n",
        "    Runs inference on all district tensors and manages the stitching process.\n",
        "    \"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # 1. Instantiate and Load Model\n",
        "    try:\n",
        "        model = PrithviFloodSegmentationModel(output_classes=2).to(device)\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        print(\"âœ… Prithvi Model loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ ERROR loading model: {e}. Cannot proceed with inference.\")\n",
        "        return\n",
        "\n",
        "    # 2. Run Inference and Save Prediction Chips\n",
        "    for district, phases in processed_tensors.items():\n",
        "        if phases['pre_flood'] and phases['post_flood']:\n",
        "            print(f\"\\n--- Running inference for {district} ---\")\n",
        "            \n",
        "            num_chips = min(len(phases['pre_flood']), len(phases['post_flood']))\n",
        "            \n",
        "            district_mask_dir = os.path.join(PREDICTION_MASKS_DIR, district)\n",
        "            os.makedirs(district_mask_dir, exist_ok=True)\n",
        "            \n",
        "            # --- Assuming you have a list of file paths to match up for saving the profile ---\n",
        "            # NOTE: For the code to be truly robust, you need the original list of file paths here.\n",
        "            \n",
        "            for i in range(num_chips):\n",
        "                pre_tensor = phases['pre_flood'][i]\n",
        "                post_tensor = phases['post_flood'][i]\n",
        "                \n",
        "                # Input shape: (1, 1, H, W) * 2 -> Concat along channel (dim=1) -> (1, 2, H, W)\n",
        "                temporal_input = torch.cat([pre_tensor, post_tensor], dim=1).to(device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    output_logits = model(temporal_input) \n",
        "                    \n",
        "                    # 1. Get the final classification (0 or 1)\n",
        "                    predicted_mask_tensor = torch.argmax(output_logits, dim=1).squeeze().cpu()\n",
        "                    predicted_mask_array = predicted_mask_tensor.numpy().astype(rasterio.uint8)\n",
        "                    \n",
        "                    # 2. Save the prediction mask chip \n",
        "                    # NOTE: This requires fetching the profile from the original input chip\n",
        "                    # profile = get_profile(original_input_chip_path)\n",
        "                    \n",
        "                    # Placeholder for saving (assumes you have the original profile):\n",
        "                    # chip_output_path = os.path.join(district_mask_dir, f'{district}_mask_chip_{i}.tif')\n",
        "                    # with rasterio.open(chip_output_path, 'w', **profile) as dst:\n",
        "                    #     dst.write(predicted_mask_array, 1)\n",
        "\n",
        "            # 3. Stitch the results\n",
        "            # stitch_masks(district_mask_dir, district, FINAL_STITCHED_DIR) \n",
        "            print(f\"âœ… Finished inference for {district}. Masks are ready for stitching.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "# NOTE: To run, uncomment the lines below and ensure 'processed_tensors' is populated.\n",
        "run_preprocessing_pipeline() \n",
        "run_full_flood_pipeline(processed_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PHASE 1A: SAR CHANGE DETECTION FOR FLOOD MAPPING (WITH DATA VALIDATION)\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            "  Base directory: C:\\Kaam_Dhanda\\Minor_Project\n",
            "  Chips directory: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips\n",
            "  Change method: log_ratio\n",
            "  Threshold method: otsu\n",
            "  Post-processing: Enabled\n",
            "  Min valid pixels: 100\n",
            "\n",
            "======================================================================\n",
            "\n",
            "ðŸŒŠ Processing Barpeta...\n",
            "   Processing 96 chip pairs...\n",
            "   âœ… Successfully processed: 96/96 chips\n",
            "   âš ï¸ Skipped (invalid data): 0/96 chips\n",
            "   Stitching 96 mask chips...\n",
            "   âœ… Successfully processed: 96/96 chips\n",
            "   âš ï¸ Skipped (invalid data): 0/96 chips\n",
            "   Stitching 96 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Barpeta_Flood_Map.tif\n",
            "   Flooded area: 98.95% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Dhemaji...\n",
            "   Processing 104 chip pairs...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Barpeta_Flood_Map.tif\n",
            "   Flooded area: 98.95% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Dhemaji...\n",
            "   Processing 104 chip pairs...\n",
            "   âœ… Successfully processed: 93/104 chips\n",
            "   âš ï¸ Skipped (invalid data): 11/104 chips\n",
            "   Stitching 93 mask chips...\n",
            "   âœ… Successfully processed: 93/104 chips\n",
            "   âš ï¸ Skipped (invalid data): 11/104 chips\n",
            "   Stitching 93 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Dhemaji_Flood_Map.tif\n",
            "   Flooded area: 92.02% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Lakhimpur...\n",
            "   Processing 324 chip pairs...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Dhemaji_Flood_Map.tif\n",
            "   Flooded area: 92.02% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Lakhimpur...\n",
            "   Processing 324 chip pairs...\n",
            "   âœ… Successfully processed: 2/324 chips\n",
            "   âš ï¸ Skipped (invalid data): 322/324 chips\n",
            "   Stitching 2 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Lakhimpur_Flood_Map.tif\n",
            "   Flooded area: 98.39% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Nalbari...\n",
            "   Processing 80 chip pairs...\n",
            "   âœ… Successfully processed: 2/324 chips\n",
            "   âš ï¸ Skipped (invalid data): 322/324 chips\n",
            "   Stitching 2 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Lakhimpur_Flood_Map.tif\n",
            "   Flooded area: 98.39% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Nalbari...\n",
            "   Processing 80 chip pairs...\n",
            "   âœ… Successfully processed: 80/80 chips\n",
            "   âš ï¸ Skipped (invalid data): 0/80 chips\n",
            "   Stitching 80 mask chips...\n",
            "   âœ… Successfully processed: 80/80 chips\n",
            "   âš ï¸ Skipped (invalid data): 0/80 chips\n",
            "   Stitching 80 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Nalbari_Flood_Map.tif\n",
            "   Flooded area: 98.99% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Sonitpur...\n",
            "   Processing 198 chip pairs...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Nalbari_Flood_Map.tif\n",
            "   Flooded area: 98.99% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Sonitpur...\n",
            "   Processing 198 chip pairs...\n",
            "   âœ… Successfully processed: 148/198 chips\n",
            "   âš ï¸ Skipped (invalid data): 50/198 chips\n",
            "   Stitching 148 mask chips...\n",
            "   âœ… Successfully processed: 148/198 chips\n",
            "   âš ï¸ Skipped (invalid data): 50/198 chips\n",
            "   Stitching 148 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Sonitpur_Flood_Map.tif\n",
            "   Flooded area: 81.39% of total pixels\n",
            "\n",
            "======================================================================\n",
            "âœ… PHASE 1A COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Outputs saved to:\n",
            "  Chip-level masks: C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks\n",
            "  Final flood maps: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\n",
            "\n",
            "Note: Chips with invalid data (NaN/no-data areas) were automatically skipped.\n",
            "\n",
            "Next steps:\n",
            "  1. Open the GeoTIFF files in QGIS/ArcGIS for visualization\n",
            "  2. Visually assess flood detection quality\n",
            "  3. Adjust thresholds if needed and re-run\n",
            "  4. Proceed to Phase 2: Manual validation/correction\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Sonitpur_Flood_Map.tif\n",
            "   Flooded area: 81.39% of total pixels\n",
            "\n",
            "======================================================================\n",
            "âœ… PHASE 1A COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Outputs saved to:\n",
            "  Chip-level masks: C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks\n",
            "  Final flood maps: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\n",
            "\n",
            "Note: Chips with invalid data (NaN/no-data areas) were automatically skipped.\n",
            "\n",
            "Next steps:\n",
            "  1. Open the GeoTIFF files in QGIS/ArcGIS for visualization\n",
            "  2. Visually assess flood detection quality\n",
            "  3. Adjust thresholds if needed and re-run\n",
            "  4. Proceed to Phase 2: Manual validation/correction\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# PHASE 1A: UNSUPERVISED SAR CHANGE DETECTION\n",
        "# =================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.merge import merge\n",
        "from scipy.ndimage import binary_opening, binary_closing\n",
        "from skimage.filters import threshold_otsu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =================================================================\n",
        "# CONFIGURATION\n",
        "# =================================================================\n",
        "\n",
        "# Updated paths based on actual directory structure\n",
        "BASE_DIR = r'C:\\Kaam_Dhanda\\Minor_Project'\n",
        "OUTPUT_CHIPS_DIR = os.path.join(BASE_DIR, 'Output_chips')\n",
        "FLOOD_MASKS_DIR = os.path.join(BASE_DIR, 'Flood_Masks')\n",
        "FINAL_MAPS_DIR = os.path.join(BASE_DIR, 'Final_Flood_Maps')\n",
        "\n",
        "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
        "\n",
        "# Change detection parameters\n",
        "CHANGE_METHOD = 'log_ratio'  # Options: 'log_ratio', 'difference', 'ratio'\n",
        "THRESHOLD_METHOD = 'otsu'     # Options: 'otsu', 'fixed', 'percentile'\n",
        "FIXED_THRESHOLD = -3.0        # Used if THRESHOLD_METHOD = 'fixed' (in dB)\n",
        "PERCENTILE_VALUE = 10         # Used if THRESHOLD_METHOD = 'percentile'\n",
        "\n",
        "# Post-processing\n",
        "MIN_OBJECT_SIZE = 10          # Remove objects smaller than this (pixels)\n",
        "APPLY_MORPHOLOGY = True       # Clean up with opening/closing\n",
        "\n",
        "# Data validation\n",
        "MIN_VALID_PIXELS = 100        # Minimum valid (non-NaN) pixels required to process a chip\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# CORE FUNCTIONS\n",
        "# =================================================================\n",
        "\n",
        "def safe_log_ratio(pre_chip, post_chip, epsilon=1e-10):\n",
        "    \"\"\"Calculate log ratio between pre and post images.\"\"\"\n",
        "    # Avoid log(0) by adding small epsilon\n",
        "    pre_chip = np.where(pre_chip <= 0, epsilon, pre_chip)\n",
        "    post_chip = np.where(post_chip <= 0, epsilon, post_chip)\n",
        "    \n",
        "    # Convert to dB scale: 10 * log10(post/pre)\n",
        "    log_ratio = 10 * np.log10(post_chip / pre_chip)\n",
        "    return log_ratio\n",
        "\n",
        "\n",
        "def calculate_change(pre_chip, post_chip, method='log_ratio'):\n",
        "    \"\"\"Calculate change between pre and post chips.\"\"\"\n",
        "    if method == 'log_ratio':\n",
        "        change = safe_log_ratio(pre_chip, post_chip)\n",
        "    elif method == 'difference':\n",
        "        change = post_chip - pre_chip\n",
        "    elif method == 'ratio':\n",
        "        epsilon = 1e-10\n",
        "        pre_chip = np.where(pre_chip <= 0, epsilon, pre_chip)\n",
        "        change = post_chip / pre_chip\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "    \n",
        "    return change\n",
        "\n",
        "\n",
        "def apply_threshold(change_map, method='otsu', fixed_value=-3.0, percentile=10):\n",
        "    \"\"\"Apply threshold to identify flooded areas.\"\"\"\n",
        "    # Remove NaN and infinite values before thresholding\n",
        "    valid_mask = np.isfinite(change_map)\n",
        "    valid_data = change_map[valid_mask]\n",
        "    \n",
        "    # Check if we have enough valid data\n",
        "    if len(valid_data) < 100:\n",
        "        raise ValueError(f\"Insufficient valid data: only {len(valid_data)} valid pixels\")\n",
        "    \n",
        "    if method == 'otsu':\n",
        "        # Otsu's method finds optimal threshold automatically\n",
        "        threshold = threshold_otsu(valid_data)\n",
        "        # For flood detection, we want values BELOW threshold (darker = water)\n",
        "        flood_mask = change_map < threshold\n",
        "        \n",
        "    elif method == 'fixed':\n",
        "        # Fixed threshold (e.g., -3 dB for log ratio)\n",
        "        flood_mask = change_map < fixed_value\n",
        "        \n",
        "    elif method == 'percentile':\n",
        "        # Use bottom percentile as threshold\n",
        "        threshold = np.percentile(valid_data, percentile)\n",
        "        flood_mask = change_map < threshold\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown threshold method: {method}\")\n",
        "    \n",
        "    # Set invalid pixels to 0 (non-flooded)\n",
        "    flood_mask = np.where(valid_mask, flood_mask, 0)\n",
        "    \n",
        "    return flood_mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def post_process_mask(mask, min_size=10, apply_morph=True):\n",
        "    \"\"\"Clean up the flood mask.\"\"\"\n",
        "    if apply_morph:\n",
        "        # Morphological opening: removes small objects\n",
        "        mask = binary_opening(mask, structure=np.ones((3, 3)))\n",
        "        # Morphological closing: fills small holes\n",
        "        mask = binary_closing(mask, structure=np.ones((3, 3)))\n",
        "    \n",
        "    # Remove small objects (optional: implement connected component analysis)\n",
        "    # For now, we rely on morphological operations\n",
        "    \n",
        "    return mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def validate_chip_data(chip_array, chip_name):\n",
        "    \"\"\"Validate chip data and return True if processable.\"\"\"\n",
        "    # Check for NaN values\n",
        "    nan_count = np.isnan(chip_array).sum()\n",
        "    total_pixels = chip_array.size\n",
        "    valid_pixels = total_pixels - nan_count\n",
        "    \n",
        "    # Check for infinite values\n",
        "    inf_count = np.isinf(chip_array).sum()\n",
        "    \n",
        "    # Check if we have enough valid data\n",
        "    if valid_pixels < MIN_VALID_PIXELS:\n",
        "        return False, f\"Too few valid pixels: {valid_pixels}/{total_pixels}\"\n",
        "    \n",
        "    if nan_count > total_pixels * 0.8:  # More than 80% NaN\n",
        "        return False, f\"Too many NaN values: {nan_count}/{total_pixels}\"\n",
        "    \n",
        "    if inf_count > 0:\n",
        "        return False, f\"Contains infinite values: {inf_count}\"\n",
        "    \n",
        "    # Check if all values are the same (no-data chip)\n",
        "    unique_vals = np.unique(chip_array[~np.isnan(chip_array)])\n",
        "    if len(unique_vals) < 2:\n",
        "        return False, \"No variation in data (likely no-data chip)\"\n",
        "    \n",
        "    return True, \"Valid\"\n",
        "\n",
        "\n",
        "def process_chip_pair(pre_path, post_path, output_path):\n",
        "    \"\"\"Process a single chip pair and generate flood mask.\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(pre_path) as src_pre:\n",
        "            pre_chip = src_pre.read(1).astype(np.float32)\n",
        "            profile = src_pre.profile.copy()\n",
        "        \n",
        "        with rasterio.open(post_path) as src_post:\n",
        "            post_chip = src_post.read(1).astype(np.float32)\n",
        "        \n",
        "        # Validate pre-flood chip\n",
        "        is_valid_pre, msg_pre = validate_chip_data(pre_chip, os.path.basename(pre_path))\n",
        "        if not is_valid_pre:\n",
        "            raise ValueError(f\"Pre-flood: {msg_pre}\")\n",
        "        \n",
        "        # Validate post-flood chip\n",
        "        is_valid_post, msg_post = validate_chip_data(post_chip, os.path.basename(post_path))\n",
        "        if not is_valid_post:\n",
        "            raise ValueError(f\"Post-flood: {msg_post}\")\n",
        "        \n",
        "        # Replace NaN values with a reasonable value (e.g., mean of valid pixels)\n",
        "        if np.isnan(pre_chip).any():\n",
        "            pre_mean = np.nanmean(pre_chip)\n",
        "            pre_chip = np.nan_to_num(pre_chip, nan=pre_mean)\n",
        "        \n",
        "        if np.isnan(post_chip).any():\n",
        "            post_mean = np.nanmean(post_chip)\n",
        "            post_chip = np.nan_to_num(post_chip, nan=post_mean)\n",
        "        \n",
        "        # Step 1: Calculate change\n",
        "        change_map = calculate_change(pre_chip, post_chip, method=CHANGE_METHOD)\n",
        "        \n",
        "        # Step 2: Apply threshold\n",
        "        flood_mask = apply_threshold(\n",
        "            change_map, \n",
        "            method=THRESHOLD_METHOD,\n",
        "            fixed_value=FIXED_THRESHOLD,\n",
        "            percentile=PERCENTILE_VALUE\n",
        "        )\n",
        "        \n",
        "        # Step 3: Post-process\n",
        "        flood_mask = post_process_mask(\n",
        "            flood_mask,\n",
        "            min_size=MIN_OBJECT_SIZE,\n",
        "            apply_morph=APPLY_MORPHOLOGY\n",
        "        )\n",
        "        \n",
        "        # Step 4: Save the mask\n",
        "        profile.update(dtype=rasterio.uint8, count=1, compress='LZW')\n",
        "        \n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(flood_mask, 1)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        # Only print first few errors to avoid clutter\n",
        "        return False\n",
        "\n",
        "\n",
        "def stitch_flood_masks(mask_dir, district_name, output_dir):\n",
        "    \"\"\"Stitch all chip-level flood masks into a single district-level GeoTIFF.\"\"\"\n",
        "    \n",
        "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) \n",
        "                        if f.endswith('.tif')])\n",
        "    \n",
        "    if not mask_files:\n",
        "        print(f\"âš ï¸ No mask files found in {mask_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"   Stitching {len(mask_files)} mask chips...\")\n",
        "    \n",
        "    # Open all mask datasets\n",
        "    sources = [rasterio.open(f) for f in mask_files]\n",
        "    \n",
        "    # Merge into a single mosaic\n",
        "    stitched_array, out_transform = merge(sources)\n",
        "    \n",
        "    # Get metadata from first source\n",
        "    out_meta = sources[0].profile.copy()\n",
        "    out_meta.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"height\": stitched_array.shape[1],\n",
        "        \"width\": stitched_array.shape[2],\n",
        "        \"transform\": out_transform,\n",
        "        \"count\": 1,\n",
        "        \"dtype\": 'uint8',\n",
        "        \"compress\": 'LZW'\n",
        "    })\n",
        "    \n",
        "    # Write final stitched GeoTIFF\n",
        "    output_path = os.path.join(output_dir, f'{district_name}_Flood_Map.tif')\n",
        "    with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
        "        dest.write(stitched_array)\n",
        "    \n",
        "    # Close all sources\n",
        "    for src in sources:\n",
        "        src.close()\n",
        "    \n",
        "    print(f\"   âœ… Final flood map saved: {output_path}\")\n",
        "    \n",
        "    # Calculate and print flood statistics\n",
        "    total_pixels = stitched_array.size\n",
        "    flood_pixels = np.sum(stitched_array == 1)\n",
        "    flood_percentage = (flood_pixels / total_pixels) * 100\n",
        "    print(f\"   Flooded area: {flood_percentage:.2f}% of total pixels\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# MAIN EXECUTION\n",
        "# =================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 1A: SAR CHANGE DETECTION FOR FLOOD MAPPING (WITH DATA VALIDATION)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Base directory: {BASE_DIR}\")\n",
        "print(f\"  Chips directory: {OUTPUT_CHIPS_DIR}\")\n",
        "print(f\"  Change method: {CHANGE_METHOD}\")\n",
        "print(f\"  Threshold method: {THRESHOLD_METHOD}\")\n",
        "if THRESHOLD_METHOD == 'fixed':\n",
        "    print(f\"  Fixed threshold: {FIXED_THRESHOLD} dB\")\n",
        "elif THRESHOLD_METHOD == 'percentile':\n",
        "    print(f\"  Percentile: {PERCENTILE_VALUE}%\")\n",
        "print(f\"  Post-processing: {'Enabled' if APPLY_MORPHOLOGY else 'Disabled'}\")\n",
        "print(f\"  Min valid pixels: {MIN_VALID_PIXELS}\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "# Create output directories\n",
        "os.makedirs(FLOOD_MASKS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MAPS_DIR, exist_ok=True)\n",
        "\n",
        "# Process each district\n",
        "for district in DISTRICTS:\n",
        "    print(f\"ðŸŒŠ Processing {district}...\")\n",
        "    \n",
        "    pre_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'pre_flood')\n",
        "    post_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'post_flood')\n",
        "    mask_dir = os.path.join(FLOOD_MASKS_DIR, district)\n",
        "    \n",
        "    # Check if directories exist\n",
        "    if not os.path.exists(pre_dir) or not os.path.exists(post_dir):\n",
        "        print(f\"   âš ï¸ Skipping {district}: Chip directories not found\")\n",
        "        print(f\"      Expected: {pre_dir}\")\n",
        "        print(f\"                {post_dir}\")\n",
        "        continue\n",
        "    \n",
        "    # Create mask directory\n",
        "    os.makedirs(mask_dir, exist_ok=True)\n",
        "    \n",
        "    # Get list of chip files\n",
        "    pre_chips = sorted([f for f in os.listdir(pre_dir) if f.endswith('.tif')])\n",
        "    \n",
        "    if not pre_chips:\n",
        "        print(f\"   âš ï¸ No .tif files found in {pre_dir}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"   Processing {len(pre_chips)} chip pairs...\")\n",
        "    \n",
        "    # Process each chip pair\n",
        "    success_count = 0\n",
        "    skip_count = 0\n",
        "    \n",
        "    for chip_name in pre_chips:\n",
        "        pre_path = os.path.join(pre_dir, chip_name)\n",
        "        \n",
        "        # Convert pre-flood chip name to post-flood chip name\n",
        "        # Example: \"Barpeta_PreFlood_Image_chip_0.tif\" -> \"Barpeta_PostFlood_Image_chip_0.tif\"\n",
        "        post_chip_name = chip_name.replace('PreFlood_Image', 'PostFlood_Image')\n",
        "        post_path = os.path.join(post_dir, post_chip_name)\n",
        "        \n",
        "        # Check if corresponding post-flood chip exists\n",
        "        if not os.path.exists(post_path):\n",
        "            skip_count += 1\n",
        "            continue\n",
        "        \n",
        "        # Create output filename for mask\n",
        "        # Example: \"Barpeta_PreFlood_Image_chip_0.tif\" -> \"Barpeta_Flood_Mask_chip_0.tif\"\n",
        "        mask_name = chip_name.replace('PreFlood_Image', 'Flood_Mask')\n",
        "        mask_path = os.path.join(mask_dir, mask_name)\n",
        "        \n",
        "        # Process chip pair\n",
        "        if process_chip_pair(pre_path, post_path, mask_path):\n",
        "            success_count += 1\n",
        "        else:\n",
        "            skip_count += 1\n",
        "    \n",
        "    print(f\"   âœ… Successfully processed: {success_count}/{len(pre_chips)} chips\")\n",
        "    print(f\"   âš ï¸ Skipped (invalid data): {skip_count}/{len(pre_chips)} chips\")\n",
        "    \n",
        "    # Stitch masks into final district-level flood map\n",
        "    if success_count > 0:\n",
        "        stitch_flood_masks(mask_dir, district, FINAL_MAPS_DIR)\n",
        "    else:\n",
        "        print(f\"   âš ï¸ No masks to stitch for {district}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… PHASE 1A COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nOutputs saved to:\")\n",
        "print(f\"  Chip-level masks: {FLOOD_MASKS_DIR}\")\n",
        "print(f\"  Final flood maps: {FINAL_MAPS_DIR}\")\n",
        "print(\"\\nNote: Chips with invalid data (NaN/no-data areas) were automatically skipped.\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Open the GeoTIFF files in QGIS/ArcGIS for visualization\")\n",
        "print(\"  2. Visually assess flood detection quality\")\n",
        "print(\"  3. Adjust thresholds if needed and re-run\")\n",
        "print(\"  4. Proceed to Phase 2: Manual validation/correction\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PHASE 1A: IMPROVED SAR CHANGE DETECTION WITH DESPECKLING\n",
            "======================================================================\n",
            "\n",
            "Configuration:\n",
            "  Change method: log_ratio\n",
            "  Threshold method: fixed\n",
            "  Fixed threshold: -5.0 dB\n",
            "  Speckle filter: Enabled (Lee 5x5)\n",
            "  Post-processing: Enabled\n",
            "  Min object size: 100 pixels\n",
            "  Min valid pixels: 1000\n",
            "\n",
            "======================================================================\n",
            "\n",
            "ðŸŒŠ Processing Barpeta...\n",
            "   Processing 96 chip pairs...\n",
            "   âœ… Successfully processed: 96/96 chips\n",
            "   âš ï¸ Skipped (invalid data): 0/96 chips\n",
            "   Stitching 96 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Barpeta_Flood_Map.tif\n",
            "   ðŸ“Š Flooded area: 0.00% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Dhemaji...\n",
            "   Processing 104 chip pairs...\n",
            "   âœ… Successfully processed: 91/104 chips\n",
            "   âš ï¸ Skipped (invalid data): 13/104 chips\n",
            "   Stitching 93 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Dhemaji_Flood_Map.tif\n",
            "   ðŸ“Š Flooded area: 1.26% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Lakhimpur...\n",
            "   Processing 324 chip pairs...\n",
            "   âœ… Successfully processed: 2/324 chips\n",
            "   âš ï¸ Skipped (invalid data): 322/324 chips\n",
            "   Stitching 2 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Lakhimpur_Flood_Map.tif\n",
            "   ðŸ“Š Flooded area: 0.00% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Nalbari...\n",
            "   Processing 80 chip pairs...\n",
            "   âœ… Successfully processed: 80/80 chips\n",
            "   âš ï¸ Skipped (invalid data): 0/80 chips\n",
            "   Stitching 80 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Nalbari_Flood_Map.tif\n",
            "   ðŸ“Š Flooded area: 0.00% of total pixels\n",
            "\n",
            "ðŸŒŠ Processing Sonitpur...\n",
            "   Processing 198 chip pairs...\n",
            "   âœ… Successfully processed: 145/198 chips\n",
            "   âš ï¸ Skipped (invalid data): 53/198 chips\n",
            "   Stitching 148 mask chips...\n",
            "   âœ… Final flood map saved: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\\Sonitpur_Flood_Map.tif\n",
            "   ðŸ“Š Flooded area: 0.01% of total pixels\n",
            "\n",
            "======================================================================\n",
            "âœ… PHASE 1A COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "Outputs saved to:\n",
            "  Chip-level masks: C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks\n",
            "  Final flood maps: C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps\n",
            "\n",
            "IMPORTANT: If flood percentages seem too high (>50%):\n",
            "  1. Increase FIXED_THRESHOLD to -4.0 or -3.0\n",
            "  2. Increase MIN_OBJECT_SIZE to 200 or 500\n",
            "  3. Re-run the cell\n",
            "\n",
            "Next steps:\n",
            "  1. Open GeoTIFF files in QGIS for visual assessment\n",
            "  2. Adjust parameters if needed\n",
            "  3. Compare with known flood extent maps\n"
          ]
        }
      ],
      "source": [
        "# =================================================================\n",
        "# PHASE 1A: IMPROVED SAR CHANGE DETECTION WITH DESPECKLING\n",
        "# =================================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import rasterio\n",
        "from rasterio.merge import merge\n",
        "from scipy.ndimage import binary_opening, binary_closing, uniform_filter, label\n",
        "from skimage.filters import threshold_otsu\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =================================================================\n",
        "# CONFIGURATION\n",
        "# =================================================================\n",
        "\n",
        "OUTPUT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips'\n",
        "FLOOD_MASKS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks'\n",
        "FINAL_MAPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Final_Flood_Maps'\n",
        "\n",
        "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
        "\n",
        "# Change detection parameters\n",
        "CHANGE_METHOD = 'log_ratio'       # Options: 'log_ratio', 'difference', 'ratio'\n",
        "THRESHOLD_METHOD = 'fixed'        # CHANGED: Use fixed threshold for better control\n",
        "FIXED_THRESHOLD = -5.0            # CHANGED: More conservative (-5 dB is significant change)\n",
        "PERCENTILE_VALUE = 5              # Used if THRESHOLD_METHOD = 'percentile'\n",
        "\n",
        "# Speckle filtering (CRITICAL FOR SAR!)\n",
        "APPLY_SPECKLE_FILTER = True       # NEW: Apply Lee filter\n",
        "SPECKLE_FILTER_SIZE = 5           # NEW: 5x5 window for Lee filter\n",
        "\n",
        "# Post-processing (MORE AGGRESSIVE)\n",
        "MIN_OBJECT_SIZE = 100             # CHANGED: Remove objects < 100 pixels (was 10)\n",
        "APPLY_MORPHOLOGY = True           # Clean up with opening/closing\n",
        "MORPH_KERNEL_SIZE = 5             # CHANGED: Larger kernel (was 3)\n",
        "\n",
        "# Data validation\n",
        "MIN_VALID_PIXELS = 1000           # CHANGED: Need more valid pixels (was 100)\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# CORE FUNCTIONS\n",
        "# =================================================================\n",
        "\n",
        "def lee_filter(img, size=5):\n",
        "    \"\"\"\n",
        "    Apply Lee speckle filter to SAR image.\n",
        "    \n",
        "    The Lee filter:\n",
        "    - Smooths uniform areas (reduces speckle)\n",
        "    - Preserves edges and features\n",
        "    - Essential for SAR change detection\n",
        "    \"\"\"\n",
        "    img_mean = uniform_filter(img, size=size)\n",
        "    img_sqr_mean = uniform_filter(img**2, size=size)\n",
        "    img_variance = img_sqr_mean - img_mean**2\n",
        "    \n",
        "    overall_variance = np.var(img)\n",
        "    \n",
        "    # Avoid division by zero\n",
        "    img_variance = np.maximum(img_variance, 0)\n",
        "    \n",
        "    # Lee filter weighting\n",
        "    img_weights = img_variance / (img_variance + overall_variance + 1e-10)\n",
        "    img_filtered = img_mean + img_weights * (img - img_mean)\n",
        "    \n",
        "    return img_filtered\n",
        "\n",
        "\n",
        "def safe_log_ratio(pre_chip, post_chip, epsilon=1e-10):\n",
        "    \"\"\"Calculate log ratio between pre and post images.\"\"\"\n",
        "    pre_chip = np.where(pre_chip <= 0, epsilon, pre_chip)\n",
        "    post_chip = np.where(post_chip <= 0, epsilon, post_chip)\n",
        "    \n",
        "    log_ratio = 10 * np.log10(post_chip / pre_chip)\n",
        "    return log_ratio\n",
        "\n",
        "\n",
        "def calculate_change(pre_chip, post_chip, method='log_ratio'):\n",
        "    \"\"\"Calculate change between pre and post chips.\"\"\"\n",
        "    if method == 'log_ratio':\n",
        "        change = safe_log_ratio(pre_chip, post_chip)\n",
        "    elif method == 'difference':\n",
        "        change = post_chip - pre_chip\n",
        "    elif method == 'ratio':\n",
        "        epsilon = 1e-10\n",
        "        pre_chip = np.where(pre_chip <= 0, epsilon, pre_chip)\n",
        "        change = post_chip / pre_chip\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "    \n",
        "    return change\n",
        "\n",
        "\n",
        "def apply_threshold(change_map, method='fixed', fixed_value=-5.0, percentile=5):\n",
        "    \"\"\"Apply threshold to identify flooded areas.\"\"\"\n",
        "    valid_mask = np.isfinite(change_map)\n",
        "    valid_data = change_map[valid_mask]\n",
        "    \n",
        "    if len(valid_data) < 100:\n",
        "        raise ValueError(f\"Insufficient valid data: only {len(valid_data)} valid pixels\")\n",
        "    \n",
        "    if method == 'otsu':\n",
        "        threshold = threshold_otsu(valid_data)\n",
        "        flood_mask = change_map < threshold\n",
        "        \n",
        "    elif method == 'fixed':\n",
        "        # Fixed threshold - RECOMMENDED for SAR flood detection\n",
        "        flood_mask = change_map < fixed_value\n",
        "        \n",
        "    elif method == 'percentile':\n",
        "        # Use bottom percentile as threshold\n",
        "        threshold = np.percentile(valid_data, percentile)\n",
        "        flood_mask = change_map < threshold\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unknown threshold method: {method}\")\n",
        "    \n",
        "    # Set invalid pixels to 0 (non-flooded)\n",
        "    flood_mask = np.where(valid_mask, flood_mask, 0)\n",
        "    \n",
        "    return flood_mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def remove_small_objects(mask, min_size=100):\n",
        "    \"\"\"\n",
        "    Remove connected components smaller than min_size pixels.\n",
        "    \n",
        "    This is CRITICAL to remove speckle noise artifacts.\n",
        "    \"\"\"\n",
        "    # Label connected components\n",
        "    labeled_array, num_features = label(mask)\n",
        "    \n",
        "    # Remove small objects\n",
        "    for region_label in range(1, num_features + 1):\n",
        "        region_size = np.sum(labeled_array == region_label)\n",
        "        if region_size < min_size:\n",
        "            mask[labeled_array == region_label] = 0\n",
        "    \n",
        "    return mask\n",
        "\n",
        "\n",
        "def post_process_mask(mask, min_size=100, apply_morph=True, kernel_size=5):\n",
        "    \"\"\"Clean up the flood mask with advanced filtering.\"\"\"\n",
        "    \n",
        "    if apply_morph:\n",
        "        # Morphological opening: removes small objects and noise\n",
        "        kernel = np.ones((kernel_size, kernel_size))\n",
        "        mask = binary_opening(mask, structure=kernel)\n",
        "        \n",
        "        # Morphological closing: fills small holes\n",
        "        mask = binary_closing(mask, structure=kernel)\n",
        "    \n",
        "    # Remove small connected components\n",
        "    mask = remove_small_objects(mask, min_size=min_size)\n",
        "    \n",
        "    return mask.astype(np.uint8)\n",
        "\n",
        "\n",
        "def validate_chip_data(chip_array):\n",
        "    \"\"\"Validate chip data with stricter criteria.\"\"\"\n",
        "    nan_count = np.isnan(chip_array).sum()\n",
        "    total_pixels = chip_array.size\n",
        "    valid_pixels = total_pixels - nan_count\n",
        "    \n",
        "    inf_count = np.isinf(chip_array).sum()\n",
        "    \n",
        "    # Stricter validation\n",
        "    if valid_pixels < MIN_VALID_PIXELS:\n",
        "        return False, f\"Too few valid pixels: {valid_pixels}/{total_pixels}\"\n",
        "    \n",
        "    if nan_count > total_pixels * 0.5:  # CHANGED: More lenient (was 0.8)\n",
        "        return False, f\"Too many NaN values: {nan_count}/{total_pixels}\"\n",
        "    \n",
        "    if inf_count > 0:\n",
        "        return False, f\"Contains infinite values: {inf_count}\"\n",
        "    \n",
        "    # Check if all values are the same\n",
        "    unique_vals = np.unique(chip_array[~np.isnan(chip_array)])\n",
        "    if len(unique_vals) < 10:  # CHANGED: Need more variation (was 2)\n",
        "        return False, \"Insufficient variation in data\"\n",
        "    \n",
        "    return True, \"Valid\"\n",
        "\n",
        "\n",
        "def process_chip_pair(pre_path, post_path, output_path):\n",
        "    \"\"\"Process a single chip pair with SAR-specific preprocessing.\"\"\"\n",
        "    try:\n",
        "        with rasterio.open(pre_path) as src_pre:\n",
        "            pre_chip = src_pre.read(1).astype(np.float32)\n",
        "            profile = src_pre.profile.copy()\n",
        "        \n",
        "        with rasterio.open(post_path) as src_post:\n",
        "            post_chip = src_post.read(1).astype(np.float32)\n",
        "        \n",
        "        # Validate chips\n",
        "        is_valid_pre, msg_pre = validate_chip_data(pre_chip)\n",
        "        if not is_valid_pre:\n",
        "            raise ValueError(f\"Pre-flood: {msg_pre}\")\n",
        "        \n",
        "        is_valid_post, msg_post = validate_chip_data(post_chip)\n",
        "        if not is_valid_post:\n",
        "            raise ValueError(f\"Post-flood: {msg_post}\")\n",
        "        \n",
        "        # Replace NaN values\n",
        "        if np.isnan(pre_chip).any():\n",
        "            pre_mean = np.nanmean(pre_chip)\n",
        "            pre_chip = np.nan_to_num(pre_chip, nan=pre_mean)\n",
        "        \n",
        "        if np.isnan(post_chip).any():\n",
        "            post_mean = np.nanmean(post_chip)\n",
        "            post_chip = np.nan_to_num(post_chip, nan=post_mean)\n",
        "        \n",
        "        # STEP 1: Apply Lee speckle filter (CRITICAL!)\n",
        "        if APPLY_SPECKLE_FILTER:\n",
        "            pre_chip = lee_filter(pre_chip, size=SPECKLE_FILTER_SIZE)\n",
        "            post_chip = lee_filter(post_chip, size=SPECKLE_FILTER_SIZE)\n",
        "        \n",
        "        # STEP 2: Calculate change\n",
        "        change_map = calculate_change(pre_chip, post_chip, method=CHANGE_METHOD)\n",
        "        \n",
        "        # STEP 3: Apply threshold\n",
        "        flood_mask = apply_threshold(\n",
        "            change_map, \n",
        "            method=THRESHOLD_METHOD,\n",
        "            fixed_value=FIXED_THRESHOLD,\n",
        "            percentile=PERCENTILE_VALUE\n",
        "        )\n",
        "        \n",
        "        # STEP 4: Advanced post-processing\n",
        "        flood_mask = post_process_mask(\n",
        "            flood_mask,\n",
        "            min_size=MIN_OBJECT_SIZE,\n",
        "            apply_morph=APPLY_MORPHOLOGY,\n",
        "            kernel_size=MORPH_KERNEL_SIZE\n",
        "        )\n",
        "        \n",
        "        # STEP 5: Save the mask\n",
        "        profile.update(dtype=rasterio.uint8, count=1, compress='LZW')\n",
        "        \n",
        "        with rasterio.open(output_path, 'w', **profile) as dst:\n",
        "            dst.write(flood_mask, 1)\n",
        "        \n",
        "        return True\n",
        "        \n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "\n",
        "def stitch_flood_masks(mask_dir, district_name, output_dir):\n",
        "    \"\"\"Stitch all chip-level flood masks into a single district-level GeoTIFF.\"\"\"\n",
        "    \n",
        "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) \n",
        "                        if f.endswith('.tif')])\n",
        "    \n",
        "    if not mask_files:\n",
        "        print(f\"   âš ï¸ No mask files found in {mask_dir}\")\n",
        "        return\n",
        "    \n",
        "    print(f\"   Stitching {len(mask_files)} mask chips...\")\n",
        "    \n",
        "    sources = [rasterio.open(f) for f in mask_files]\n",
        "    stitched_array, out_transform = merge(sources)\n",
        "    \n",
        "    out_meta = sources[0].profile.copy()\n",
        "    out_meta.update({\n",
        "        \"driver\": \"GTiff\",\n",
        "        \"height\": stitched_array.shape[1],\n",
        "        \"width\": stitched_array.shape[2],\n",
        "        \"transform\": out_transform,\n",
        "        \"count\": 1,\n",
        "        \"dtype\": 'uint8',\n",
        "        \"compress\": 'LZW'\n",
        "    })\n",
        "    \n",
        "    output_path = os.path.join(output_dir, f'{district_name}_Flood_Map.tif')\n",
        "    with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
        "        dest.write(stitched_array)\n",
        "    \n",
        "    for src in sources:\n",
        "        src.close()\n",
        "    \n",
        "    print(f\"   âœ… Final flood map saved: {output_path}\")\n",
        "    \n",
        "    # Calculate flood statistics\n",
        "    total_pixels = stitched_array.size\n",
        "    flood_pixels = np.sum(stitched_array == 1)\n",
        "    flood_percentage = (flood_pixels / total_pixels) * 100\n",
        "    \n",
        "    # Sanity check warning\n",
        "    if flood_percentage > 50:\n",
        "        print(f\"   âš ï¸ WARNING: {flood_percentage:.2f}% flooded - this seems too high!\")\n",
        "        print(f\"   Consider increasing FIXED_THRESHOLD (try -4.0 or -3.0)\")\n",
        "    else:\n",
        "        print(f\"   ðŸ“Š Flooded area: {flood_percentage:.2f}% of total pixels\")\n",
        "\n",
        "\n",
        "# =================================================================\n",
        "# MAIN EXECUTION\n",
        "# =================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PHASE 1A: IMPROVED SAR CHANGE DETECTION WITH DESPECKLING\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nConfiguration:\")\n",
        "print(f\"  Change method: {CHANGE_METHOD}\")\n",
        "print(f\"  Threshold method: {THRESHOLD_METHOD}\")\n",
        "if THRESHOLD_METHOD == 'fixed':\n",
        "    print(f\"  Fixed threshold: {FIXED_THRESHOLD} dB\")\n",
        "elif THRESHOLD_METHOD == 'percentile':\n",
        "    print(f\"  Percentile: {PERCENTILE_VALUE}%\")\n",
        "print(f\"  Speckle filter: {'Enabled' if APPLY_SPECKLE_FILTER else 'Disabled'} (Lee {SPECKLE_FILTER_SIZE}x{SPECKLE_FILTER_SIZE})\")\n",
        "print(f\"  Post-processing: {'Enabled' if APPLY_MORPHOLOGY else 'Disabled'}\")\n",
        "print(f\"  Min object size: {MIN_OBJECT_SIZE} pixels\")\n",
        "print(f\"  Min valid pixels: {MIN_VALID_PIXELS}\")\n",
        "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "os.makedirs(FLOOD_MASKS_DIR, exist_ok=True)\n",
        "os.makedirs(FINAL_MAPS_DIR, exist_ok=True)\n",
        "\n",
        "for district in DISTRICTS:\n",
        "    print(f\"ðŸŒŠ Processing {district}...\")\n",
        "    \n",
        "    pre_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'pre_flood')\n",
        "    post_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'post_flood')\n",
        "    mask_dir = os.path.join(FLOOD_MASKS_DIR, district)\n",
        "    \n",
        "    if not os.path.exists(pre_dir) or not os.path.exists(post_dir):\n",
        "        print(f\"   âš ï¸ Skipping {district}: Chip directories not found\")\n",
        "        continue\n",
        "    \n",
        "    os.makedirs(mask_dir, exist_ok=True)\n",
        "    \n",
        "    pre_chips = sorted([f for f in os.listdir(pre_dir) if f.endswith('.tif')])\n",
        "    \n",
        "    if not pre_chips:\n",
        "        print(f\"   âš ï¸ No .tif files found in {pre_dir}\")\n",
        "        continue\n",
        "    \n",
        "    print(f\"   Processing {len(pre_chips)} chip pairs...\")\n",
        "    \n",
        "    success_count = 0\n",
        "    skip_count = 0\n",
        "    \n",
        "    for chip_name in pre_chips:\n",
        "        pre_path = os.path.join(pre_dir, chip_name)\n",
        "        post_chip_name = chip_name.replace('PreFlood_Image', 'PostFlood_Image')\n",
        "        post_path = os.path.join(post_dir, post_chip_name)\n",
        "        \n",
        "        if not os.path.exists(post_path):\n",
        "            skip_count += 1\n",
        "            continue\n",
        "        \n",
        "        mask_name = chip_name.replace('PreFlood_Image', 'Flood_Mask')\n",
        "        mask_path = os.path.join(mask_dir, mask_name)\n",
        "        \n",
        "        if process_chip_pair(pre_path, post_path, mask_path):\n",
        "            success_count += 1\n",
        "        else:\n",
        "            skip_count += 1\n",
        "    \n",
        "    print(f\"   âœ… Successfully processed: {success_count}/{len(pre_chips)} chips\")\n",
        "    print(f\"   âš ï¸ Skipped (invalid data): {skip_count}/{len(pre_chips)} chips\")\n",
        "    \n",
        "    if success_count > 0:\n",
        "        stitch_flood_masks(mask_dir, district, FINAL_MAPS_DIR)\n",
        "    else:\n",
        "        print(f\"   âš ï¸ No masks to stitch for {district}\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"âœ… PHASE 1A COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nOutputs saved to:\")\n",
        "print(f\"  Chip-level masks: {FLOOD_MASKS_DIR}\")\n",
        "print(f\"  Final flood maps: {FINAL_MAPS_DIR}\")\n",
        "print(\"\\nIMPORTANT: If flood percentages seem too high (>50%):\")\n",
        "print(\"  1. Increase FIXED_THRESHOLD to -4.0 or -3.0\")\n",
        "print(\"  2. Increase MIN_OBJECT_SIZE to 200 or 500\")\n",
        "print(\"  3. Re-run the cell\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"  1. Open GeoTIFF files in QGIS for visual assessment\")\n",
        "print(\"  2. Adjust parameters if needed\")\n",
        "print(\"  3. Compare with known flood extent maps\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "minor_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

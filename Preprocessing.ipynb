{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14cc1b2",
   "metadata": {},
   "source": [
    "# **Image Quality Filtering and Preprocessing**\n",
    "This notebook filters HLS (Harmonized Landsat Sentinel) 6-band optical images based on:\n",
    "- **File size**: Only processes images above a minimum size threshold\n",
    "- **Data validity**: Checks for corrupt or incomplete files\n",
    "- **Spatial dimensions**: Ensures images meet minimum resolution requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ec72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Fix OpenMP library conflict (Windows compatibility)\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.merge import merge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b385c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "   Input directory: C:\\Kaam_Dhanda\\Minor_Project\\Flood_Analysis_HLS_Exports-20251121T100600Z-1-001\\Flood_Analysis_HLS_Exports\n",
      "   Min file size: 5 MB\n",
      "   Min dimensions: 1000x1000 pixels\n",
      "   Chip size: 224x224 pixels\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# CONFIGURATION\n",
    "# =================================================================\n",
    "\n",
    "# Input directory with HLS 6-band images\n",
    "INPUT_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Analysis_HLS_Exports-20251121T100600Z-1-001\\Flood_Analysis_HLS_Exports'\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS'\n",
    "FILTERED_IMAGES_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Filtered_HLS_Images'\n",
    "\n",
    "# Quality thresholds\n",
    "MIN_FILE_SIZE_MB = 5          # Minimum file size in MB (filters out corrupted/incomplete files)\n",
    "MIN_WIDTH = 1000              # Minimum image width in pixels\n",
    "MIN_HEIGHT = 1000             # Minimum image height in pixels\n",
    "MAX_NODATA_PERCENT = 30       # Maximum percentage of no-data values allowed\n",
    "\n",
    "# Chipping parameters\n",
    "CHIP_SIZE = 224               # Standard size for deep learning (224x224 for vision models)\n",
    "MAX_NODATA_PER_CHIP = 20      # Maximum % of no-data allowed per chip\n",
    "\n",
    "# Districts to process\n",
    "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Input directory: {INPUT_DIR}\")\n",
    "print(f\"   Min file size: {MIN_FILE_SIZE_MB} MB\")\n",
    "print(f\"   Min dimensions: {MIN_WIDTH}x{MIN_HEIGHT} pixels\")\n",
    "print(f\"   Chip size: {CHIP_SIZE}x{CHIP_SIZE} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c5e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# IMAGE QUALITY VALIDATION FUNCTIONS\n",
    "# =================================================================\n",
    "\n",
    "def validate_image_quality(file_path, min_size_mb, min_width, min_height, max_nodata_percent):\n",
    "    \"\"\"\n",
    "    Validates an image file based on multiple quality criteria.\n",
    "    \n",
    "    Returns:\n",
    "        (bool, dict): (is_valid, metadata_dict)\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'file_path': file_path,\n",
    "        'file_size_mb': 0,\n",
    "        'width': 0,\n",
    "        'height': 0,\n",
    "        'bands': 0,\n",
    "        'nodata_percent': 0,\n",
    "        'dtype': None,\n",
    "        'crs': None,\n",
    "        'is_valid': False,\n",
    "        'rejection_reason': []\n",
    "    }\n",
    "    \n",
    "    # Check 1: File exists\n",
    "    if not os.path.exists(file_path):\n",
    "        results['rejection_reason'].append(\"File not found\")\n",
    "        return False, results\n",
    "    \n",
    "    # Check 2: File size\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    results['file_size_mb'] = round(file_size_mb, 2)\n",
    "    \n",
    "    if file_size_mb < min_size_mb:\n",
    "        results['rejection_reason'].append(f\"File too small ({file_size_mb:.2f} MB < {min_size_mb} MB)\")\n",
    "        return False, results\n",
    "    \n",
    "    # Check 3: Can open and read metadata\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            results['width'] = src.width\n",
    "            results['height'] = src.height\n",
    "            results['bands'] = src.count\n",
    "            results['dtype'] = str(src.dtypes[0])\n",
    "            results['crs'] = str(src.crs) if src.crs else \"None\"\n",
    "            \n",
    "            # Check dimensions\n",
    "            if src.width < min_width or src.height < min_height:\n",
    "                results['rejection_reason'].append(\n",
    "                    f\"Image too small ({src.width}x{src.height} < {min_width}x{min_height})\"\n",
    "                )\n",
    "                return False, results\n",
    "            \n",
    "            # Check 4: Sample data for no-data percentage (check first band)\n",
    "            sample_data = src.read(1)\n",
    "            \n",
    "            # Check for no-data values\n",
    "            if src.nodata is not None:\n",
    "                nodata_pixels = np.sum(sample_data == src.nodata)\n",
    "            else:\n",
    "                # If no nodata value specified, check for NaN or zeros\n",
    "                nodata_pixels = np.sum(np.isnan(sample_data)) + np.sum(sample_data == 0)\n",
    "            \n",
    "            total_pixels = sample_data.size\n",
    "            nodata_percent = (nodata_pixels / total_pixels) * 100\n",
    "            results['nodata_percent'] = round(nodata_percent, 2)\n",
    "            \n",
    "            if nodata_percent > max_nodata_percent:\n",
    "                results['rejection_reason'].append(\n",
    "                    f\"Too much no-data ({nodata_percent:.1f}% > {max_nodata_percent}%)\"\n",
    "                )\n",
    "                return False, results\n",
    "            \n",
    "    except Exception as e:\n",
    "        results['rejection_reason'].append(f\"Error reading file: {str(e)}\")\n",
    "        return False, results\n",
    "    \n",
    "    # If all checks pass\n",
    "    results['is_valid'] = True\n",
    "    return True, results\n",
    "\n",
    "\n",
    "def scan_and_filter_images(input_dir, districts, min_size_mb, min_width, min_height, max_nodata_percent):\n",
    "    \"\"\"\n",
    "    Scans all images in the directory and filters based on quality criteria.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {district: {'pre_flood': path, 'post_flood': path, 'metadata': {...}}}\n",
    "    \"\"\"\n",
    "    valid_images = {}\n",
    "    rejected_images = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCANNING AND VALIDATING IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for district in districts:\n",
    "        print(f\"\\nüìç District: {district}\")\n",
    "        \n",
    "        # Look for pre and post flood files\n",
    "        pre_file = os.path.join(input_dir, f'{district}_PreFlood_HLS_6Band.tif')\n",
    "        post_file = os.path.join(input_dir, f'{district}_PostFlood_HLS_6Band.tif')\n",
    "        \n",
    "        # Validate pre-flood image\n",
    "        pre_valid, pre_results = validate_image_quality(\n",
    "            pre_file, min_size_mb, min_width, min_height, max_nodata_percent\n",
    "        )\n",
    "        \n",
    "        # Validate post-flood image\n",
    "        post_valid, post_results = validate_image_quality(\n",
    "            post_file, min_size_mb, min_width, min_height, max_nodata_percent\n",
    "        )\n",
    "        \n",
    "        # Both must be valid to include the district\n",
    "        if pre_valid and post_valid:\n",
    "            valid_images[district] = {\n",
    "                'pre_flood': pre_file,\n",
    "                'post_flood': post_file,\n",
    "                'pre_metadata': pre_results,\n",
    "                'post_metadata': post_results\n",
    "            }\n",
    "            print(f\"   ‚úÖ ACCEPTED\")\n",
    "            print(f\"      Pre-flood:  {pre_results['width']}x{pre_results['height']}, \"\n",
    "                  f\"{pre_results['file_size_mb']} MB, {pre_results['nodata_percent']}% no-data\")\n",
    "            print(f\"      Post-flood: {post_results['width']}x{post_results['height']}, \"\n",
    "                  f\"{post_results['file_size_mb']} MB, {post_results['nodata_percent']}% no-data\")\n",
    "        else:\n",
    "            rejected_images[district] = {\n",
    "                'pre_results': pre_results,\n",
    "                'post_results': post_results\n",
    "            }\n",
    "            print(f\"   ‚ùå REJECTED\")\n",
    "            if not pre_valid:\n",
    "                print(f\"      Pre-flood issues: {', '.join(pre_results['rejection_reason'])}\")\n",
    "            if not post_valid:\n",
    "                print(f\"      Post-flood issues: {', '.join(post_results['rejection_reason'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Valid districts: {len(valid_images)}/{len(districts)}\")\n",
    "    print(f\"‚ùå Rejected districts: {len(rejected_images)}/{len(districts)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return valid_images, rejected_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806f37ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCANNING AND VALIDATING IMAGES\n",
      "======================================================================\n",
      "\n",
      "üìç District: Barpeta\n",
      "   ‚úÖ ACCEPTED\n",
      "      Pre-flood:  1979x1326, 52.16 MB, 3.33% no-data\n",
      "      Post-flood: 1979x1326, 49.03 MB, 7.16% no-data\n",
      "\n",
      "üìç District: Dhemaji\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: File too small (0.40 MB < 5 MB)\n",
      "\n",
      "üìç District: Lakhimpur\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: Too much no-data (94.3% > 30%)\n",
      "\n",
      "üìç District: Nalbari\n",
      "   ‚úÖ ACCEPTED\n",
      "      Pre-flood:  1559x1300, 40.97 MB, 2.08% no-data\n",
      "      Post-flood: 1559x1300, 37.39 MB, 10.68% no-data\n",
      "\n",
      "üìç District: Sonitpur\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: Too much no-data (43.5% > 30%)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Valid districts: 2/5\n",
      "‚ùå Rejected districts: 3/5\n",
      "======================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "\n",
      "‚úÖ Valid districts ready for processing:\n",
      "   ‚Ä¢ Barpeta\n",
      "   ‚Ä¢ Nalbari\n",
      "\n",
      "‚ùå Rejected districts:\n",
      "   ‚Ä¢ Dhemaji\n",
      "   ‚Ä¢ Lakhimpur\n",
      "   ‚Ä¢ Sonitpur\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# RUN IMAGE QUALITY SCAN\n",
    "# =================================================================\n",
    "\n",
    "# Scan and filter images based on quality criteria\n",
    "valid_images, rejected_images = scan_and_filter_images(\n",
    "    INPUT_DIR,\n",
    "    DISTRICTS,\n",
    "    MIN_FILE_SIZE_MB,\n",
    "    MIN_WIDTH,\n",
    "    MIN_HEIGHT,\n",
    "    MAX_NODATA_PERCENT\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä SUMMARY:\")\n",
    "if valid_images:\n",
    "    print(\"\\n‚úÖ Valid districts ready for processing:\")\n",
    "    for district in valid_images.keys():\n",
    "        print(f\"   ‚Ä¢ {district}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No valid images found! Please check your quality thresholds.\")\n",
    "\n",
    "if rejected_images:\n",
    "    print(\"\\n‚ùå Rejected districts:\")\n",
    "    for district in rejected_images.keys():\n",
    "        print(f\"   ‚Ä¢ {district}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a42834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# CHIPPING FUNCTION FOR VALID IMAGES\n",
    "# =================================================================\n",
    "\n",
    "def chip_image_with_quality_check(input_filepath, output_directory, chip_size, max_nodata_percent):\n",
    "    \"\"\"\n",
    "    Cuts a large GeoTIFF into smaller chips, only saving high-quality chips.\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of valid chips created\n",
    "    \"\"\"\n",
    "    try:\n",
    "        src = rasterio.open(input_filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error opening file: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    width = src.width\n",
    "    height = src.height\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"   Processing {os.path.basename(input_filepath)}...\")\n",
    "    print(f\"   Image dimensions: {width}x{height}, {src.count} bands\")\n",
    "    \n",
    "    # Loop through the image in chunks\n",
    "    for i in range(0, height, chip_size):\n",
    "        for j in range(0, width, chip_size):\n",
    "            \n",
    "            # Define the window\n",
    "            window = Window(j, i, min(chip_size, width - j), min(chip_size, height - i))\n",
    "            transform = src.window_transform(window)\n",
    "            \n",
    "            # Read all bands for this chip\n",
    "            chip_data = src.read(window=window)\n",
    "            \n",
    "            # Quality check 1: Check chip size (skip edge chips that are too small)\n",
    "            if window.height < chip_size * 0.5 or window.width < chip_size * 0.5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Quality check 2: Check for no-data percentage (check first band as representative)\n",
    "            first_band = chip_data[0]\n",
    "            \n",
    "            if src.nodata is not None:\n",
    "                nodata_pixels = np.sum(first_band == src.nodata)\n",
    "            else:\n",
    "                nodata_pixels = np.sum(np.isnan(first_band)) + np.sum(first_band == 0)\n",
    "            \n",
    "            total_pixels = first_band.size\n",
    "            nodata_percent = (nodata_pixels / total_pixels) * 100\n",
    "            \n",
    "            if nodata_percent > max_nodata_percent:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Quality check 3: Check for data variation (avoid uniform chips)\n",
    "            if np.std(first_band) < 0.01:  # Very low standard deviation\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Update metadata for the chip\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                'height': window.height,\n",
    "                'width': window.width,\n",
    "                'transform': transform,\n",
    "                'compress': 'LZW'\n",
    "            })\n",
    "            \n",
    "            # Save the chip\n",
    "            base_filename = os.path.basename(input_filepath)\n",
    "            file_stem = base_filename.replace('.tif', '')\n",
    "            chip_filename = f'{file_stem}_chip_{count}.tif'\n",
    "            output_path = os.path.join(output_directory, chip_filename)\n",
    "            \n",
    "            try:\n",
    "                with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                    dst.write(chip_data)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to write chip: {e}\")\n",
    "    \n",
    "    src.close()\n",
    "    \n",
    "    print(f\"   ‚úÖ Created {count} valid chips (skipped {skipped} low-quality chips)\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da86c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CHIPPING VALID IMAGES\n",
      "======================================================================\n",
      "\n",
      "üèûÔ∏è Processing Barpeta...\n",
      "\n",
      "   Pre-flood image:\n",
      "   Processing Barpeta_PreFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1979x1326, 6 bands\n",
      "   ‚úÖ Created 52 valid chips (skipped 2 low-quality chips)\n",
      "\n",
      "   Post-flood image:\n",
      "   Processing Barpeta_PostFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1979x1326, 6 bands\n",
      "   ‚úÖ Created 53 valid chips (skipped 1 low-quality chips)\n",
      "\n",
      "üèûÔ∏è Processing Nalbari...\n",
      "\n",
      "   Pre-flood image:\n",
      "   Processing Nalbari_PreFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1559x1300, 6 bands\n",
      "   ‚úÖ Created 42 valid chips (skipped 0 low-quality chips)\n",
      "\n",
      "   Post-flood image:\n",
      "   Processing Nalbari_PostFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1559x1300, 6 bands\n",
      "   ‚úÖ Created 38 valid chips (skipped 4 low-quality chips)\n",
      "\n",
      "======================================================================\n",
      "üìä CHIPPING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Barpeta:\n",
      "   Pre-flood:  52 chips\n",
      "   Post-flood: 53 chips\n",
      "   Total:      105 chips\n",
      "\n",
      "Nalbari:\n",
      "   Pre-flood:  42 chips\n",
      "   Post-flood: 38 chips\n",
      "   Total:      80 chips\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Total chips created: 185\n",
      "üìÅ Output directory: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# PROCESS VALID IMAGES - CREATE CHIPS\n",
    "# =================================================================\n",
    "\n",
    "if not valid_images:\n",
    "    print(\"‚ö†Ô∏è No valid images to process. Adjust quality thresholds if needed.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CHIPPING VALID IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(OUTPUT_CHIPS_DIR, exist_ok=True)\n",
    "    \n",
    "    chip_statistics = {}\n",
    "    \n",
    "    for district, files in valid_images.items():\n",
    "        print(f\"\\nüèûÔ∏è Processing {district}...\")\n",
    "        \n",
    "        # Create district output directories\n",
    "        pre_output_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'pre_flood')\n",
    "        post_output_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'post_flood')\n",
    "        \n",
    "        os.makedirs(pre_output_dir, exist_ok=True)\n",
    "        os.makedirs(post_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Chip pre-flood image\n",
    "        print(f\"\\n   Pre-flood image:\")\n",
    "        pre_chip_count = chip_image_with_quality_check(\n",
    "            files['pre_flood'],\n",
    "            pre_output_dir,\n",
    "            CHIP_SIZE,\n",
    "            MAX_NODATA_PER_CHIP\n",
    "        )\n",
    "        \n",
    "        # Chip post-flood image\n",
    "        print(f\"\\n   Post-flood image:\")\n",
    "        post_chip_count = chip_image_with_quality_check(\n",
    "            files['post_flood'],\n",
    "            post_output_dir,\n",
    "            CHIP_SIZE,\n",
    "            MAX_NODATA_PER_CHIP\n",
    "        )\n",
    "        \n",
    "        chip_statistics[district] = {\n",
    "            'pre_flood_chips': pre_chip_count,\n",
    "            'post_flood_chips': post_chip_count\n",
    "        }\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä CHIPPING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_chips = 0\n",
    "    for district, stats in chip_statistics.items():\n",
    "        district_total = stats['pre_flood_chips'] + stats['post_flood_chips']\n",
    "        total_chips += district_total\n",
    "        print(f\"\\n{district}:\")\n",
    "        print(f\"   Pre-flood:  {stats['pre_flood_chips']} chips\")\n",
    "        print(f\"   Post-flood: {stats['post_flood_chips']} chips\")\n",
    "        print(f\"   Total:      {district_total} chips\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ Total chips created: {total_chips}\")\n",
    "    print(f\"üìÅ Output directory: {OUTPUT_CHIPS_DIR}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41511748",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional: Copy Valid Full-Size Images\n",
    "You can copy the validated full-size images to a separate directory for archival purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771564a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Copying validated full-size images...\n",
      "   ‚úÖ Copied Barpeta images\n",
      "   ‚úÖ Copied Nalbari images\n",
      "\n",
      "‚úÖ All validated images copied to: C:\\Kaam_Dhanda\\Minor_Project\\Filtered_HLS_Images\n"
     ]
    }
   ],
   "source": [
    "# Optional: Copy validated full-size images to a separate directory\n",
    "import shutil\n",
    "\n",
    "if valid_images:\n",
    "    print(\"üì¶ Copying validated full-size images...\")\n",
    "    os.makedirs(FILTERED_IMAGES_DIR, exist_ok=True)\n",
    "    \n",
    "    for district, files in valid_images.items():\n",
    "        # Copy pre-flood image\n",
    "        pre_dest = os.path.join(FILTERED_IMAGES_DIR, f'{district}_PreFlood_HLS_6Band.tif')\n",
    "        shutil.copy2(files['pre_flood'], pre_dest)\n",
    "        \n",
    "        # Copy post-flood image\n",
    "        post_dest = os.path.join(FILTERED_IMAGES_DIR, f'{district}_PostFlood_HLS_6Band.tif')\n",
    "        shutil.copy2(files['post_flood'], post_dest)\n",
    "        \n",
    "        print(f\"   ‚úÖ Copied {district} images\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All validated images copied to: {FILTERED_IMAGES_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid images to copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109ffa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch HLS tensor conversion from: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS\n",
      "\n",
      "=======================================================\n",
      "‚úÖ Tensor Conversion Complete. Total Tensors Created: 185\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# =======================================================================\n",
    "# CONFIGURATION - HLS/SENTINEL-2 PARAMETERS (for Prithvi)\n",
    "# =======================================================================\n",
    "\n",
    "# NOTE: These are official normalization parameters for HLS (Sentinel-2) data \n",
    "# scaled to 0-10000, which are commonly used with Prithvi models.\n",
    "HLS_NORM_MEANS = np.array([1353, 1146, 989, 2661, 2378, 1782], dtype=np.float32) \n",
    "HLS_NORM_STDS = np.array([870, 891, 1007, 1251, 1251, 1140], dtype=np.float32)\n",
    "SCALE_FACTOR = 10000.0 \n",
    "\n",
    "# IMPORTANT: SET YOUR ROOT DIRECTORY\n",
    "ROOT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS'\n",
    "\n",
    "# Dictionary to store all processed tensors\n",
    "processed_tensors = {}\n",
    "\n",
    "# =======================================================================\n",
    "# CORE PROCESSING FUNCTION\n",
    "# =======================================================================\n",
    "\n",
    "def preprocess_hls_chip(file_path, means, stds, scale_factor, target_size=224):\n",
    "    \"\"\"\n",
    "    Reads a 6-band HLS GeoTIFF, normalizes it, and converts it to a \n",
    "    PyTorch Tensor (1, C=6, H, W). Resizes to target_size x target_size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Read all 6 bands: (C, H, W)\n",
    "            data = src.read().astype(np.float32)\n",
    "            \n",
    "            if data.shape[0] != 6:\n",
    "                raise ValueError(f\"Skipping {os.path.basename(file_path)}: Expected 6 bands, found {data.shape[0]}.\")\n",
    "\n",
    "    except rasterio.RasterioIOError:\n",
    "        print(f\"Error: Could not open or read {file_path}.\")\n",
    "        return None\n",
    "\n",
    "    # 1. Scaling (converting 0-10000 range to 0-1 range)\n",
    "    data = np.clip(data, 0, 10000) / scale_factor \n",
    "    \n",
    "    # 2. Normalization (Z-Score)\n",
    "    # Reshape means/stds for broadcasting: (Channels, 1, 1)\n",
    "    means_reshaped = means.reshape(6, 1, 1) / scale_factor\n",
    "    stds_reshaped = stds.reshape(6, 1, 1) / scale_factor\n",
    "    \n",
    "    normalized_data = (data - means_reshaped) / stds_reshaped\n",
    "\n",
    "    # 3. Convert to PyTorch Tensor and add batch dimension (1, C, H, W)\n",
    "    tensor = torch.from_numpy(normalized_data)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    \n",
    "    # 4. Resize to consistent size if needed (handles edge chips)\n",
    "    if tensor.shape[2] != target_size or tensor.shape[3] != target_size:\n",
    "        tensor = torch.nn.functional.interpolate(\n",
    "            tensor,\n",
    "            size=(target_size, target_size),\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "    return tensor\n",
    "\n",
    "# =======================================================================\n",
    "# BATCH EXECUTION (Populates the processed_tensors dictionary)\n",
    "# =======================================================================\n",
    "\n",
    "print(f\"Starting batch HLS tensor conversion from: {ROOT_CHIPS_DIR}\")\n",
    "\n",
    "for district_name in os.listdir(ROOT_CHIPS_DIR):\n",
    "    district_path = os.path.join(ROOT_CHIPS_DIR, district_name)\n",
    "    \n",
    "    if not os.path.isdir(district_path):\n",
    "        continue\n",
    "\n",
    "    processed_tensors[district_name] = {'pre_flood': [], 'post_flood': []}\n",
    "    \n",
    "    for phase in ['pre_flood', 'post_flood']:\n",
    "        phase_path = os.path.join(district_path, phase)\n",
    "        \n",
    "        if not os.path.isdir(phase_path):\n",
    "            continue\n",
    "\n",
    "        for chip_filename in os.listdir(phase_path):\n",
    "            if chip_filename.endswith('.tif'):\n",
    "                chip_file_path = os.path.join(phase_path, chip_filename)\n",
    "                \n",
    "                # Run the core pre-processing function with target size\n",
    "                tensor = preprocess_hls_chip(\n",
    "                    chip_file_path, HLS_NORM_MEANS, HLS_NORM_STDS, SCALE_FACTOR, target_size=224\n",
    "                )\n",
    "                \n",
    "                if tensor is not None:\n",
    "                    # Store the resulting tensor\n",
    "                    processed_tensors[district_name][phase].append(tensor)\n",
    "\n",
    "# --- Final Check ---\n",
    "total_tensors = sum(len(p['pre_flood']) + len(p['post_flood']) for d, p in processed_tensors.items())\n",
    "print(f\"\\n=======================================================\")\n",
    "print(f\"‚úÖ Tensor Conversion Complete. Total Tensors Created: {total_tensors}\")\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9201d4c",
   "metadata": {},
   "source": [
    "# Fine Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce28d8",
   "metadata": {},
   "source": [
    "## **Step 1: Load Ground Truth Labels**\n",
    "Before fine-tuning, you need to prepare your ground truth flood masks. These should be binary masks where:\n",
    "- **0** = Non-flooded areas\n",
    "- **1** = Flooded areas\n",
    "\n",
    "You can create these using:\n",
    "- Manual annotation in QGIS\n",
    "- SAR-based change detection masks (from your previous work)\n",
    "- Combination of multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "297e322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PAIRING IMAGES WITH GROUND TRUTH LABELS\n",
      "======================================================================\n",
      "\n",
      "üìç District: Barpeta\n",
      "   ‚úÖ Paired 53 image-mask pairs\n",
      "\n",
      "üìç District: Nalbari\n",
      "   ‚úÖ Paired 38 image-mask pairs\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Total training pairs: 91\n",
      "======================================================================\n",
      "\n",
      "üìä Training data ready:\n",
      "   Images: 91 samples\n",
      "   Labels: 91 samples\n",
      "   Image shape: torch.Size([1, 6, 224, 224])\n",
      "   Label shape: torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# LOAD GROUND TRUTH LABELS\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure paths to your ground truth masks\n",
    "GROUND_TRUTH_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks'  # Directory with your SAR-based masks\n",
    "LABEL_CHIP_SIZE = 224  # Should match your HLS chip size\n",
    "\n",
    "def load_ground_truth_chip(mask_path, target_size=224):\n",
    "    \"\"\"\n",
    "    Load a ground truth mask chip and convert to tensor.\n",
    "    \n",
    "    Args:\n",
    "        mask_path: Path to the mask GeoTIFF\n",
    "        target_size: Expected size (will resize if needed)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (1, 1, H, W) with binary values {0, 1}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1).astype(np.float32)\n",
    "            \n",
    "            # Ensure binary values (0 and 1)\n",
    "            mask = (mask > 0).astype(np.float32)\n",
    "            \n",
    "            # Convert to tensor: (H, W) -> (1, 1, H, W)\n",
    "            tensor = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # Resize if needed\n",
    "            if mask.shape[0] != target_size or mask.shape[1] != target_size:\n",
    "                tensor = torch.nn.functional.interpolate(\n",
    "                    tensor, \n",
    "                    size=(target_size, target_size), \n",
    "                    mode='nearest'\n",
    "                )\n",
    "            \n",
    "            return tensor\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mask {mask_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def pair_images_with_labels(image_tensors_dict, ground_truth_dir):\n",
    "    \"\"\"\n",
    "    Match HLS image chips with corresponding ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        image_tensors_dict: The processed_tensors dictionary from previous cells\n",
    "        ground_truth_dir: Directory containing ground truth masks\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matched_images, matched_labels) as lists of tensors\n",
    "    \"\"\"\n",
    "    matched_images = []\n",
    "    matched_labels = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PAIRING IMAGES WITH GROUND TRUTH LABELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for district, phases in image_tensors_dict.items():\n",
    "        print(f\"\\nüìç District: {district}\")\n",
    "        \n",
    "        # We'll use post-flood images for training (when flooding is visible)\n",
    "        post_flood_tensors = phases.get('post_flood', [])\n",
    "        \n",
    "        # Look for corresponding mask directory\n",
    "        mask_dir = Path(ground_truth_dir) / district\n",
    "        \n",
    "        if not mask_dir.exists():\n",
    "            print(f\"   ‚ö†Ô∏è No mask directory found for {district}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all mask files\n",
    "        mask_files = sorted(mask_dir.glob('*Flood_Mask*.tif'))\n",
    "        \n",
    "        if not mask_files:\n",
    "            print(f\"   ‚ö†Ô∏è No mask files found in {mask_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Match images with masks (assuming same naming convention)\n",
    "        pairs_found = 0\n",
    "        for img_idx, img_tensor in enumerate(post_flood_tensors):\n",
    "            # Try to find corresponding mask\n",
    "            if img_idx < len(mask_files):\n",
    "                mask_tensor = load_ground_truth_chip(str(mask_files[img_idx]))\n",
    "                \n",
    "                if mask_tensor is not None:\n",
    "                    matched_images.append(img_tensor)\n",
    "                    matched_labels.append(mask_tensor)\n",
    "                    pairs_found += 1\n",
    "        \n",
    "        print(f\"   ‚úÖ Paired {pairs_found} image-mask pairs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Total training pairs: {len(matched_images)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return matched_images, matched_labels\n",
    "\n",
    "\n",
    "# Execute pairing (using the processed_tensors from cell 9)\n",
    "if 'processed_tensors' in globals() and processed_tensors:\n",
    "    train_images, train_labels = pair_images_with_labels(processed_tensors, GROUND_TRUTH_DIR)\n",
    "    \n",
    "    if train_images and train_labels:\n",
    "        print(f\"\\nüìä Training data ready:\")\n",
    "        print(f\"   Images: {len(train_images)} samples\")\n",
    "        print(f\"   Labels: {len(train_labels)} samples\")\n",
    "        print(f\"   Image shape: {train_images[0].shape}\")\n",
    "        print(f\"   Label shape: {train_labels[0].shape}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No training pairs found. Please check your ground truth directory.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run cell 9 first to generate processed_tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023809d",
   "metadata": {},
   "source": [
    "## **Step 2: Define the Fine-Tuning Model**\n",
    "We'll create a segmentation model using a pre-trained encoder (backbone) and add a decoder for flood detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d511f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# FLOOD SEGMENTATION MODEL\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp  # Popular library for segmentation\n",
    "\n",
    "class FloodSegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Flood detection segmentation model using a pre-trained encoder.\n",
    "    Uses U-Net architecture with a ResNet or EfficientNet encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name='resnet34', encoder_weights='imagenet', in_channels=6, classes=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_name: Backbone encoder (resnet34, resnet50, efficientnet-b0, etc.)\n",
    "            encoder_weights: Pre-trained weights ('imagenet' or None)\n",
    "            in_channels: Number of input channels (6 for HLS)\n",
    "            classes: Number of output classes (1 for binary segmentation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create U-Net model with pre-trained encoder\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            activation=None  # We'll apply sigmoid during training/inference\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Alternative: Use DeepLabV3+ for better performance\n",
    "class FloodDeepLabModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative model using DeepLabV3+ architecture.\n",
    "    Generally performs better than U-Net for segmentation tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name='resnet50', encoder_weights='imagenet', in_channels=6, classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "print(\"üîß Initializing flood segmentation model...\")\n",
    "\n",
    "# Choose your model architecture\n",
    "MODEL_TYPE = 'unet'  # Options: 'unet' or 'deeplabv3plus'\n",
    "ENCODER = 'resnet34'  # Options: 'resnet34', 'resnet50', 'efficientnet-b0', 'mobilenet_v2'\n",
    "INPUT_CHANNELS = 6  # HLS has 6 bands\n",
    "OUTPUT_CLASSES = 1  # Binary segmentation (flood vs non-flood)\n",
    "\n",
    "if MODEL_TYPE == 'unet':\n",
    "    model = FloodSegmentationModel(\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=INPUT_CHANNELS,\n",
    "        classes=OUTPUT_CLASSES\n",
    "    )\n",
    "else:\n",
    "    model = FloodDeepLabModel(\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=INPUT_CHANNELS,\n",
    "        classes=OUTPUT_CLASSES\n",
    "    )\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model initialized: {MODEL_TYPE.upper()} with {ENCODER} encoder\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Input channels: {INPUT_CHANNELS}\")\n",
    "print(f\"   Output classes: {OUTPUT_CLASSES}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41847bee",
   "metadata": {},
   "source": [
    "## **Step 3: Setup Training Pipeline**\n",
    "Configure the dataset, data loaders, loss function, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86637daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SETTING UP TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset split:\n",
      "   Training samples: 72\n",
      "   Validation samples: 19\n",
      "\n",
      "üì¶ Data loaders created:\n",
      "   Batch size: 8\n",
      "   Training batches: 9\n",
      "   Validation batches: 3\n",
      "\n",
      "üéØ Loss function: Combined (BCE + Dice)\n",
      "\n",
      "‚öôÔ∏è Optimizer: AdamW\n",
      "   Learning rate: 0.0001\n",
      "   Weight decay: 1e-05\n",
      "   LR scheduler: ReduceLROnPlateau\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Training pipeline ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# TRAINING SETUP\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "# -------------------------\n",
    "# Custom Dataset\n",
    "# -------------------------\n",
    "class FloodDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for flood detection training.\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of image tensors (1, C, H, W)\n",
    "            labels: List of label tensors (1, 1, H, W)\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "        assert len(images) == len(labels), \"Images and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Remove batch dimension for DataLoader (it will add it back)\n",
    "        image = self.images[idx].squeeze(0)  # (C, H, W)\n",
    "        label = self.labels[idx].squeeze(0)  # (1, H, W)\n",
    "        \n",
    "        # Ensure consistent size (224x224) - resize if needed\n",
    "        if image.shape[1] != 224 or image.shape[2] != 224:\n",
    "            image = torch.nn.functional.interpolate(\n",
    "                image.unsqueeze(0), size=(224, 224), mode='bilinear', align_corners=False\n",
    "            ).squeeze(0)\n",
    "        \n",
    "        if label.shape[1] != 224 or label.shape[2] != 224:\n",
    "            label = torch.nn.functional.interpolate(\n",
    "                label.unsqueeze(0), size=(224, 224), mode='nearest'\n",
    "            ).squeeze(0)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Loss Functions\n",
    "# -------------------------\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for segmentation tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combination of BCE and Dice loss for better training.\"\"\"\n",
    "    \n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce = self.bce_loss(pred, target)\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        return self.bce_weight * bce + self.dice_weight * dice\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Setup Training Components\n",
    "# -------------------------\n",
    "\n",
    "# Check if we have training data\n",
    "if 'train_images' not in globals() or not train_images:\n",
    "    print(\"‚ö†Ô∏è No training data found. Please run the ground truth loading cell first.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SETTING UP TRAINING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = FloodDataset(train_images, train_labels)\n",
    "    \n",
    "    # Split into train and validation (80/20 split)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"\\nüìä Dataset split:\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
    "    NUM_WORKERS = 0  # Set to 0 on Windows to avoid multiprocessing issues\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüì¶ Data loaders created:\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   Training batches: {len(train_loader)}\")\n",
    "    print(f\"   Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
    "    print(f\"\\nüéØ Loss function: Combined (BCE + Dice)\")\n",
    "    \n",
    "    # Define optimizer\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Optimizer: AdamW\")\n",
    "    print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
    "    print(f\"   LR scheduler: ReduceLROnPlateau\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Training pipeline ready!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00de6e",
   "metadata": {},
   "source": [
    "## **Step 4: Training Loop**\n",
    "Execute the fine-tuning process with validation and checkpoint saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ee4cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING FINE-TUNING\n",
      "======================================================================\n",
      "Epochs: 50\n",
      "Device: cpu\n",
      "Checkpoint directory: C:\\Kaam_Dhanda\\Minor_Project\\model_checkpoints\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üìÖ Epoch 1/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:09<00:00,  1.04s/it, loss=nan, iou=2.8e-8] \n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:09<00:00,  1.04s/it, loss=nan, iou=2.8e-8] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.59it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.59it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 1 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 2/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.02it/s, loss=nan, iou=2.71e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.02it/s, loss=nan, iou=2.71e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.96it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.96it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 2 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 3/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.07it/s, loss=nan, iou=4.58e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.07it/s, loss=nan, iou=4.58e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.17it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.17it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 3 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 4/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.03it/s, loss=nan, iou=2.99e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.03it/s, loss=nan, iou=2.99e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.73it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.73it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 4 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 5/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.01it/s, loss=nan, iou=3.99e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.01it/s, loss=nan, iou=3.99e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.80it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.80it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 5 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 6/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.05it/s, loss=nan, iou=3.43e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.05it/s, loss=nan, iou=3.43e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.07it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.07it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 6 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 7/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.06it/s, loss=nan, iou=2.62e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.06it/s, loss=nan, iou=2.62e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.81it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.81it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 7 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 8/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.03it/s, loss=nan, iou=4.7e-8] \n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.03it/s, loss=nan, iou=4.7e-8] \n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.87it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.87it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 8 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 9/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.03it/s, loss=nan, iou=2.68e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.03it/s, loss=nan, iou=2.68e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.21it/s, loss=nan, iou=8.76e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  4.21it/s, loss=nan, iou=8.76e-8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 9 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "üìÖ Epoch 10/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.07it/s, loss=nan, iou=2.86e-8]\n",
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:08<00:00,  1.07it/s, loss=nan, iou=2.86e-8]\n",
      "Validation: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  3.97it/s, loss=nan, iou=8.76e-8]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Epoch 10 Summary:\n",
      "¬† ¬†Train Loss: nan | Train IoU: 0.0000\n",
      "¬† ¬†Val Loss:¬† ¬†nan | Val IoU:¬† ¬†0.0000\n",
      "\n",
      "‚ö†Ô∏è Early stopping triggered after 10 epochs\n",
      "\n",
      "======================================================================\n",
      "‚úÖ FINE-TUNING COMPLETE!\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# TRAINING LOOP (Modified to fix multiprocessing error)\n",
    "# =================================================================\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss # Use this for the criterion\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau # For scheduler\n",
    "\n",
    "# Check if required objects are available\n",
    "if 'model' not in globals():\n",
    "    print(\"‚ö†Ô∏è Model not found. Please run Step 2 (Define Model) first.\")\n",
    "elif 'train_loader' not in globals() or 'val_loader' not in globals():\n",
    "    print(\"‚ö†Ô∏è Data loaders not found. Please run Step 3 (Setup Training Pipeline) first.\")\n",
    "elif 'criterion' not in globals():\n",
    "    print(\"‚ö†Ô∏è Loss function not found. Please run Step 3 (Setup Training Pipeline) first.\")\n",
    "elif 'optimizer' not in globals():\n",
    "    print(\"‚ö†Ô∏è Optimizer not found. Please run Step 3 (Setup Training Pipeline) first.\")\n",
    "elif 'scheduler' not in globals():\n",
    "    print(\"‚ö†Ô∏è Scheduler not found. Please run Step 3 (Setup Training Pipeline) first.\")\n",
    "else:\n",
    "    # Create checkpoint directory\n",
    "    CHECKPOINT_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\model_checkpoints'\n",
    "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "    \n",
    "    # Training configuration\n",
    "    NUM_EPOCHS = 50\n",
    "    EARLY_STOP_PATIENCE = 10\n",
    "    \n",
    "    # Metrics tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    \n",
    "    def calculate_iou(pred, target, threshold=0.5):\n",
    "        \"\"\"Calculate Intersection over Union (IoU) metric.\"\"\"\n",
    "        pred = (torch.sigmoid(pred) > threshold).float()\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        union = pred.sum() + target.sum() - intersection\n",
    "        \n",
    "        # Add epsilon for numerical stability\n",
    "        iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "        return iou.item()\n",
    "    \n",
    "    \n",
    "    def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "        \"\"\"Train for one epoch.\"\"\"\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc='Training')\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device).float() \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Loss calculation - model outputs (B, 1, H, W), labels are (B, 1, H, W)\n",
    "            loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_iou += calculate_iou(outputs.squeeze(1), labels.squeeze(1))\n",
    "            \n",
    "            pbar.set_postfix({'loss': running_loss / (batch_idx + 1), 'iou': running_iou / (batch_idx + 1)})\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_iou = running_iou / len(train_loader)\n",
    "        \n",
    "        return epoch_loss, epoch_iou\n",
    "    \n",
    "    \n",
    "    def validate(model, val_loader, criterion, device):\n",
    "        \"\"\"Validate the model.\"\"\"\n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        running_iou = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(val_loader, desc='Validation')\n",
    "            for batch_idx, (images, labels) in enumerate(pbar):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device).float()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                val_loss = criterion(outputs.squeeze(1), labels.squeeze(1))\n",
    "                \n",
    "                running_loss += val_loss.item()\n",
    "                running_iou += calculate_iou(outputs.squeeze(1), labels.squeeze(1))\n",
    "                \n",
    "                pbar.set_postfix({'loss': running_loss / (batch_idx + 1), 'iou': running_iou / (batch_idx + 1)})\n",
    "                \n",
    "        epoch_loss = running_loss / len(val_loader)\n",
    "        epoch_iou = running_iou / len(val_loader)\n",
    "        \n",
    "        return epoch_loss, epoch_iou\n",
    "    \n",
    "    \n",
    "    # --- MAIN EXECUTION LOOP ---\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING FINE-TUNING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nüìÖ Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_iou = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_iou = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"¬† ¬†Train Loss: {train_loss:.4f} | Train IoU: {train_iou:.4f}\")\n",
    "        print(f\"¬† ¬†Val Loss:¬† ¬†{val_loss:.4f} | Val IoU:¬† ¬†{val_iou:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_iou': val_iou,\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            print(f\"¬† ¬†‚úÖ Saved best model (Val Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "            \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ FINE-TUNING COMPLETE!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6024cc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from terratorch.registry import BACKBONE_REGISTRY # Required for Prithvi loading\n",
    "\n",
    "# =================================================================\n",
    "# MODEL DEFINITION (FINAL FIXED VERSION)\n",
    "# =================================================================\n",
    "\n",
    "class PrithviFloodSegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model wrapper that loads the Prithvi-EO-2.0-600M backbone and adapts \n",
    "    its input layer to accept 2 channels (VV-pre, VV-post).\n",
    "    \"\"\"\n",
    "    def __init__(self, output_classes=1): # Set to 1 output channel for BCE loss\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Load the Prithvi-EO-2.0-600M Backbone\n",
    "        self.backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_600\", pretrained=True)\n",
    "        \n",
    "        # 2. Adapt Input Layer for 2 Channels (CRITICAL FIX)\n",
    "        # FIX: Assume 'proj' IS the Conv3d layer itself due to the structural change.\n",
    "        original_conv = self.backbone.patch_embed.proj # <-- NO [0] INDEX\n",
    "        original_weights = original_conv.weight.data\n",
    "\n",
    "        # Create the new 2-channel convolution layer\n",
    "        new_conv = nn.Conv3d(\n",
    "            in_channels=2, # Set input channels to 2 (VV-pre, VV-post)\n",
    "            out_channels=original_weights.shape[0], \n",
    "            kernel_size=original_conv.kernel_size,\n",
    "            stride=original_conv.stride,\n",
    "            padding=original_conv.padding,\n",
    "            bias=original_conv.bias is not None\n",
    "        )\n",
    "        \n",
    "        # Adapt weights: Take the mean across the original 6 channels' weights, \n",
    "        # and repeat/tile for the 2 input channels.\n",
    "        adapted_weights = original_weights.mean(dim=1, keepdim=True).repeat(1, 2, 1, 1, 1)\n",
    "        new_conv.weight.data = adapted_weights\n",
    "        \n",
    "        # Replace the original convolution layer\n",
    "        self.backbone.patch_embed.proj = new_conv # <-- Assign the new Conv3d layer\n",
    "\n",
    "        # 3. Simple Segmentation Head\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1024, out_channels=256, kernel_size=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=256, out_channels=output_classes, kernel_size=1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape from DataLoader: (B, C=2, H, W)\n",
    "        x = x.unsqueeze(2) # Output shape: (B, 2, T=1, H, W)\n",
    "        features = self.backbone(x) \n",
    "        output_logits = self.segmentation_head(features)\n",
    "        \n",
    "        # Reshape output to (Batch, H, W) for BCEWithLogitsLoss\n",
    "        output_logits = output_logits.squeeze(2).squeeze(1) \n",
    "        return output_logits\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# DATASET DEFINITION (Needed for DataLoader)\n",
    "# =================================================================\n",
    "\n",
    "class FloodDataset(Dataset):\n",
    "    \"\"\"Dataset for pairing HLS input tensors (X) with Ground Truth label tensors (Y).\"\"\"\n",
    "    def __init__(self, input_tensors_list, label_tensors_list):\n",
    "        self.inputs = input_tensors_list \n",
    "        self.labels = label_tensors_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.inputs[idx] \n",
    "        Y = self.labels[idx]\n",
    "        return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ef3abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:terratorch.models.backbones.prithvi_vit:model_bands not passed. Assuming bands are ordered in the same way as [<HLSBands.BLUE: 'BLUE'>, <HLSBands.GREEN: 'GREEN'>, <HLSBands.RED: 'RED'>, <HLSBands.NIR_NARROW: 'NIR_NARROW'>, <HLSBands.SWIR_1: 'SWIR_1'>, <HLSBands.SWIR_2: 'SWIR_2'>].Pretrained patch_embed layer may be misaligned with current bands\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: HEAD https://huggingface.co/ibm-nasa-geospatial/Prithvi-EO-2.0-600M/resolve/main/Prithvi_EO_V2_600M.pt \"HTTP/1.1 302 Found\"\n",
      "INFO:root:Loaded weights for HLSBands.BLUE in position 0 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.GREEN in position 1 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.RED in position 2 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.NIR_NARROW in position 3 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_1 in position 4 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_2 in position 5 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.BLUE in position 0 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.GREEN in position 1 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.RED in position 2 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.NIR_NARROW in position 3 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_1 in position 4 of patch embed\n",
      "INFO:root:Loaded weights for HLSBands.SWIR_2 in position 5 of patch embed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All model components and data loaders are now initialized. You can now run the training loop.\n"
     ]
    }
   ],
   "source": [
    "# --- Training Initialization: FINAL CLEANED SETUP ---\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.dataloader import default_collate # Required for collation fix\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "\n",
    "# NOTE: Ensure your custom classes (PrithviFloodSegmentationModel, FloodDataset) are defined in previous cells.\n",
    "\n",
    "# --- Define Missing Variables (CRITICAL: These lists must be populated) ---\n",
    "# Example: train_inputs_list, train_labels_list, val_inputs_list, val_labels_list\n",
    "# You must ensure these lists are NOT empty (ValueError otherwise).\n",
    "# Assuming they are defined and ready from a previous step.\n",
    "# ----------------------------------\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1. Instantiate the Model (output_classes=1 for binary segmentation/BCE loss)\n",
    "# This assumes the PrithviFloodSegmentationModel class definition is in a previous cell.\n",
    "model = PrithviFloodSegmentationModel(output_classes=1) \n",
    "model.to(device) \n",
    "\n",
    "# 2. Instantiate Datasets and DataLoaders\n",
    "# NOTE: If your lists are empty, you will get a ValueError here.\n",
    "train_dataset = FloodDataset(train_inputs_list, train_labels_list)\n",
    "val_dataset = FloodDataset(val_inputs_list, val_labels_list)\n",
    "\n",
    "# CRITICAL FIX: num_workers=0 and explicit collate_fn for stability\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=default_collate\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=4, \n",
    "    shuffle=False, \n",
    "    num_workers=0,\n",
    "    collate_fn=default_collate\n",
    ")\n",
    "\n",
    "# 3. Instantiate Criterion, Optimizer, and Scheduler\n",
    "criterion = BCEWithLogitsLoss().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
    "\n",
    "print(\"‚úÖ All model components and data loaders are now initialized. You can now run the training loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40901eae",
   "metadata": {},
   "source": [
    "## **Step 5: Inference and Prediction**\n",
    "Use the trained model to generate flood predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490f70f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RUN THIS AFTER TRAINING IS COMPLETE ---\n",
    "def run_final_inference_and_stitching(best_model_path, all_chips_dict):\n",
    "    # 1. Load the Best Model\n",
    "    # Ensure the model object is instantiated and moved to the correct device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PrithviFloodSegmentationModel(output_classes=1).to(device)\n",
    "    \n",
    "    # Load the state dictionary of the best model saved during training\n",
    "    model.load_state_dict(torch.load(best_model_path)['model_state_dict'])\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    print(f\"Model successfully loaded from {best_model_path}.\")\n",
    "\n",
    "    # 2. Run Inference on All Data\n",
    "    for district, phases in all_chips_dict.items():\n",
    "        # NOTE: You would iterate through ALL chips (pre_flood and post_flood) here, \n",
    "        # combining them into temporal tensors as needed for the model input.\n",
    "        \n",
    "        # --- (Inference and saving logic goes here) ---\n",
    "        # For each predicted chip (predicted_mask_array):\n",
    "        # 1. Save the mask chip to the 'intermediate_masks' folder.\n",
    "        # 2. Call stitch_flood_masks once all chips for that district are processed.\n",
    "        \n",
    "        # stitch_flood_masks(intermediate_mask_dir, district, FINAL_STITCHED_DIR) \n",
    "        pass \n",
    "\n",
    "# Example Call:\n",
    "# run_final_inference_and_stitching(os.path.join(CHECKPOINT_DIR, 'best_model.pth'), processed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2352445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This code runs immediately AFTER training is complete ---\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the following variables are defined from previous cells:\n",
    "# model (the PrithviFloodSegmentationModel class instance)\n",
    "# CHECKPOINT_DIR (the folder where best_model.pth is saved)\n",
    "# FINAL_STITCHED_DIR (your final output folder)\n",
    "# processed_tensors (your dictionary of all 185 processed input chips)\n",
    "# stitch_flood_masks (your defined stitching function)\n",
    "\n",
    "# --- Define Path to Best Model ---\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_best_model_and_predict():\n",
    "    \"\"\"Loads the best model and runs inference on all chips.\"\"\"\n",
    "    print(\"\\n--- PHASE III: INFERENCE AND STITCHING ---\")\n",
    "    \n",
    "    # 1. Load the Best Model Weights\n",
    "    if not os.path.exists(BEST_MODEL_PATH):\n",
    "        print(f\"‚ùå Error: Checkpoint not found at {BEST_MODEL_PATH}. Cannot start inference.\")\n",
    "        return\n",
    "\n",
    "    # Load the model structure (must instantiate the class first)\n",
    "    final_model = PrithviFloodSegmentationModel(output_classes=1).to(device)\n",
    "    \n",
    "    # Load the state dictionary from the saved file\n",
    "    final_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device)['model_state_dict'])\n",
    "    final_model.eval() # Set to evaluation mode\n",
    "\n",
    "    print(\"‚úÖ Best model weights loaded successfully. Starting prediction...\")\n",
    "\n",
    "    # 2. Run Inference on All Chips (for ALL districts)\n",
    "    for district, phases in processed_tensors.items():\n",
    "        print(f\"\\n-> Predicting masks for {district}...\")\n",
    "        \n",
    "        # Prepare directory for the raw prediction chips\n",
    "        raw_mask_chips_dir = os.path.join(FINAL_STITCHED_DIR, 'raw_prediction_chips', district)\n",
    "        os.makedirs(raw_mask_chips_dir, exist_ok=True)\n",
    "        \n",
    "        # Loop through all chips for temporal input\n",
    "        num_chips = min(len(phases['pre_flood']), len(phases['post_flood']))\n",
    "        \n",
    "        for i in range(num_chips):\n",
    "            pre_tensor = phases['pre_flood'][i]\n",
    "            post_tensor = phases['post_flood'][i]\n",
    "            \n",
    "            # Create the temporal input (B, C=2, H, W)\n",
    "            temporal_input = torch.cat([pre_tensor, post_tensor], dim=1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get model output (logits)\n",
    "                output_logits = final_model(temporal_input) \n",
    "                \n",
    "                # Classification: Convert logits to probability (sigmoid) and then to binary (round)\n",
    "                predicted_mask_tensor = torch.sigmoid(output_logits).round()\n",
    "                \n",
    "                # Convert to NumPy array (H, W)\n",
    "                predicted_mask_array = predicted_mask_tensor.squeeze().cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "                # --- (SAVING LOGIC IS OMITTED but would go here, saving predicted_mask_array) ---\n",
    "                # NOTE: You would need to retrieve the rasterio profile from the original input chip\n",
    "                # and save the predicted_mask_array as a GeoTIFF chip here.\n",
    "        \n",
    "        # 3. Stitch Masks into Final Product\n",
    "        # This call assumes the raw chips were saved to 'raw_prediction_chips'\n",
    "        # stitch_flood_masks(raw_mask_chips_dir, district, FINAL_STITCHED_DIR)\n",
    "        print(f\"‚úÖ Final prediction complete for {district}. Ready for stitching.\")\n",
    "\n",
    "# load_best_model_and_predict() # Uncomment this line to run the final inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df491a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss # Loss function for binary segmentation\n",
    "\n",
    "# --- 1. Custom Dataset Definition (Crucial for PyTorch) ---\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, input_tensors, label_tensors):\n",
    "        # input_tensors are your X (HLS images)\n",
    "        self.inputs = input_tensors \n",
    "        # label_tensors are your Y (Ground Truth Masks)\n",
    "        self.labels = label_tensors \n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is the number of chips\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a pair of (Image, Label) for the training loop\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# --- 2. Training Loop Setup ---\n",
    "def setup_training(model, train_inputs, train_labels):\n",
    "    # A. Create Dataset and DataLoader\n",
    "    train_dataset = FloodDataset(train_inputs, train_labels)\n",
    "    # DataLoader manages batching and shuffling\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True) \n",
    "\n",
    "    # B. Define Loss and Optimizer\n",
    "    criterion = BCEWithLogitsLoss() # Good loss function for binary segmentation\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5) # Use a small learning rate for fine-tuning\n",
    "\n",
    "    # C. Start Training (Conceptual loop structure)\n",
    "    num_epochs = 10\n",
    "    print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- FORWARD PASS (Model Prediction) ---\n",
    "            outputs = model(inputs) \n",
    "            \n",
    "            # --- BACKWARD PASS (Learning) ---\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} complete. Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# --- 3. The FINAL STEP (Inference) ---\n",
    "# After training, you replace model.train() with model.eval() and run inference \n",
    "# on the remaining (unlabeled) chips to generate your final flood masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb8307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- This code runs immediately AFTER training is complete ---\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Assuming the following variables are defined from previous cells:\n",
    "# model (the PrithviFloodSegmentationModel class instance)\n",
    "# CHECKPOINT_DIR (the folder where best_model.pth is saved)\n",
    "# FINAL_STITCHED_DIR (your final output folder)\n",
    "# processed_tensors (your dictionary of all 185 processed input chips)\n",
    "# stitch_flood_masks (your defined stitching function)\n",
    "\n",
    "# --- Define Path to Best Model ---\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_best_model_and_predict():\n",
    "    \"\"\"Loads the best model and runs inference on all chips.\"\"\"\n",
    "    print(\"\\n--- PHASE III: INFERENCE AND STITCHING ---\")\n",
    "    \n",
    "    # 1. Load the Best Model Weights\n",
    "    if not os.path.exists(BEST_MODEL_PATH):\n",
    "        print(f\"‚ùå Error: Checkpoint not found at {BEST_MODEL_PATH}. Cannot start inference.\")\n",
    "        return\n",
    "\n",
    "    # Load the model structure (must instantiate the class first)\n",
    "    final_model = PrithviFloodSegmentationModel(output_classes=1).to(device)\n",
    "    \n",
    "    # Load the state dictionary from the saved file\n",
    "    final_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device)['model_state_dict'])\n",
    "    final_model.eval() # Set to evaluation mode\n",
    "\n",
    "    print(\"‚úÖ Best model weights loaded successfully. Starting prediction...\")\n",
    "\n",
    "    # 2. Run Inference on All Chips (for ALL districts)\n",
    "    for district, phases in processed_tensors.items():\n",
    "        print(f\"\\n-> Predicting masks for {district}...\")\n",
    "        \n",
    "        # Prepare directory for the raw prediction chips\n",
    "        raw_mask_chips_dir = os.path.join(FINAL_STITCHED_DIR, 'raw_prediction_chips', district)\n",
    "        os.makedirs(raw_mask_chips_dir, exist_ok=True)\n",
    "        \n",
    "        # Loop through all chips for temporal input\n",
    "        num_chips = min(len(phases['pre_flood']), len(phases['post_flood']))\n",
    "        \n",
    "        for i in range(num_chips):\n",
    "            pre_tensor = phases['pre_flood'][i]\n",
    "            post_tensor = phases['post_flood'][i]\n",
    "            \n",
    "            # Create the temporal input (B, C=2, H, W)\n",
    "            temporal_input = torch.cat([pre_tensor, post_tensor], dim=1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get model output (logits)\n",
    "                output_logits = final_model(temporal_input) \n",
    "                \n",
    "                # Classification: Convert logits to probability (sigmoid) and then to binary (round)\n",
    "                predicted_mask_tensor = torch.sigmoid(output_logits).round()\n",
    "                \n",
    "                # Convert to NumPy array (H, W)\n",
    "                predicted_mask_array = predicted_mask_tensor.squeeze().cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "                # --- (SAVING LOGIC IS OMITTED but would go here, saving predicted_mask_array) ---\n",
    "                # NOTE: You would need to retrieve the rasterio profile from the original input chip\n",
    "                # and save the predicted_mask_array as a GeoTIFF chip here.\n",
    "        \n",
    "        # 3. Stitch Masks into Final Product\n",
    "        # This call assumes the raw chips were saved to 'raw_prediction_chips'\n",
    "        # stitch_flood_masks(raw_mask_chips_dir, district, FINAL_STITCHED_DIR)\n",
    "        print(f\"‚úÖ Final prediction complete for {district}. Ready for stitching.\")\n",
    "\n",
    "# load_best_model_and_predict() # Uncomment this line to run the final inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18b250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "CHECKING PREREQUISITES FOR INFERENCE\n",
      "======================================================================\n",
      "\n",
      "‚ùå ERROR: Model checkpoint not found!\n",
      "   Expected location: C:\\Kaam_Dhanda\\Minor_Project\\model_checkpoints\\best_model.pth\n",
      "\n",
      "üìã REQUIRED STEPS:\n",
      "   1. Go back to Cell 19 (Training Loop)\n",
      "   2. Run the training loop to train the model\n",
      "   3. Training will save 'best_model.pth' in the checkpoints directory\n",
      "   4. After training completes, come back and run this cell\n",
      "\n",
      "‚ö†Ô∏è Cannot proceed with inference without a trained model.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# FINAL EXECUTION - INFERENCE ON ALL DISTRICTS\n",
    "# =================================================================\n",
    "# ‚ö†Ô∏è IMPORTANT: Run Cell 19 (Training Loop) first before running this cell!\n",
    "# This cell requires a trained model checkpoint to exist.\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CHECKING PREREQUISITES FOR INFERENCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Check if model checkpoint exists\n",
    "if not os.path.exists(BEST_MODEL_PATH):\n",
    "    print(f\"\\n‚ùå ERROR: Model checkpoint not found!\")\n",
    "    print(f\"   Expected location: {BEST_MODEL_PATH}\")\n",
    "    print(f\"\\nüìã REQUIRED STEPS:\")\n",
    "    print(f\"   1. Go back to Cell 19 (Training Loop)\")\n",
    "    print(f\"   2. Run the training loop to train the model\")\n",
    "    print(f\"   3. Training will save 'best_model.pth' in the checkpoints directory\")\n",
    "    print(f\"   4. After training completes, come back and run this cell\")\n",
    "    print(\"\\n‚ö†Ô∏è Cannot proceed with inference without a trained model.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Model checkpoint found: {BEST_MODEL_PATH}\")\n",
    "    print(f\"\\nüöÄ Starting inference on all districts...\")\n",
    "    load_best_model_and_predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from rasterio.merge import merge\n",
    "from terratorch.registry import BACKBONE_REGISTRY\n",
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "# =================================================================\n",
    "# I. CONFIGURATION & FILE MANAGEMENT\n",
    "# =================================================================\n",
    "\n",
    "# NOTE: Replace these paths with the actual locations on your system\n",
    "ROOT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS'\n",
    "CHECKPOINT_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\model_checkpoints'\n",
    "FINAL_STITCHED_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Final_Stitched_Maps'\n",
    "PREDICTION_MASKS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\intermediate_masks'\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "\n",
    "# Initialize directories\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(FINAL_STITCHED_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTION_MASKS_DIR, exist_ok=True)\n",
    "\n",
    "# Placeholder for Data Lists (THESE MUST BE POPULATED BY YOUR PREPROCESSING SCRIPT)\n",
    "# For example:\n",
    "# train_inputs_list = [...] \n",
    "# train_labels_list = [...]\n",
    "# val_inputs_list = [...] \n",
    "# val_labels_list = [...]\n",
    "# all_chips_list = [...] # All 185 chips for final prediction\n",
    "\n",
    "# =================================================================\n",
    "# II. CORE MODEL DEFINITION (Prithvi Adaptation)\n",
    "# =================================================================\n",
    "\n",
    "class PrithviFloodSegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Model wrapper that loads the Prithvi-EO-2.0-600M backbone and adapts \n",
    "    its input layer to accept 2 channels (VV-pre, VV-post).\n",
    "    \"\"\"\n",
    "    def __init__(self, output_classes=1): # 1 output channel for BCE loss\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Load the Prithvi-EO-2.0-600M Backbone\n",
    "        self.backbone = BACKBONE_REGISTRY.build(\"prithvi_eo_v2_600\", pretrained=True)\n",
    "        \n",
    "        # 2. Adapt Input Layer for 2 Channels (CRITICAL FIX)\n",
    "        original_conv = self.backbone.patch_embed.proj # Assume 'proj' is the Conv3d layer\n",
    "        original_weights = original_conv.weight.data\n",
    "\n",
    "        new_conv = nn.Conv3d(\n",
    "            in_channels=2, \n",
    "            out_channels=original_weights.shape[0], \n",
    "            kernel_size=original_conv.kernel_size,\n",
    "            stride=original_conv.stride,\n",
    "            padding=original_conv.padding,\n",
    "            bias=original_conv.bias is not None\n",
    "        )\n",
    "        \n",
    "        # Adapt weights: average across the original 6 channels and repeat for the 2 inputs\n",
    "        adapted_weights = original_weights.mean(dim=1, keepdim=True).repeat(1, 2, 1, 1, 1)\n",
    "        new_conv.weight.data = adapted_weights\n",
    "        \n",
    "        self.backbone.patch_embed.proj = new_conv \n",
    "\n",
    "        # 3. Simple Segmentation Head\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=1024, out_channels=256, kernel_size=1), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=256, out_channels=output_classes, kernel_size=1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape from DataLoader: (B, C=2, H, W)\n",
    "        x = x.unsqueeze(2) # Add Time dimension: (B, 2, T=1, H, W)\n",
    "        features = self.backbone(x) \n",
    "        output_logits = self.segmentation_head(features)\n",
    "        \n",
    "        # Reshape output to (Batch, H, W) for BCEWithLogitsLoss\n",
    "        output_logits = output_logits.squeeze(2).squeeze(1) \n",
    "        return output_logits\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# III. DATALOADER AND HELPER FUNCTIONS\n",
    "# =================================================================\n",
    "\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, input_tensors_list, label_tensors_list):\n",
    "        self.inputs = input_tensors_list \n",
    "        self.labels = label_tensors_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.inputs[idx] \n",
    "        Y = self.labels[idx]\n",
    "        return X, Y\n",
    "\n",
    "# NOTE: This function requires the original file paths to be passed/managed\n",
    "def get_original_profile(district_name, chip_index):\n",
    "    # This is a conceptual placeholder. In a final pipeline, the original file path\n",
    "    # must be retrieved from a database/list to fetch the correct rasterio profile.\n",
    "    \n",
    "    # --- Example: Assuming original files are in a known structure ---\n",
    "    original_file_path = os.path.join(ROOT_CHIPS_DIR, district_name, 'pre_flood', f'{district_name}_PreFlood_Image_chip_{chip_index}.tif')\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(original_file_path) as src:\n",
    "            return src.profile.copy()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read original profile for chip {chip_index}: {e}\")\n",
    "        return None\n",
    "\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) metric.\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return iou.item()\n",
    "\n",
    "def stitch_flood_masks(mask_dir, district_name, output_dir):\n",
    "    \"\"\"Stitch all chip-level flood masks into a single district-level GeoTIFF.\"\"\"\n",
    "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
    "    \n",
    "    if not mask_files:\n",
    "        print(f\" ¬† ‚ö†Ô∏è No mask files found in {mask_dir}\")\n",
    "        return\n",
    "    \n",
    "    sources = [rasterio.open(f) for f in mask_files]\n",
    "    stitched_array, out_transform = merge(sources)\n",
    "    out_meta = sources[0].profile.copy()\n",
    "    \n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": stitched_array.shape[1],\n",
    "        \"width\": stitched_array.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": 'uint8',\n",
    "        \"compress\": 'LZW'\n",
    "    })\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'{district_name}_Flood_Map_Predicted.tif')\n",
    "    with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(stitched_array, 1)\n",
    "        \n",
    "    for src in sources: src.close()\n",
    "        \n",
    "    print(f\" ¬† ‚úÖ Final stitched map saved: {output_path}\")\n",
    "\n",
    "# =================================================================\n",
    "# IV. THE THREE STEPS OF EXECUTION (Fine-Tuning, Validation, and Inference)\n",
    "# =================================================================\n",
    "\n",
    "def run_fine_tuning_pipeline(train_inputs, train_labels, val_inputs, val_labels):\n",
    "    \"\"\"\n",
    "    Step 1: Initializes and executes the model training process.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- PHASE I: MODEL TRAINING ---\")\n",
    "    \n",
    "    # 1. Setup Device, Model, and DataLoaders\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = PrithviFloodSegmentationModel(output_classes=1).to(device)\n",
    "    \n",
    "    train_dataset = FloodDataset(train_inputs, train_labels)\n",
    "    val_dataset = FloodDataset(val_inputs, val_labels)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "    # 2. Setup Optimizer, Criterion, and Scheduler\n",
    "    criterion = BCEWithLogitsLoss().to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3)\n",
    "    \n",
    "    # --- Training Loop Logic (Simplified, conceptual) ---\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        # ... (Actual loop that calls optimizer.step() and criterion is omitted for brevity) ...\n",
    "        # After one epoch, calculate loss and save checkpoint:\n",
    "        \n",
    "        # NOTE: Replace with actual calculated loss\n",
    "        val_loss = 0.5 - (epoch / 100) # Simulating decreasing loss\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "             best_val_loss = val_loss\n",
    "             checkpoint_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "             # torch.save(model.state_dict(), checkpoint_path) # NOTE: Uncomment in real run\n",
    "             print(f\"Epoch {epoch+1}: Model saved. Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    return os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "\n",
    "\n",
    "def run_final_inference(best_model_path, all_chips_data):\n",
    "    \"\"\"\n",
    "    Step 2 & 3: Loads the best model and runs inference/stitching on ALL chips.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- PHASE II: INFERENCE AND STITCHING ---\")\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    final_model = PrithviFloodSegmentationModel(output_classes=1).to(device)\n",
    "    \n",
    "    if not os.path.exists(best_model_path):\n",
    "        print(f\"‚ùå Error: Checkpoint not found at {best_model_path}. Cannot proceed.\")\n",
    "        return\n",
    "\n",
    "    # Load the best trained weights\n",
    "    # NOTE: In a real run, this should load the saved weights\n",
    "    # final_model.load_state_dict(torch.load(best_model_path, map_location=device)['model_state_dict'])\n",
    "    final_model.eval() \n",
    "    print(\"‚úÖ Best model weights loaded successfully. Starting prediction...\")\n",
    "\n",
    "    # 2. Run Inference on All Chips\n",
    "    for district, chip_data in all_chips_data.items():\n",
    "        print(f\"\\n-> Predicting and stitching for {district}...\")\n",
    "        \n",
    "        raw_mask_chips_dir = os.path.join(PREDICTION_MASKS_DIR, district)\n",
    "        os.makedirs(raw_mask_chips_dir, exist_ok=True)\n",
    "        \n",
    "        # NOTE: This loop is conceptual and relies on your file structure.\n",
    "        # It must save the prediction tensor as a GeoTIFF chip.\n",
    "        \n",
    "        # --- Conceptual Prediction Loop ---\n",
    "        # for i, (pre_tensor, post_tensor, original_profile) in enumerate(zip_data):\n",
    "        #     temporal_input = torch.cat([pre_tensor, post_tensor], dim=1).to(device)\n",
    "        #     with torch.no_grad():\n",
    "        #         output_logits = final_model(temporal_input) \n",
    "        #         predicted_mask_array = (torch.sigmoid(output_logits).round()).squeeze().cpu().numpy().astype(np.uint8)\n",
    "        #         # Save logic using rasterio and the original profile goes here...\n",
    "        \n",
    "        print(f\"‚úÖ Prediction complete for {district}. Starting stitching...\")\n",
    "\n",
    "        # 3. Stitch Masks into Final Product\n",
    "        stitch_flood_masks(raw_mask_chips_dir, district, FINAL_STITCHED_DIR)\n",
    "\n",
    "# --- EXAMPLE EXECUTION (To be run after data is loaded) ---\n",
    "# NOTE: Replace the conceptual lists/paths with your actual data from the notebook\n",
    "# final_checkpoint_path = run_fine_tuning_pipeline(train_inputs_list, train_labels_list, val_inputs_list, val_labels_list)\n",
    "# run_final_inference(final_checkpoint_path, processed_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a1689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from torch import nn\n",
    "from terratorch.registry import BACKBONE_REGISTRY \n",
    "\n",
    "# --- Configuration (MUST MATCH PREVIOUS CELLS) ---\n",
    "CHECKPOINT_DIR = r'C:\\Kaam_Danda\\Minor_Project\\model_checkpoints'\n",
    "FINAL_STITCHED_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Final_Stitched_Maps'\n",
    "PREDICTION_MASKS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\intermediate_masks'\n",
    "\n",
    "BEST_MODEL_PATH = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# NOTE: The definition of PrithviFloodSegmentationModel and processed_tensors \n",
    "# must be available in your notebook environment before running this.\n",
    "\n",
    "# =================================================================\n",
    "# FINAL INFERENCE AND STITCHING FUNCTION\n",
    "# =================================================================\n",
    "\n",
    "def stitch_flood_masks(mask_dir, district_name, output_dir):\n",
    "    \"\"\"Stitch all chip-level flood masks into a single district-level GeoTIFF.\"\"\"\n",
    "    \n",
    "    mask_files = sorted([os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.tif')])\n",
    "    \n",
    "    if not mask_files:\n",
    "        print(f\" ¬† ‚ö†Ô∏è No mask chips found in {mask_dir} for stitching.\")\n",
    "        return\n",
    "        \n",
    "    # Open all mask datasets\n",
    "    sources = [rasterio.open(f) for f in mask_files]\n",
    "    \n",
    "    # Merge into a single mosaic\n",
    "    stitched_array, out_transform = merge(sources)\n",
    "    \n",
    "    out_meta = sources[0].profile.copy()\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": stitched_array.shape[1],\n",
    "        \"width\": stitched_array.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": 'uint8',\n",
    "        \"compress\": 'LZW'\n",
    "    })\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f'{district_name}_Final_Flood_Map_Predicted.tif')\n",
    "    with rasterio.open(output_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(stitched_array, 1)\n",
    "        \n",
    "    for src in sources: src.close()\n",
    "        \n",
    "    print(f\" ¬† ‚úÖ Final stitched map saved to: {output_path}\")\n",
    "\n",
    "\n",
    "def load_best_model_and_predict(all_chips_data):\n",
    "    \"\"\"Loads the best model, runs inference on all chips, and stitches the results.\"\"\"\n",
    "    \n",
    "    print(\"\\n--- PHASE III: INFERENCE AND STITCHING ---\")\n",
    "    \n",
    "    # 1. Load the Best Model\n",
    "    if not os.path.exists(BEST_MODEL_PATH):\n",
    "        print(f\"‚ùå Error: Checkpoint not found at {BEST_MODEL_PATH}. Cannot start inference.\")\n",
    "        return\n",
    "\n",
    "    # Instantiate the model structure (PrithviFloodSegmentationModel must be defined in your notebook)\n",
    "    final_model = PrithviFloodSegmentationModel(output_classes=1).to(device)\n",
    "    \n",
    "    # Load the best trained weights\n",
    "    final_model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device)['model_state_dict'])\n",
    "    final_model.eval() \n",
    "    print(\"‚úÖ Best model weights loaded successfully. Starting prediction on all chips...\")\n",
    "\n",
    "    # 2. Run Inference on All Chips and Save Masks\n",
    "    for district, chip_data in all_chips_data.items():\n",
    "        print(f\"\\n-> Predicting and stitching for {district}...\")\n",
    "        \n",
    "        raw_mask_chips_dir = os.path.join(PREDICTION_MASKS_DIR, district)\n",
    "        os.makedirs(raw_mask_chips_dir, exist_ok=True)\n",
    "        \n",
    "        num_chips = min(len(chip_data['pre_flood']), len(chip_data['post_flood']))\n",
    "        \n",
    "        for i in range(num_chips):\n",
    "            pre_tensor = chip_data['pre_flood'][i]\n",
    "            post_tensor = chip_data['post_flood'][i]\n",
    "            \n",
    "            # Create the temporal input (B, C=2, H, W)\n",
    "            temporal_input = torch.cat([pre_tensor, post_tensor], dim=1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Get model output (logits) and convert to binary mask\n",
    "                output_logits = final_model(temporal_input) \n",
    "                \n",
    "                # Classification: Apply sigmoid, then round to get 0 or 1\n",
    "                predicted_mask_tensor = torch.sigmoid(output_logits).round()\n",
    "                predicted_mask_array = predicted_mask_tensor.squeeze().cpu().numpy().astype(np.uint8)\n",
    "                \n",
    "                # --- Get Profile and Save Chip (Conceptual: Requires file path linkage) ---\n",
    "                # NOTE: You must have a way to link the index 'i' back to the original GeoTIFF \n",
    "                # file path to get its metadata profile (transform, CRS, etc.) for saving.\n",
    "                \n",
    "                # Placeholder for profile saving (MUST BE IMPLEMENTED IN REAL CODE)\n",
    "                # This will skip the actual saving, but completes the logic flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920a7237",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minor_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

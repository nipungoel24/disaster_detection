{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e14cc1b2",
   "metadata": {},
   "source": [
    "# **Image Quality Filtering and Preprocessing**\n",
    "This notebook filters HLS (Harmonized Landsat Sentinel) 6-band optical images based on:\n",
    "- **File size**: Only processes images above a minimum size threshold\n",
    "- **Data validity**: Checks for corrupt or incomplete files\n",
    "- **Spatial dimensions**: Ensures images meet minimum resolution requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7ec72fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from rasterio.merge import merge\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95b385c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Configuration loaded\n",
      "   Input directory: C:\\Kaam_Dhanda\\Minor_Project\\Flood_Analysis_HLS_Exports-20251121T100600Z-1-001\\Flood_Analysis_HLS_Exports\n",
      "   Min file size: 5 MB\n",
      "   Min dimensions: 1000x1000 pixels\n",
      "   Chip size: 224x224 pixels\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# CONFIGURATION\n",
    "# =================================================================\n",
    "\n",
    "# Input directory with HLS 6-band images\n",
    "INPUT_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Analysis_HLS_Exports-20251121T100600Z-1-001\\Flood_Analysis_HLS_Exports'\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS'\n",
    "FILTERED_IMAGES_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Filtered_HLS_Images'\n",
    "\n",
    "# Quality thresholds\n",
    "MIN_FILE_SIZE_MB = 5          # Minimum file size in MB (filters out corrupted/incomplete files)\n",
    "MIN_WIDTH = 1000              # Minimum image width in pixels\n",
    "MIN_HEIGHT = 1000             # Minimum image height in pixels\n",
    "MAX_NODATA_PERCENT = 30       # Maximum percentage of no-data values allowed\n",
    "\n",
    "# Chipping parameters\n",
    "CHIP_SIZE = 224               # Standard size for deep learning (224x224 for vision models)\n",
    "MAX_NODATA_PER_CHIP = 20      # Maximum % of no-data allowed per chip\n",
    "\n",
    "# Districts to process\n",
    "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"   Input directory: {INPUT_DIR}\")\n",
    "print(f\"   Min file size: {MIN_FILE_SIZE_MB} MB\")\n",
    "print(f\"   Min dimensions: {MIN_WIDTH}x{MIN_HEIGHT} pixels\")\n",
    "print(f\"   Chip size: {CHIP_SIZE}x{CHIP_SIZE} pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c5e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# IMAGE QUALITY VALIDATION FUNCTIONS\n",
    "# =================================================================\n",
    "\n",
    "def validate_image_quality(file_path, min_size_mb, min_width, min_height, max_nodata_percent):\n",
    "    \"\"\"\n",
    "    Validates an image file based on multiple quality criteria.\n",
    "    \n",
    "    Returns:\n",
    "        (bool, dict): (is_valid, metadata_dict)\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'file_path': file_path,\n",
    "        'file_size_mb': 0,\n",
    "        'width': 0,\n",
    "        'height': 0,\n",
    "        'bands': 0,\n",
    "        'nodata_percent': 0,\n",
    "        'dtype': None,\n",
    "        'crs': None,\n",
    "        'is_valid': False,\n",
    "        'rejection_reason': []\n",
    "    }\n",
    "    \n",
    "    # Check 1: File exists\n",
    "    if not os.path.exists(file_path):\n",
    "        results['rejection_reason'].append(\"File not found\")\n",
    "        return False, results\n",
    "    \n",
    "    # Check 2: File size\n",
    "    file_size_bytes = os.path.getsize(file_path)\n",
    "    file_size_mb = file_size_bytes / (1024 * 1024)\n",
    "    results['file_size_mb'] = round(file_size_mb, 2)\n",
    "    \n",
    "    if file_size_mb < min_size_mb:\n",
    "        results['rejection_reason'].append(f\"File too small ({file_size_mb:.2f} MB < {min_size_mb} MB)\")\n",
    "        return False, results\n",
    "    \n",
    "    # Check 3: Can open and read metadata\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            results['width'] = src.width\n",
    "            results['height'] = src.height\n",
    "            results['bands'] = src.count\n",
    "            results['dtype'] = str(src.dtypes[0])\n",
    "            results['crs'] = str(src.crs) if src.crs else \"None\"\n",
    "            \n",
    "            # Check dimensions\n",
    "            if src.width < min_width or src.height < min_height:\n",
    "                results['rejection_reason'].append(\n",
    "                    f\"Image too small ({src.width}x{src.height} < {min_width}x{min_height})\"\n",
    "                )\n",
    "                return False, results\n",
    "            \n",
    "            # Check 4: Sample data for no-data percentage (check first band)\n",
    "            sample_data = src.read(1)\n",
    "            \n",
    "            # Check for no-data values\n",
    "            if src.nodata is not None:\n",
    "                nodata_pixels = np.sum(sample_data == src.nodata)\n",
    "            else:\n",
    "                # If no nodata value specified, check for NaN or zeros\n",
    "                nodata_pixels = np.sum(np.isnan(sample_data)) + np.sum(sample_data == 0)\n",
    "            \n",
    "            total_pixels = sample_data.size\n",
    "            nodata_percent = (nodata_pixels / total_pixels) * 100\n",
    "            results['nodata_percent'] = round(nodata_percent, 2)\n",
    "            \n",
    "            if nodata_percent > max_nodata_percent:\n",
    "                results['rejection_reason'].append(\n",
    "                    f\"Too much no-data ({nodata_percent:.1f}% > {max_nodata_percent}%)\"\n",
    "                )\n",
    "                return False, results\n",
    "            \n",
    "    except Exception as e:\n",
    "        results['rejection_reason'].append(f\"Error reading file: {str(e)}\")\n",
    "        return False, results\n",
    "    \n",
    "    # If all checks pass\n",
    "    results['is_valid'] = True\n",
    "    return True, results\n",
    "\n",
    "\n",
    "def scan_and_filter_images(input_dir, districts, min_size_mb, min_width, min_height, max_nodata_percent):\n",
    "    \"\"\"\n",
    "    Scans all images in the directory and filters based on quality criteria.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {district: {'pre_flood': path, 'post_flood': path, 'metadata': {...}}}\n",
    "    \"\"\"\n",
    "    valid_images = {}\n",
    "    rejected_images = {}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SCANNING AND VALIDATING IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for district in districts:\n",
    "        print(f\"\\nüìç District: {district}\")\n",
    "        \n",
    "        # Look for pre and post flood files\n",
    "        pre_file = os.path.join(input_dir, f'{district}_PreFlood_HLS_6Band.tif')\n",
    "        post_file = os.path.join(input_dir, f'{district}_PostFlood_HLS_6Band.tif')\n",
    "        \n",
    "        # Validate pre-flood image\n",
    "        pre_valid, pre_results = validate_image_quality(\n",
    "            pre_file, min_size_mb, min_width, min_height, max_nodata_percent\n",
    "        )\n",
    "        \n",
    "        # Validate post-flood image\n",
    "        post_valid, post_results = validate_image_quality(\n",
    "            post_file, min_size_mb, min_width, min_height, max_nodata_percent\n",
    "        )\n",
    "        \n",
    "        # Both must be valid to include the district\n",
    "        if pre_valid and post_valid:\n",
    "            valid_images[district] = {\n",
    "                'pre_flood': pre_file,\n",
    "                'post_flood': post_file,\n",
    "                'pre_metadata': pre_results,\n",
    "                'post_metadata': post_results\n",
    "            }\n",
    "            print(f\"   ‚úÖ ACCEPTED\")\n",
    "            print(f\"      Pre-flood:  {pre_results['width']}x{pre_results['height']}, \"\n",
    "                  f\"{pre_results['file_size_mb']} MB, {pre_results['nodata_percent']}% no-data\")\n",
    "            print(f\"      Post-flood: {post_results['width']}x{post_results['height']}, \"\n",
    "                  f\"{post_results['file_size_mb']} MB, {post_results['nodata_percent']}% no-data\")\n",
    "        else:\n",
    "            rejected_images[district] = {\n",
    "                'pre_results': pre_results,\n",
    "                'post_results': post_results\n",
    "            }\n",
    "            print(f\"   ‚ùå REJECTED\")\n",
    "            if not pre_valid:\n",
    "                print(f\"      Pre-flood issues: {', '.join(pre_results['rejection_reason'])}\")\n",
    "            if not post_valid:\n",
    "                print(f\"      Post-flood issues: {', '.join(post_results['rejection_reason'])}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Valid districts: {len(valid_images)}/{len(districts)}\")\n",
    "    print(f\"‚ùå Rejected districts: {len(rejected_images)}/{len(districts)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return valid_images, rejected_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "806f37ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SCANNING AND VALIDATING IMAGES\n",
      "======================================================================\n",
      "\n",
      "üìç District: Barpeta\n",
      "   ‚úÖ ACCEPTED\n",
      "      Pre-flood:  1979x1326, 52.16 MB, 3.33% no-data\n",
      "      Post-flood: 1979x1326, 49.03 MB, 7.16% no-data\n",
      "\n",
      "üìç District: Dhemaji\n",
      "   ‚úÖ ACCEPTED\n",
      "      Pre-flood:  1979x1326, 52.16 MB, 3.33% no-data\n",
      "      Post-flood: 1979x1326, 49.03 MB, 7.16% no-data\n",
      "\n",
      "üìç District: Dhemaji\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: File too small (0.40 MB < 5 MB)\n",
      "\n",
      "üìç District: Lakhimpur\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: File too small (0.40 MB < 5 MB)\n",
      "\n",
      "üìç District: Lakhimpur\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: Too much no-data (94.3% > 30%)\n",
      "\n",
      "üìç District: Nalbari\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: Too much no-data (94.3% > 30%)\n",
      "\n",
      "üìç District: Nalbari\n",
      "   ‚úÖ ACCEPTED\n",
      "      Pre-flood:  1559x1300, 40.97 MB, 2.08% no-data\n",
      "      Post-flood: 1559x1300, 37.39 MB, 10.68% no-data\n",
      "\n",
      "üìç District: Sonitpur\n",
      "   ‚úÖ ACCEPTED\n",
      "      Pre-flood:  1559x1300, 40.97 MB, 2.08% no-data\n",
      "      Post-flood: 1559x1300, 37.39 MB, 10.68% no-data\n",
      "\n",
      "üìç District: Sonitpur\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: Too much no-data (43.5% > 30%)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Valid districts: 2/5\n",
      "‚ùå Rejected districts: 3/5\n",
      "======================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "\n",
      "‚úÖ Valid districts ready for processing:\n",
      "   ‚Ä¢ Barpeta\n",
      "   ‚Ä¢ Nalbari\n",
      "\n",
      "‚ùå Rejected districts:\n",
      "   ‚Ä¢ Dhemaji\n",
      "   ‚Ä¢ Lakhimpur\n",
      "   ‚Ä¢ Sonitpur\n",
      "   ‚ùå REJECTED\n",
      "      Post-flood issues: Too much no-data (43.5% > 30%)\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Valid districts: 2/5\n",
      "‚ùå Rejected districts: 3/5\n",
      "======================================================================\n",
      "\n",
      "üìä SUMMARY:\n",
      "\n",
      "‚úÖ Valid districts ready for processing:\n",
      "   ‚Ä¢ Barpeta\n",
      "   ‚Ä¢ Nalbari\n",
      "\n",
      "‚ùå Rejected districts:\n",
      "   ‚Ä¢ Dhemaji\n",
      "   ‚Ä¢ Lakhimpur\n",
      "   ‚Ä¢ Sonitpur\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# RUN IMAGE QUALITY SCAN\n",
    "# =================================================================\n",
    "\n",
    "# Scan and filter images based on quality criteria\n",
    "valid_images, rejected_images = scan_and_filter_images(\n",
    "    INPUT_DIR,\n",
    "    DISTRICTS,\n",
    "    MIN_FILE_SIZE_MB,\n",
    "    MIN_WIDTH,\n",
    "    MIN_HEIGHT,\n",
    "    MAX_NODATA_PERCENT\n",
    ")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nüìä SUMMARY:\")\n",
    "if valid_images:\n",
    "    print(\"\\n‚úÖ Valid districts ready for processing:\")\n",
    "    for district in valid_images.keys():\n",
    "        print(f\"   ‚Ä¢ {district}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No valid images found! Please check your quality thresholds.\")\n",
    "\n",
    "if rejected_images:\n",
    "    print(\"\\n‚ùå Rejected districts:\")\n",
    "    for district in rejected_images.keys():\n",
    "        print(f\"   ‚Ä¢ {district}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a42834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# CHIPPING FUNCTION FOR VALID IMAGES\n",
    "# =================================================================\n",
    "\n",
    "def chip_image_with_quality_check(input_filepath, output_directory, chip_size, max_nodata_percent):\n",
    "    \"\"\"\n",
    "    Cuts a large GeoTIFF into smaller chips, only saving high-quality chips.\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of valid chips created\n",
    "    \"\"\"\n",
    "    try:\n",
    "        src = rasterio.open(input_filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error opening file: {e}\")\n",
    "        return 0\n",
    "    \n",
    "    width = src.width\n",
    "    height = src.height\n",
    "    count = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    print(f\"   Processing {os.path.basename(input_filepath)}...\")\n",
    "    print(f\"   Image dimensions: {width}x{height}, {src.count} bands\")\n",
    "    \n",
    "    # Loop through the image in chunks\n",
    "    for i in range(0, height, chip_size):\n",
    "        for j in range(0, width, chip_size):\n",
    "            \n",
    "            # Define the window\n",
    "            window = Window(j, i, min(chip_size, width - j), min(chip_size, height - i))\n",
    "            transform = src.window_transform(window)\n",
    "            \n",
    "            # Read all bands for this chip\n",
    "            chip_data = src.read(window=window)\n",
    "            \n",
    "            # Quality check 1: Check chip size (skip edge chips that are too small)\n",
    "            if window.height < chip_size * 0.5 or window.width < chip_size * 0.5:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Quality check 2: Check for no-data percentage (check first band as representative)\n",
    "            first_band = chip_data[0]\n",
    "            \n",
    "            if src.nodata is not None:\n",
    "                nodata_pixels = np.sum(first_band == src.nodata)\n",
    "            else:\n",
    "                nodata_pixels = np.sum(np.isnan(first_band)) + np.sum(first_band == 0)\n",
    "            \n",
    "            total_pixels = first_band.size\n",
    "            nodata_percent = (nodata_pixels / total_pixels) * 100\n",
    "            \n",
    "            if nodata_percent > max_nodata_percent:\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Quality check 3: Check for data variation (avoid uniform chips)\n",
    "            if np.std(first_band) < 0.01:  # Very low standard deviation\n",
    "                skipped += 1\n",
    "                continue\n",
    "            \n",
    "            # Update metadata for the chip\n",
    "            profile = src.profile.copy()\n",
    "            profile.update({\n",
    "                'height': window.height,\n",
    "                'width': window.width,\n",
    "                'transform': transform,\n",
    "                'compress': 'LZW'\n",
    "            })\n",
    "            \n",
    "            # Save the chip\n",
    "            base_filename = os.path.basename(input_filepath)\n",
    "            file_stem = base_filename.replace('.tif', '')\n",
    "            chip_filename = f'{file_stem}_chip_{count}.tif'\n",
    "            output_path = os.path.join(output_directory, chip_filename)\n",
    "            \n",
    "            try:\n",
    "                with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                    dst.write(chip_data)\n",
    "                count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è Failed to write chip: {e}\")\n",
    "    \n",
    "    src.close()\n",
    "    \n",
    "    print(f\"   ‚úÖ Created {count} valid chips (skipped {skipped} low-quality chips)\")\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4da86c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "CHIPPING VALID IMAGES\n",
      "======================================================================\n",
      "\n",
      "üèûÔ∏è Processing Barpeta...\n",
      "\n",
      "   Pre-flood image:\n",
      "   Processing Barpeta_PreFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1979x1326, 6 bands\n",
      "   ‚úÖ Created 52 valid chips (skipped 2 low-quality chips)\n",
      "\n",
      "   Post-flood image:\n",
      "   Processing Barpeta_PostFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1979x1326, 6 bands\n",
      "   ‚úÖ Created 52 valid chips (skipped 2 low-quality chips)\n",
      "\n",
      "   Post-flood image:\n",
      "   Processing Barpeta_PostFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1979x1326, 6 bands\n",
      "   ‚úÖ Created 53 valid chips (skipped 1 low-quality chips)\n",
      "\n",
      "üèûÔ∏è Processing Nalbari...\n",
      "\n",
      "   Pre-flood image:\n",
      "   Processing Nalbari_PreFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1559x1300, 6 bands\n",
      "   ‚úÖ Created 53 valid chips (skipped 1 low-quality chips)\n",
      "\n",
      "üèûÔ∏è Processing Nalbari...\n",
      "\n",
      "   Pre-flood image:\n",
      "   Processing Nalbari_PreFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1559x1300, 6 bands\n",
      "   ‚úÖ Created 42 valid chips (skipped 0 low-quality chips)\n",
      "\n",
      "   Post-flood image:\n",
      "   Processing Nalbari_PostFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1559x1300, 6 bands\n",
      "   ‚úÖ Created 42 valid chips (skipped 0 low-quality chips)\n",
      "\n",
      "   Post-flood image:\n",
      "   Processing Nalbari_PostFlood_HLS_6Band.tif...\n",
      "   Image dimensions: 1559x1300, 6 bands\n",
      "   ‚úÖ Created 38 valid chips (skipped 4 low-quality chips)\n",
      "\n",
      "======================================================================\n",
      "üìä CHIPPING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Barpeta:\n",
      "   Pre-flood:  52 chips\n",
      "   Post-flood: 53 chips\n",
      "   Total:      105 chips\n",
      "\n",
      "Nalbari:\n",
      "   Pre-flood:  42 chips\n",
      "   Post-flood: 38 chips\n",
      "   Total:      80 chips\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Total chips created: 185\n",
      "üìÅ Output directory: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS\n",
      "======================================================================\n",
      "   ‚úÖ Created 38 valid chips (skipped 4 low-quality chips)\n",
      "\n",
      "======================================================================\n",
      "üìä CHIPPING SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Barpeta:\n",
      "   Pre-flood:  52 chips\n",
      "   Post-flood: 53 chips\n",
      "   Total:      105 chips\n",
      "\n",
      "Nalbari:\n",
      "   Pre-flood:  42 chips\n",
      "   Post-flood: 38 chips\n",
      "   Total:      80 chips\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Total chips created: 185\n",
      "üìÅ Output directory: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# PROCESS VALID IMAGES - CREATE CHIPS\n",
    "# =================================================================\n",
    "\n",
    "if not valid_images:\n",
    "    print(\"‚ö†Ô∏è No valid images to process. Adjust quality thresholds if needed.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CHIPPING VALID IMAGES\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create output directories\n",
    "    os.makedirs(OUTPUT_CHIPS_DIR, exist_ok=True)\n",
    "    \n",
    "    chip_statistics = {}\n",
    "    \n",
    "    for district, files in valid_images.items():\n",
    "        print(f\"\\nüèûÔ∏è Processing {district}...\")\n",
    "        \n",
    "        # Create district output directories\n",
    "        pre_output_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'pre_flood')\n",
    "        post_output_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'post_flood')\n",
    "        \n",
    "        os.makedirs(pre_output_dir, exist_ok=True)\n",
    "        os.makedirs(post_output_dir, exist_ok=True)\n",
    "        \n",
    "        # Chip pre-flood image\n",
    "        print(f\"\\n   Pre-flood image:\")\n",
    "        pre_chip_count = chip_image_with_quality_check(\n",
    "            files['pre_flood'],\n",
    "            pre_output_dir,\n",
    "            CHIP_SIZE,\n",
    "            MAX_NODATA_PER_CHIP\n",
    "        )\n",
    "        \n",
    "        # Chip post-flood image\n",
    "        print(f\"\\n   Post-flood image:\")\n",
    "        post_chip_count = chip_image_with_quality_check(\n",
    "            files['post_flood'],\n",
    "            post_output_dir,\n",
    "            CHIP_SIZE,\n",
    "            MAX_NODATA_PER_CHIP\n",
    "        )\n",
    "        \n",
    "        chip_statistics[district] = {\n",
    "            'pre_flood_chips': pre_chip_count,\n",
    "            'post_flood_chips': post_chip_count\n",
    "        }\n",
    "    \n",
    "    # Display final summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"üìä CHIPPING SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total_chips = 0\n",
    "    for district, stats in chip_statistics.items():\n",
    "        district_total = stats['pre_flood_chips'] + stats['post_flood_chips']\n",
    "        total_chips += district_total\n",
    "        print(f\"\\n{district}:\")\n",
    "        print(f\"   Pre-flood:  {stats['pre_flood_chips']} chips\")\n",
    "        print(f\"   Post-flood: {stats['post_flood_chips']} chips\")\n",
    "        print(f\"   Total:      {district_total} chips\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"‚úÖ Total chips created: {total_chips}\")\n",
    "    print(f\"üìÅ Output directory: {OUTPUT_CHIPS_DIR}\")\n",
    "    print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41511748",
   "metadata": {},
   "source": [
    "---\n",
    "## Optional: Copy Valid Full-Size Images\n",
    "You can copy the validated full-size images to a separate directory for archival purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "771564a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Copying validated full-size images...\n",
      "   ‚úÖ Copied Barpeta images\n",
      "   ‚úÖ Copied Nalbari images\n",
      "\n",
      "‚úÖ All validated images copied to: C:\\Kaam_Dhanda\\Minor_Project\\Filtered_HLS_Images\n"
     ]
    }
   ],
   "source": [
    "# Optional: Copy validated full-size images to a separate directory\n",
    "import shutil\n",
    "\n",
    "if valid_images:\n",
    "    print(\"üì¶ Copying validated full-size images...\")\n",
    "    os.makedirs(FILTERED_IMAGES_DIR, exist_ok=True)\n",
    "    \n",
    "    for district, files in valid_images.items():\n",
    "        # Copy pre-flood image\n",
    "        pre_dest = os.path.join(FILTERED_IMAGES_DIR, f'{district}_PreFlood_HLS_6Band.tif')\n",
    "        shutil.copy2(files['pre_flood'], pre_dest)\n",
    "        \n",
    "        # Copy post-flood image\n",
    "        post_dest = os.path.join(FILTERED_IMAGES_DIR, f'{district}_PostFlood_HLS_6Band.tif')\n",
    "        shutil.copy2(files['post_flood'], post_dest)\n",
    "        \n",
    "        print(f\"   ‚úÖ Copied {district} images\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ All validated images copied to: {FILTERED_IMAGES_DIR}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No valid images to copy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "109ffa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch HLS tensor conversion from: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS\n",
      "\n",
      "=======================================================\n",
      "‚úÖ Tensor Conversion Complete. Total Tensors Created: 185\n",
      "=======================================================\n",
      "\n",
      "=======================================================\n",
      "‚úÖ Tensor Conversion Complete. Total Tensors Created: 185\n",
      "=======================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# =======================================================================\n",
    "# CONFIGURATION - HLS/SENTINEL-2 PARAMETERS (for Prithvi)\n",
    "# =======================================================================\n",
    "\n",
    "# NOTE: These are official normalization parameters for HLS (Sentinel-2) data \n",
    "# scaled to 0-10000, which are commonly used with Prithvi models.\n",
    "HLS_NORM_MEANS = np.array([1353, 1146, 989, 2661, 2378, 1782], dtype=np.float32) \n",
    "HLS_NORM_STDS = np.array([870, 891, 1007, 1251, 1251, 1140], dtype=np.float32)\n",
    "SCALE_FACTOR = 10000.0 \n",
    "\n",
    "# IMPORTANT: SET YOUR ROOT DIRECTORY\n",
    "ROOT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips_HLS'\n",
    "\n",
    "# Dictionary to store all processed tensors\n",
    "processed_tensors = {}\n",
    "\n",
    "# =======================================================================\n",
    "# CORE PROCESSING FUNCTION\n",
    "# =======================================================================\n",
    "\n",
    "def preprocess_hls_chip(file_path, means, stds, scale_factor):\n",
    "    \"\"\"\n",
    "    Reads a 6-band HLS GeoTIFF, normalizes it, and converts it to a \n",
    "    PyTorch Tensor (1, C=6, H, W).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Read all 6 bands: (C, H, W)\n",
    "            data = src.read().astype(np.float32)\n",
    "            \n",
    "            if data.shape[0] != 6:\n",
    "                raise ValueError(f\"Skipping {os.path.basename(file_path)}: Expected 6 bands, found {data.shape[0]}.\")\n",
    "\n",
    "    except rasterio.RasterioIOError:\n",
    "        print(f\"Error: Could not open or read {file_path}.\")\n",
    "        return None, None\n",
    "\n",
    "    # 1. Scaling (converting 0-10000 range to 0-1 range)\n",
    "    data = np.clip(data, 0, 10000) / scale_factor \n",
    "    \n",
    "    # 2. Normalization (Z-Score)\n",
    "    # Reshape means/stds for broadcasting: (Channels, 1, 1)\n",
    "    means_reshaped = means.reshape(6, 1, 1) / scale_factor\n",
    "    stds_reshaped = stds.reshape(6, 1, 1) / scale_factor\n",
    "    \n",
    "    normalized_data = (data - means_reshaped) / stds_reshaped\n",
    "\n",
    "    # 3. Convert to PyTorch Tensor and add batch dimension (1, C, H, W)\n",
    "    tensor = torch.from_numpy(normalized_data)\n",
    "    tensor = tensor.unsqueeze(0) \n",
    "\n",
    "    return tensor\n",
    "\n",
    "# =======================================================================\n",
    "# BATCH EXECUTION (Populates the processed_tensors dictionary)\n",
    "# =======================================================================\n",
    "\n",
    "print(f\"Starting batch HLS tensor conversion from: {ROOT_CHIPS_DIR}\")\n",
    "\n",
    "for district_name in os.listdir(ROOT_CHIPS_DIR):\n",
    "    district_path = os.path.join(ROOT_CHIPS_DIR, district_name)\n",
    "    \n",
    "    if not os.path.isdir(district_path):\n",
    "        continue\n",
    "\n",
    "    processed_tensors[district_name] = {'pre_flood': [], 'post_flood': []}\n",
    "    \n",
    "    for phase in ['pre_flood', 'post_flood']:\n",
    "        phase_path = os.path.join(district_path, phase)\n",
    "        \n",
    "        if not os.path.isdir(phase_path): continue\n",
    "\n",
    "        for chip_filename in os.listdir(phase_path):\n",
    "            if chip_filename.endswith('.tif'):\n",
    "                chip_file_path = os.path.join(phase_path, chip_filename)\n",
    "                \n",
    "                # Run the core pre-processing function\n",
    "                tensor = preprocess_hls_chip(\n",
    "                    chip_file_path, HLS_NORM_MEANS, HLS_NORM_STDS, SCALE_FACTOR\n",
    "                )\n",
    "                \n",
    "                if tensor is not None:\n",
    "                    # Store the resulting tensor\n",
    "                    processed_tensors[district_name][phase].append(tensor)\n",
    "\n",
    "# --- Final Check ---\n",
    "total_tensors = sum(len(p['pre_flood']) + len(p['post_flood']) for d, p in processed_tensors.items())\n",
    "print(f\"\\n=======================================================\")\n",
    "print(f\"‚úÖ Tensor Conversion Complete. Total Tensors Created: {total_tensors}\")\n",
    "print(\"=======================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9201d4c",
   "metadata": {},
   "source": [
    "# Fine Tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ce28d8",
   "metadata": {},
   "source": [
    "## **Step 1: Load Ground Truth Labels**\n",
    "Before fine-tuning, you need to prepare your ground truth flood masks. These should be binary masks where:\n",
    "- **0** = Non-flooded areas\n",
    "- **1** = Flooded areas\n",
    "\n",
    "You can create these using:\n",
    "- Manual annotation in QGIS\n",
    "- SAR-based change detection masks (from your previous work)\n",
    "- Combination of multiple data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "297e322d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "PAIRING IMAGES WITH GROUND TRUTH LABELS\n",
      "======================================================================\n",
      "\n",
      "üìç District: Barpeta\n",
      "   ‚úÖ Paired 53 image-mask pairs\n",
      "\n",
      "üìç District: Nalbari\n",
      "   ‚úÖ Paired 53 image-mask pairs\n",
      "\n",
      "üìç District: Nalbari\n",
      "   ‚úÖ Paired 38 image-mask pairs\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Total training pairs: 91\n",
      "======================================================================\n",
      "\n",
      "üìä Training data ready:\n",
      "   Images: 91 samples\n",
      "   Labels: 91 samples\n",
      "   Image shape: torch.Size([1, 6, 224, 224])\n",
      "   Label shape: torch.Size([1, 1, 224, 224])\n",
      "   ‚úÖ Paired 38 image-mask pairs\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Total training pairs: 91\n",
      "======================================================================\n",
      "\n",
      "üìä Training data ready:\n",
      "   Images: 91 samples\n",
      "   Labels: 91 samples\n",
      "   Image shape: torch.Size([1, 6, 224, 224])\n",
      "   Label shape: torch.Size([1, 1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# LOAD GROUND TRUTH LABELS\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure paths to your ground truth masks\n",
    "GROUND_TRUTH_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Masks'  # Directory with your SAR-based masks\n",
    "LABEL_CHIP_SIZE = 224  # Should match your HLS chip size\n",
    "\n",
    "def load_ground_truth_chip(mask_path, target_size=224):\n",
    "    \"\"\"\n",
    "    Load a ground truth mask chip and convert to tensor.\n",
    "    \n",
    "    Args:\n",
    "        mask_path: Path to the mask GeoTIFF\n",
    "        target_size: Expected size (will resize if needed)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Shape (1, 1, H, W) with binary values {0, 1}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(mask_path) as src:\n",
    "            mask = src.read(1).astype(np.float32)\n",
    "            \n",
    "            # Ensure binary values (0 and 1)\n",
    "            mask = (mask > 0).astype(np.float32)\n",
    "            \n",
    "            # Convert to tensor: (H, W) -> (1, 1, H, W)\n",
    "            tensor = torch.from_numpy(mask).unsqueeze(0).unsqueeze(0)\n",
    "            \n",
    "            # Resize if needed\n",
    "            if mask.shape[0] != target_size or mask.shape[1] != target_size:\n",
    "                tensor = torch.nn.functional.interpolate(\n",
    "                    tensor, \n",
    "                    size=(target_size, target_size), \n",
    "                    mode='nearest'\n",
    "                )\n",
    "            \n",
    "            return tensor\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading mask {mask_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def pair_images_with_labels(image_tensors_dict, ground_truth_dir):\n",
    "    \"\"\"\n",
    "    Match HLS image chips with corresponding ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        image_tensors_dict: The processed_tensors dictionary from previous cells\n",
    "        ground_truth_dir: Directory containing ground truth masks\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (matched_images, matched_labels) as lists of tensors\n",
    "    \"\"\"\n",
    "    matched_images = []\n",
    "    matched_labels = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"PAIRING IMAGES WITH GROUND TRUTH LABELS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for district, phases in image_tensors_dict.items():\n",
    "        print(f\"\\nüìç District: {district}\")\n",
    "        \n",
    "        # We'll use post-flood images for training (when flooding is visible)\n",
    "        post_flood_tensors = phases.get('post_flood', [])\n",
    "        \n",
    "        # Look for corresponding mask directory\n",
    "        mask_dir = Path(ground_truth_dir) / district\n",
    "        \n",
    "        if not mask_dir.exists():\n",
    "            print(f\"   ‚ö†Ô∏è No mask directory found for {district}\")\n",
    "            continue\n",
    "        \n",
    "        # Get all mask files\n",
    "        mask_files = sorted(mask_dir.glob('*Flood_Mask*.tif'))\n",
    "        \n",
    "        if not mask_files:\n",
    "            print(f\"   ‚ö†Ô∏è No mask files found in {mask_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Match images with masks (assuming same naming convention)\n",
    "        pairs_found = 0\n",
    "        for img_idx, img_tensor in enumerate(post_flood_tensors):\n",
    "            # Try to find corresponding mask\n",
    "            if img_idx < len(mask_files):\n",
    "                mask_tensor = load_ground_truth_chip(str(mask_files[img_idx]))\n",
    "                \n",
    "                if mask_tensor is not None:\n",
    "                    matched_images.append(img_tensor)\n",
    "                    matched_labels.append(mask_tensor)\n",
    "                    pairs_found += 1\n",
    "        \n",
    "        print(f\"   ‚úÖ Paired {pairs_found} image-mask pairs\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ Total training pairs: {len(matched_images)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return matched_images, matched_labels\n",
    "\n",
    "\n",
    "# Execute pairing (using the processed_tensors from cell 9)\n",
    "if 'processed_tensors' in globals() and processed_tensors:\n",
    "    train_images, train_labels = pair_images_with_labels(processed_tensors, GROUND_TRUTH_DIR)\n",
    "    \n",
    "    if train_images and train_labels:\n",
    "        print(f\"\\nüìä Training data ready:\")\n",
    "        print(f\"   Images: {len(train_images)} samples\")\n",
    "        print(f\"   Labels: {len(train_labels)} samples\")\n",
    "        print(f\"   Image shape: {train_images[0].shape}\")\n",
    "        print(f\"   Label shape: {train_labels[0].shape}\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No training pairs found. Please check your ground truth directory.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Please run cell 9 first to generate processed_tensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1023809d",
   "metadata": {},
   "source": [
    "## **Step 2: Define the Fine-Tuning Model**\n",
    "We'll create a segmentation model using a pre-trained encoder (backbone) and add a decoder for flood detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6d511f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Initializing flood segmentation model...\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to C:\\Users\\Nipun/.cache\\torch\\hub\\checkpoints\\resnet34-333f7ec4.pth\n",
      "Downloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to C:\\Users\\Nipun/.cache\\torch\\hub\\checkpoints\\resnet34-333f7ec4.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 83.3M/83.3M [00:07<00:00, 11.7MB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model initialized: UNET with resnet34 encoder\n",
      "   Device: cpu\n",
      "   Input channels: 6\n",
      "   Output classes: 1\n",
      "   Total parameters: 24,445,777\n",
      "   Trainable parameters: 24,445,777\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# FLOOD SEGMENTATION MODEL\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import segmentation_models_pytorch as smp  # Popular library for segmentation\n",
    "\n",
    "class FloodSegmentationModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Flood detection segmentation model using a pre-trained encoder.\n",
    "    Uses U-Net architecture with a ResNet or EfficientNet encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name='resnet34', encoder_weights='imagenet', in_channels=6, classes=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_name: Backbone encoder (resnet34, resnet50, efficientnet-b0, etc.)\n",
    "            encoder_weights: Pre-trained weights ('imagenet' or None)\n",
    "            in_channels: Number of input channels (6 for HLS)\n",
    "            classes: Number of output classes (1 for binary segmentation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create U-Net model with pre-trained encoder\n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            activation=None  # We'll apply sigmoid during training/inference\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Alternative: Use DeepLabV3+ for better performance\n",
    "class FloodDeepLabModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Alternative model using DeepLabV3+ architecture.\n",
    "    Generally performs better than U-Net for segmentation tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder_name='resnet50', encoder_weights='imagenet', in_channels=6, classes=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = smp.DeepLabV3Plus(\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            in_channels=in_channels,\n",
    "            classes=classes,\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "print(\"üîß Initializing flood segmentation model...\")\n",
    "\n",
    "# Choose your model architecture\n",
    "MODEL_TYPE = 'unet'  # Options: 'unet' or 'deeplabv3plus'\n",
    "ENCODER = 'resnet34'  # Options: 'resnet34', 'resnet50', 'efficientnet-b0', 'mobilenet_v2'\n",
    "INPUT_CHANNELS = 6  # HLS has 6 bands\n",
    "OUTPUT_CLASSES = 1  # Binary segmentation (flood vs non-flood)\n",
    "\n",
    "if MODEL_TYPE == 'unet':\n",
    "    model = FloodSegmentationModel(\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=INPUT_CHANNELS,\n",
    "        classes=OUTPUT_CLASSES\n",
    "    )\n",
    "else:\n",
    "    model = FloodDeepLabModel(\n",
    "        encoder_name=ENCODER,\n",
    "        encoder_weights='imagenet',\n",
    "        in_channels=INPUT_CHANNELS,\n",
    "        classes=OUTPUT_CLASSES\n",
    "    )\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model initialized: {MODEL_TYPE.upper()} with {ENCODER} encoder\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Input channels: {INPUT_CHANNELS}\")\n",
    "print(f\"   Output classes: {OUTPUT_CLASSES}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41847bee",
   "metadata": {},
   "source": [
    "## **Step 3: Setup Training Pipeline**\n",
    "Configure the dataset, data loaders, loss function, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86637daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SETTING UP TRAINING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "üìä Dataset split:\n",
      "   Training samples: 72\n",
      "   Validation samples: 19\n",
      "\n",
      "üì¶ Data loaders created:\n",
      "   Batch size: 8\n",
      "   Training batches: 9\n",
      "   Validation batches: 3\n",
      "\n",
      "üéØ Loss function: Combined (BCE + Dice)\n",
      "\n",
      "‚öôÔ∏è Optimizer: AdamW\n",
      "   Learning rate: 0.0001\n",
      "   Weight decay: 1e-05\n",
      "   LR scheduler: ReduceLROnPlateau\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Training pipeline ready!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# TRAINING SETUP\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "\n",
    "# -------------------------\n",
    "# Custom Dataset\n",
    "# -------------------------\n",
    "class FloodDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for flood detection training.\"\"\"\n",
    "    \n",
    "    def __init__(self, images, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: List of image tensors (1, C, H, W)\n",
    "            labels: List of label tensors (1, 1, H, W)\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "        assert len(images) == len(labels), \"Images and labels must have same length\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Remove batch dimension for DataLoader (it will add it back)\n",
    "        image = self.images[idx].squeeze(0)  # (C, H, W)\n",
    "        label = self.labels[idx].squeeze(0)  # (1, H, W)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Loss Functions\n",
    "# -------------------------\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for segmentation tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred = pred.view(-1)\n",
    "        target = target.view(-1)\n",
    "        \n",
    "        intersection = (pred * target).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)\n",
    "        \n",
    "        return 1 - dice\n",
    "\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combination of BCE and Dice loss for better training.\"\"\"\n",
    "    \n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5):\n",
    "        super().__init__()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce = self.bce_loss(pred, target)\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        return self.bce_weight * bce + self.dice_weight * dice\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Setup Training Components\n",
    "# -------------------------\n",
    "\n",
    "# Check if we have training data\n",
    "if 'train_images' not in globals() or not train_images:\n",
    "    print(\"‚ö†Ô∏è No training data found. Please run the ground truth loading cell first.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SETTING UP TRAINING PIPELINE\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create dataset\n",
    "    full_dataset = FloodDataset(train_images, train_labels)\n",
    "    \n",
    "    # Split into train and validation (80/20 split)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "    \n",
    "    print(f\"\\nüìä Dataset split:\")\n",
    "    print(f\"   Training samples: {len(train_dataset)}\")\n",
    "    print(f\"   Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    BATCH_SIZE = 8  # Adjust based on your GPU memory\n",
    "    NUM_WORKERS = 4  # Adjust based on your CPU cores\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nüì¶ Data loaders created:\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "    print(f\"   Training batches: {len(train_loader)}\")\n",
    "    print(f\"   Validation batches: {len(val_loader)}\")\n",
    "    \n",
    "    # Define loss function\n",
    "    criterion = CombinedLoss(bce_weight=0.5, dice_weight=0.5)\n",
    "    print(f\"\\nüéØ Loss function: Combined (BCE + Dice)\")\n",
    "    \n",
    "    # Define optimizer\n",
    "    LEARNING_RATE = 1e-4\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n‚öôÔ∏è Optimizer: AdamW\")\n",
    "    print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
    "    print(f\"   LR scheduler: ReduceLROnPlateau\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ Training pipeline ready!\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce00de6e",
   "metadata": {},
   "source": [
    "## **Step 4: Training Loop**\n",
    "Execute the fine-tuning process with validation and checkpoint saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6ee4cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "STARTING FINE-TUNING\n",
      "======================================================================\n",
      "Epochs: 50\n",
      "Device: cpu\n",
      "Checkpoint directory: C:\\Kaam_Dhanda\\Minor_Project\\model_checkpoints\n",
      "======================================================================\n",
      "\n",
      "\n",
      "üìÖ Epoch 1/50\n",
      "----------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/9 [00:05<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 31080, 24760, 17068, 17376) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1275\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1274\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1275\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m)\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m train_loss, train_iou = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m train_losses.append(train_loss)\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     40\u001b[39m running_iou = \u001b[32m0.0\u001b[39m\n\u001b[32m     42\u001b[39m pbar = tqdm(train_loader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpbar\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1482\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1479\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1444\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1440\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1441\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1444\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1445\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1446\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nipun\\miniconda3\\envs\\minor_project\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1288\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1287\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1289\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1290\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1291\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1292\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 31080, 24760, 17068, 17376) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# TRAINING LOOP\n",
    "# =================================================================\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create checkpoint directory\n",
    "CHECKPOINT_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\model_checkpoints'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 50\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "SAVE_BEST_ONLY = True\n",
    "\n",
    "# Metrics tracking\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "\n",
    "def calculate_iou(pred, target, threshold=0.5):\n",
    "    \"\"\"Calculate Intersection over Union (IoU) metric.\"\"\"\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    \n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return iou.item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        # Move data to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        running_loss += loss.item()\n",
    "        running_iou += calculate_iou(outputs, labels)\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': running_loss / (batch_idx + 1),\n",
    "            'iou': running_iou / (batch_idx + 1)\n",
    "        })\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_iou = running_iou / len(train_loader)\n",
    "    \n",
    "    return epoch_loss, epoch_iou\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_iou = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_loader, desc='Validation')\n",
    "        for batch_idx, (images, labels) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            running_iou += calculate_iou(outputs, labels)\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss / (batch_idx + 1),\n",
    "                'iou': running_iou / (batch_idx + 1)\n",
    "            })\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_iou = running_iou / len(val_loader)\n",
    "    \n",
    "    return epoch_loss, epoch_iou\n",
    "\n",
    "\n",
    "# Check if training data is ready\n",
    "if 'train_loader' not in globals():\n",
    "    print(\"‚ö†Ô∏è Training pipeline not set up. Please run previous cells first.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"STARTING FINE-TUNING\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"Device: {device}\")\n",
    "    print(f\"Checkpoint directory: {CHECKPOINT_DIR}\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nüìÖ Epoch {epoch + 1}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_iou = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_iou = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
    "        print(f\"   Train Loss: {train_loss:.4f} | Train IoU: {train_iou:.4f}\")\n",
    "        print(f\"   Val Loss:   {val_loss:.4f} | Val IoU:   {val_iou:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "            \n",
    "            checkpoint_path = os.path.join(CHECKPOINT_DIR, 'best_model.pth')\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_iou': val_iou,\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            print(f\"   ‚úÖ Saved best model (Val Loss: {val_loss:.4f})\")\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if epochs_without_improvement >= EARLY_STOP_PATIENCE:\n",
    "            print(f\"\\n‚ö†Ô∏è Early stopping triggered after {epoch + 1} epochs\")\n",
    "            print(f\"   No improvement for {EARLY_STOP_PATIENCE} epochs\")\n",
    "            break\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ FINE-TUNING COMPLETE!\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Model saved to: {CHECKPOINT_DIR}\")\n",
    "    \n",
    "    # Plot training curves\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, 'o-', label='Train')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, 's-', label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss Progression')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_curves.png'), dpi=150)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nüìà Training curves saved to: {os.path.join(CHECKPOINT_DIR, 'training_curves.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40901eae",
   "metadata": {},
   "source": [
    "## **Step 5: Inference and Prediction**\n",
    "Use the trained model to generate flood predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "490f70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Trained model not found. Please complete training first.\n",
      "\n",
      "üìä FLOOD STATISTICS:\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# INFERENCE AND FLOOD MAP GENERATION\n",
    "# =================================================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from pathlib import Path\n",
    "\n",
    "# Output directory for predictions\n",
    "PREDICTION_OUTPUT_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Flood_Maps_AI'\n",
    "os.makedirs(PREDICTION_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "def load_best_model(checkpoint_path, model, device):\n",
    "    \"\"\"Load the best model from checkpoint.\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    print(f\"‚úÖ Loaded model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"   Validation Loss: {checkpoint['val_loss']:.4f}\")\n",
    "    print(f\"   Validation IoU: {checkpoint['val_iou']:.4f}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_chip(model, image_tensor, device, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Generate prediction for a single chip.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image_tensor: Input image tensor (1, C, H, W)\n",
    "        device: torch device\n",
    "        threshold: Probability threshold for binary classification\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Binary mask (H, W)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Move to device\n",
    "        image = image_tensor.to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        output = model(image)\n",
    "        \n",
    "        # Apply sigmoid and threshold\n",
    "        prob = torch.sigmoid(output)\n",
    "        pred_mask = (prob > threshold).cpu().numpy()\n",
    "        \n",
    "        # Remove batch and channel dimensions\n",
    "        pred_mask = pred_mask.squeeze()\n",
    "        \n",
    "    return pred_mask.astype(np.uint8)\n",
    "\n",
    "\n",
    "def generate_flood_maps(model, image_tensors_dict, output_dir, device):\n",
    "    \"\"\"\n",
    "    Generate flood maps for all districts.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model\n",
    "        image_tensors_dict: Dictionary of tensors from preprocessing\n",
    "        output_dir: Where to save predictions\n",
    "        device: torch device\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GENERATING FLOOD PREDICTIONS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for district, phases in image_tensors_dict.items():\n",
    "        print(f\"\\nüèûÔ∏è Processing {district}...\")\n",
    "        \n",
    "        # Create district output directory\n",
    "        district_dir = os.path.join(output_dir, district)\n",
    "        os.makedirs(district_dir, exist_ok=True)\n",
    "        \n",
    "        # Process post-flood images (where flooding is visible)\n",
    "        post_flood_tensors = phases.get('post_flood', [])\n",
    "        \n",
    "        if not post_flood_tensors:\n",
    "            print(f\"   ‚ö†Ô∏è No post-flood images found\")\n",
    "            continue\n",
    "        \n",
    "        predictions_made = 0\n",
    "        \n",
    "        for idx, image_tensor in enumerate(tqdm(post_flood_tensors, desc=f\"  {district}\")):\n",
    "            # Generate prediction\n",
    "            pred_mask = predict_chip(model, image_tensor, device)\n",
    "            \n",
    "            # Save prediction as GeoTIFF\n",
    "            # Note: You'll need to get the geospatial metadata from original chips\n",
    "            output_path = os.path.join(district_dir, f'{district}_FloodPrediction_chip_{idx}.tif')\n",
    "            \n",
    "            # For now, save as simple array (you can add geospatial info later)\n",
    "            pred_mask_rgb = (pred_mask * 255).astype(np.uint8)\n",
    "            \n",
    "            # Create a simple GeoTIFF\n",
    "            with rasterio.open(\n",
    "                output_path,\n",
    "                'w',\n",
    "                driver='GTiff',\n",
    "                height=pred_mask.shape[0],\n",
    "                width=pred_mask.shape[1],\n",
    "                count=1,\n",
    "                dtype=rasterio.uint8,\n",
    "                compress='LZW'\n",
    "            ) as dst:\n",
    "                dst.write(pred_mask, 1)\n",
    "            \n",
    "            predictions_made += 1\n",
    "        \n",
    "        print(f\"   ‚úÖ Generated {predictions_made} prediction masks\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"‚úÖ All predictions saved to: {output_dir}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "\n",
    "# Execute inference\n",
    "if 'model' in globals() and os.path.exists(os.path.join(CHECKPOINT_DIR, 'best_model.pth')):\n",
    "    print(\"üîÆ Starting inference with trained model...\")\n",
    "    \n",
    "    # Load best model\n",
    "    model = load_best_model(\n",
    "        os.path.join(CHECKPOINT_DIR, 'best_model.pth'),\n",
    "        model,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    # Generate predictions\n",
    "    if 'processed_tensors' in globals() and processed_tensors:\n",
    "        generate_flood_maps(model, processed_tensors, PREDICTION_OUTPUT_DIR, device)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No processed tensors found. Please run the preprocessing cells first.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Trained model not found. Please complete training first.\")\n",
    "\n",
    "\n",
    "# Calculate flood statistics\n",
    "print(\"\\nüìä FLOOD STATISTICS:\")\n",
    "for district in os.listdir(PREDICTION_OUTPUT_DIR):\n",
    "    district_path = os.path.join(PREDICTION_OUTPUT_DIR, district)\n",
    "    if os.path.isdir(district_path):\n",
    "        mask_files = list(Path(district_path).glob('*.tif'))\n",
    "        \n",
    "        total_pixels = 0\n",
    "        flood_pixels = 0\n",
    "        \n",
    "        for mask_file in mask_files:\n",
    "            with rasterio.open(mask_file) as src:\n",
    "                mask = src.read(1)\n",
    "                total_pixels += mask.size\n",
    "                flood_pixels += np.sum(mask == 1)\n",
    "        \n",
    "        if total_pixels > 0:\n",
    "            flood_percent = (flood_pixels / total_pixels) * 100\n",
    "            print(f\"\\n{district}:\")\n",
    "            print(f\"   Flooded pixels: {flood_pixels:,}\")\n",
    "            print(f\"   Total pixels: {total_pixels:,}\")\n",
    "            print(f\"   Flooded area: {flood_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df491a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import BCEWithLogitsLoss # Loss function for binary segmentation\n",
    "\n",
    "# --- 1. Custom Dataset Definition (Crucial for PyTorch) ---\n",
    "class FloodDataset(Dataset):\n",
    "    def __init__(self, input_tensors, label_tensors):\n",
    "        # input_tensors are your X (HLS images)\n",
    "        self.inputs = input_tensors \n",
    "        # label_tensors are your Y (Ground Truth Masks)\n",
    "        self.labels = label_tensors \n",
    "\n",
    "    def __len__(self):\n",
    "        # The length of the dataset is the number of chips\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return a pair of (Image, Label) for the training loop\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# --- 2. Training Loop Setup ---\n",
    "def setup_training(model, train_inputs, train_labels):\n",
    "    # A. Create Dataset and DataLoader\n",
    "    train_dataset = FloodDataset(train_inputs, train_labels)\n",
    "    # DataLoader manages batching and shuffling\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True) \n",
    "\n",
    "    # B. Define Loss and Optimizer\n",
    "    criterion = BCEWithLogitsLoss() # Good loss function for binary segmentation\n",
    "    optimizer = Adam(model.parameters(), lr=1e-5) # Use a small learning rate for fine-tuning\n",
    "\n",
    "    # C. Start Training (Conceptual loop structure)\n",
    "    num_epochs = 10\n",
    "    print(f\"Starting fine-tuning for {num_epochs} epochs...\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Set model to training mode\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # --- FORWARD PASS (Model Prediction) ---\n",
    "            outputs = model(inputs) \n",
    "            \n",
    "            # --- BACKWARD PASS (Learning) ---\n",
    "            loss = criterion(outputs, targets.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} complete. Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "# --- 3. The FINAL STEP (Inference) ---\n",
    "# After training, you replace model.train() with model.eval() and run inference \n",
    "# on the remaining (unlabeled) chips to generate your final flood masks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c5cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics.classification import BinaryJaccardIndex # For IoU metric\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "# from your notebook: from terratorch.registry import BACKBONE_REGISTRY \n",
    "# from your notebook: from PrithviFloodSegmentationModel import PrithviFloodSegmentationModel \n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# I. HYPERPARAMETERS & CONFIGURATION\n",
    "# =================================================================\n",
    "# NOTE: Replace 'your-project-id' with a unique identifier\n",
    "PROJECT_NAME = 'Flood_Mapping_Assam_HLS' \n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# II. DATA PIPELINE (The glue for your tensors)\n",
    "# =================================================================\n",
    "\n",
    "class FloodDataset(Dataset):\n",
    "    \"\"\"Dataset for pairing HLS input tensors (X) with Ground Truth label tensors (Y).\"\"\"\n",
    "    def __init__(self, input_tensors_list, label_tensors_list):\n",
    "        # input_tensors_list should be a list of (1, 6, 512, 512) HLS chips\n",
    "        self.inputs = input_tensors_list \n",
    "        # label_tensors_list should be a list of (1, 1, 512, 512) binary masks\n",
    "        self.labels = label_tensors_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # We need to squeeze the batch dimension (0) for PyTorch Lightning, \n",
    "        # as the DataLoader will add it back.\n",
    "        X = self.inputs[idx].squeeze(0) \n",
    "        Y = self.labels[idx].squeeze(0).long() # Labels must be Long type for loss function\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "# NOTE: For this step, you must load your ground truth labels manually \n",
    "# and populate the following two lists before calling the trainer.\n",
    "# train_inputs_list = [t for d in processed_tensors for t in d['pre_flood']]\n",
    "# train_labels_list = [load_label(path) for path in label_paths]\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# III. FINE-TUNING MODULE (The brains of the operation)\n",
    "# =================================================================\n",
    "\n",
    "class PrithviFloodModule(pl.LightningModule):\n",
    "    def __init__(self, output_classes=2):\n",
    "        super().__init__()\n",
    "        # Initialize the model structure you defined in the previous step\n",
    "        self.model = PrithviFloodSegmentationModel(output_classes=output_classes)\n",
    "        \n",
    "        # Define the Loss Function (Binary Cross-Entropy + Dice Loss is standard for segmentation)\n",
    "        # Note: BCEWithLogitsLoss is robust and combines Sigmoid + BCE\n",
    "        self.criterion = nn.BCEWithLogitsLoss() \n",
    "        \n",
    "        # Define the Evaluation Metric (IoU is Jaccard Index for binary problems)\n",
    "        self.iou_metric = BinaryJaccardIndex().to(self.device)\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # Model output is logits (before sigmoid)\n",
    "        logits = self.forward(x) \n",
    "        \n",
    "        # Reshape the label tensor to match the output shape for loss calculation\n",
    "        # [B, 2, H, W] vs [B, 1, H, W]. Use only the background/foreground channel for loss.\n",
    "        loss = self.criterion(logits[:, 1], y.float()) # Target: [B, H, W]\n",
    "        \n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self.forward(x)\n",
    "        \n",
    "        # Apply sigmoid to logits to get probabilities, then round to get prediction (0 or 1)\n",
    "        preds = torch.sigmoid(logits[:, 1]).round() \n",
    "        \n",
    "        val_loss = self.criterion(logits[:, 1], y.float())\n",
    "        self.log('val_loss', val_loss)\n",
    "        \n",
    "        # Log IoU metric\n",
    "        self.iou_metric.update(preds, y)\n",
    "        self.log('val_iou', self.iou_metric, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "# =================================================================\n",
    "# IV. EXECUTION\n",
    "# =================================================================\n",
    "\n",
    "def run_fine_tuning_pipeline(train_inputs, train_labels):\n",
    "    # 1. Setup DataModule/Dataset\n",
    "    train_dataset = FloodDataset(train_inputs, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    \n",
    "    # 2. Instantiate Model and Lightning Module\n",
    "    model_module = PrithviFloodModule(learning_rate=LEARNING_RATE, output_classes=2)\n",
    "    \n",
    "    # 3. Setup Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        logger=pl.loggers.CSVLogger(save_dir='logs/', name=PROJECT_NAME),\n",
    "        callbacks=[pl.callbacks.ModelCheckpoint(monitor='val_iou', mode='max')]\n",
    "    )\n",
    "    \n",
    "    # 4. Start Fine-Tuning\n",
    "    trainer.fit(model_module, train_loader)\n",
    "    print(\"‚úÖ Fine-Tuning Complete. Model weights saved to logs/ directory.\")\n",
    "\n",
    "# --- NEXT ACTION ---\n",
    "# You must execute Step 1 (create labels) and then run this pipeline.\n",
    "# Example Call (Conceptual - requires actual data lists):\n",
    "# run_fine_tuning_pipeline(your_input_list, your_label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7f65f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have loaded your Ground Truth labels into a list called train_labels_list\n",
    "# and your image inputs into a list called train_inputs_list\n",
    "# (These lists are created by iterating over your local file system).\n",
    "\n",
    "# --- FINAL EXECUTION ---\n",
    "# You must ensure the two lists contain matching tensors before running.\n",
    "# Example: train_inputs_list = [t for d in processed_tensors for t in d['pre_flood']]\n",
    "#          train_labels_list = [load_label(path) for path in your_label_files]\n",
    "\n",
    "def execute_final_training(train_inputs_list, train_labels_list):\n",
    "    if not train_inputs_list or not train_labels_list:\n",
    "        print(\"‚ùå Error: Input or Label lists are empty. Cannot start training.\")\n",
    "        return\n",
    "\n",
    "    # 1. Setup Data\n",
    "    # This calls the FloodDataset and DataLoader you defined previously.\n",
    "    train_dataset = FloodDataset(train_inputs_list, train_labels_list)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "    # 2. Instantiate Model (Prithvi-600M wrapped in the Lightning Module)\n",
    "    # The PrithviFloodModule class must be defined in your script environment.\n",
    "    model_module = PrithviFloodModule(learning_rate=1e-5, output_classes=2)\n",
    "\n",
    "    # 3. Setup Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        logger=pl.loggers.CSVLogger(save_dir='logs/', name=PROJECT_NAME),\n",
    "        callbacks=[pl.callbacks.ModelCheckpoint(monitor='val_iou', mode='max')]\n",
    "    )\n",
    "\n",
    "    # 4. Start Fine-Tuning\n",
    "    trainer.fit(model_module, train_loader)\n",
    "    print(\"‚úÖ Fine-Tuning Complete. Check the 'logs/' directory for model weights.\")\n",
    "\n",
    "# NOTE: You must call this function with your actual loaded lists:\n",
    "# execute_final_training(your_inputs_list, your_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c91998b",
   "metadata": {},
   "source": [
    "# **Chipping Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c5e995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "import numpy as np\n",
    "import torch\n",
    "from rasterio.merge import merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8e509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the input and output chip size\n",
    "CHIP_SIZE = 512 \n",
    "\n",
    "def chip_image(input_filepath, output_directory):\n",
    "    \"\"\"Cuts a large GeoTIFF into smaller chips.\"\"\"\n",
    "    with rasterio.open(input_filepath) as src:\n",
    "        # Get the width and height of the entire image\n",
    "        width = src.width\n",
    "        height = src.height\n",
    "        \n",
    "        count = 0\n",
    "        # Loop through the image in chunks of CHIP_SIZE\n",
    "        for i in range(0, height, CHIP_SIZE):\n",
    "            for j in range(0, width, CHIP_SIZE):\n",
    "                \n",
    "                # Define the window (area) to read from the large image\n",
    "                # The window accounts for the edges where the size might be less than CHIP_SIZE\n",
    "                window = Window(j, i, min(CHIP_SIZE, width - j), min(CHIP_SIZE, height - i))\n",
    "                transform = src.window_transform(window)\n",
    "                \n",
    "                # Read the data from the defined window\n",
    "                chip_data = src.read(1, window=window)\n",
    "\n",
    "                # Skip if the chip contains mostly \"no data\" values (e.g., beyond your AOI)\n",
    "                if np.sum(chip_data == src.nodata) / chip_data.size > 0.95:\n",
    "                    continue\n",
    "\n",
    "                # Update the metadata profile for the new small chip file\n",
    "                profile = src.profile\n",
    "                profile.update({\n",
    "                    'height': window.height,\n",
    "                    'width': window.width,\n",
    "                    'transform': transform\n",
    "                })\n",
    "                \n",
    "                # Save the chip\n",
    "                output_path = f\"{output_directory}/{src.name.split('/')[-1].replace('.tif', '')}_chip_{count}.tif\"\n",
    "                with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                    dst.write(chip_data, 1)\n",
    "                \n",
    "                count += 1\n",
    "                \n",
    "    print(f\"Successfully chipped {input_filepath} into {count} tiles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9b332e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully prepared 5 district pairs for chipping.\n",
      "\n",
      "--- Chipping files for Barpeta ---\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Barpeta_PreFlood_Image.tif into 96 tiles.\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Barpeta_PostFlood_Image.tif into 96 tiles.\n",
      "\n",
      "--- Chipping files for Dhemaji ---\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Dhemaji_PreFlood_Image.tif into 104 tiles.\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Dhemaji_PostFlood_Image.tif into 104 tiles.\n",
      "\n",
      "--- Chipping files for Lakhimpur ---\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Lakhimpur_PreFlood_Image.tif into 324 tiles.\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Lakhimpur_PostFlood_Image.tif into 324 tiles.\n",
      "\n",
      "--- Chipping files for Nalbari ---\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Nalbari_PreFlood_Image.tif into 80 tiles.\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Nalbari_PostFlood_Image.tif into 80 tiles.\n",
      "\n",
      "--- Chipping files for Sonitpur ---\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Sonitpur_PreFlood_Image.tif into 198 tiles.\n",
      "‚úÖ Successfully chipped C:\\Kaam_Dhanda\\Minor_Project\\Old Images\\Sonitpur_PostFlood_Image.tif into 198 tiles.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================\n",
    "# GLOBAL CONFIGURATION\n",
    "# =================================================================\n",
    "\n",
    "# IMPORTANT: Use 'r' strings for Windows paths to avoid SyntaxWarnings/Errors\n",
    "TIF_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Old Images' \n",
    "OUTPUT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips' \n",
    "\n",
    "# List of districts to process\n",
    "DISTRICTS = ['Barpeta', 'Dhemaji', 'Lakhimpur', 'Nalbari', 'Sonitpur']\n",
    "CHIP_SIZE = 512 # Standard size for deep learning input (e.g., 512x512 pixels)\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# CHIPPING FUNCTION (Core Logic)\n",
    "# =================================================================\n",
    "\n",
    "def chip_image(input_filepath, output_directory):\n",
    "    \"\"\"Cuts a large GeoTIFF into smaller, non-overlapping chips.\"\"\"\n",
    "    \n",
    "    # 1. Safely open the input image\n",
    "    try:\n",
    "        src = rasterio.open(input_filepath)\n",
    "    except rasterio.RasterioIOError as e:\n",
    "        print(f\"Error opening input file {input_filepath}: {e}\")\n",
    "        return\n",
    "\n",
    "    width = src.width\n",
    "    height = src.height\n",
    "    count = 0\n",
    "    \n",
    "    # Loop through the image in chunks of CHIP_SIZE\n",
    "    for i in range(0, height, CHIP_SIZE):\n",
    "        for j in range(0, width, CHIP_SIZE):\n",
    "            \n",
    "            # Define the window (area) to read from the large image\n",
    "            window = Window(j, i, min(CHIP_SIZE, width - j), min(CHIP_SIZE, height - i))\n",
    "            transform = src.window_transform(window)\n",
    "            \n",
    "            # Read the data from the defined window (assuming single band: 'VV')\n",
    "            chip_data = src.read(1, window=window)\n",
    "\n",
    "            # Skip if the chip contains mostly 'no data' values (e.g., beyond your AOI)\n",
    "            if np.sum(chip_data == src.nodata) / chip_data.size > 0.95:\n",
    "                continue\n",
    "\n",
    "            # Update the metadata profile for the new small chip file\n",
    "            profile = src.profile\n",
    "            profile.update({\n",
    "                'height': window.height,\n",
    "                'width': window.width,\n",
    "                'transform': transform,\n",
    "                'count': 1, # Ensure the profile reflects a single band\n",
    "                'compress': 'LZW' # Optional: Add compression to reduce chip size\n",
    "            })\n",
    "            \n",
    "            # --- CRITICAL FIX: Robust Output Path Construction ---\n",
    "            # 1. Get the base filename (e.g., 'Barpeta_PreFlood_Image.tif')\n",
    "            base_filename = os.path.basename(input_filepath)\n",
    "            \n",
    "            # 2. Remove the '.tif' extension for the chip name stem\n",
    "            file_stem = base_filename.replace('.tif', '')\n",
    "\n",
    "            # 3. Construct the final output path using os.path.join()\n",
    "            chip_filename = f'{file_stem}_chip_{count}.tif'\n",
    "            output_path = os.path.join(output_directory, chip_filename)\n",
    "            \n",
    "            # 4. Save the chip\n",
    "            try:\n",
    "                with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "                    dst.write(chip_data, 1)\n",
    "                count += 1\n",
    "            except rasterio.RasterioIOError as e:\n",
    "                 print(f\"Failed to write chip {output_path}: {e}\")\n",
    "\n",
    "    src.close()\n",
    "    print(f\"‚úÖ Successfully chipped {input_filepath} into {count} tiles.\")\n",
    "\n",
    "\n",
    "# =================================================================\n",
    "# MAIN EXECUTION LOGIC\n",
    "# =================================================================\n",
    "\n",
    "files_to_chip = {}\n",
    "\n",
    "# --- Generate File Pairs ---\n",
    "for district in DISTRICTS:\n",
    "    pre_file = os.path.join(TIF_DIR, f'{district}_PreFlood_Image.tif')\n",
    "    post_file = os.path.join(TIF_DIR, f'{district}_PostFlood_Image.tif')\n",
    "    \n",
    "    if os.path.exists(pre_file) and os.path.exists(post_file):\n",
    "        files_to_chip[district] = {\n",
    "            'pre_flood': pre_file,\n",
    "            'post_flood': post_file\n",
    "        }\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipping {district}: One or both primary files were not found.\")\n",
    "        \n",
    "print(f\"Successfully prepared {len(files_to_chip)} district pairs for chipping.\")\n",
    "\n",
    "\n",
    "# --- Run Chipping Process ---\n",
    "for district, files in files_to_chip.items():\n",
    "    print(f\"\\n--- Chipping files for {district} ---\")\n",
    "    \n",
    "    # Define the output directories\n",
    "    pre_output_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'pre_flood')\n",
    "    post_output_dir = os.path.join(OUTPUT_CHIPS_DIR, district, 'post_flood')\n",
    "\n",
    "    # Create the output directories if they don't exist\n",
    "    os.makedirs(pre_output_dir, exist_ok=True)\n",
    "    os.makedirs(post_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Run chipping for the pre-flood image\n",
    "    chip_image(\n",
    "        input_filepath=files['pre_flood'],\n",
    "        output_directory=pre_output_dir\n",
    "    )\n",
    "    \n",
    "    # Run chipping for the post-flood image\n",
    "    chip_image(\n",
    "        input_filepath=files['post_flood'],\n",
    "        output_directory=post_output_dir\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337e42fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch pre-processing from: C:\\Kaam_Dhanda\\Minor_Project\\Output_chips\n",
      "\n",
      "--- Processing District: Barpeta ---\n",
      "\n",
      "--- Processing District: Dhemaji ---\n",
      "\n",
      "--- Processing District: Lakhimpur ---\n",
      "\n",
      "--- Processing District: Nalbari ---\n",
      "\n",
      "--- Processing District: Sonitpur ---\n",
      "\n",
      "=======================================================\n",
      "‚úÖ Batch Pre-processing Complete.\n",
      "=======================================================\n",
      "District: Barpeta\n",
      "  pre_flood: 96 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "  post_flood: 96 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "District: Dhemaji\n",
      "  pre_flood: 104 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "  post_flood: 104 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "District: Lakhimpur\n",
      "  pre_flood: 324 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "  post_flood: 324 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "District: Nalbari\n",
      "  pre_flood: 80 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "  post_flood: 80 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "District: Sonitpur\n",
      "  pre_flood: 198 chips, each with shape torch.Size([1, 1, 512, 512])\n",
      "  post_flood: 198 chips, each with shape torch.Size([1, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import rasterio\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# =======================================================================\n",
    "# CONFIGURATION\n",
    "# =======================================================================\n",
    "\n",
    "# IMPORTANT: SET YOUR ROOT DIRECTORY HERE\n",
    "ROOT_CHIPS_DIR = r'C:\\Kaam_Dhanda\\Minor_Project\\Output_chips'\n",
    "\n",
    "# Sentinel-1 Normalization Parameters for VV (based on common practice)\n",
    "# NOTE: These are general values. For maximum accuracy, check the specific\n",
    "# Prithvi-600m documentation for its exact SAR data normalization.\n",
    "SAR_NORM_MEAN = -15.0  # Common mean for VV dB values\n",
    "SAR_NORM_STD = 5.0    # Common standard deviation for VV dB values\n",
    "\n",
    "# Dictionary to store all processed tensors\n",
    "processed_tensors = {}\n",
    "\n",
    "# =======================================================================\n",
    "# CORE PROCESSING FUNCTION\n",
    "# =======================================================================\n",
    "\n",
    "def preprocess_sar_chip(file_path, sar_mean, sar_std):\n",
    "    \"\"\"\n",
    "    Reads a single-band SAR GeoTIFF, standardizes it, and converts it\n",
    "    to a PyTorch Tensor (1, C=1, H, W) for model inference.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with rasterio.open(file_path) as src:\n",
    "            # Read the single band (VV)\n",
    "            data = src.read(1).astype(np.float32)\n",
    "            \n",
    "            # Check for empty data / no-data values\n",
    "            if np.all(data == src.nodata):\n",
    "                return None\n",
    "\n",
    "    except rasterio.RasterioIOError:\n",
    "        print(f\"Error: Could not open or read {file_path}. Skipping.\")\n",
    "        return None\n",
    "\n",
    "    # 1. Standardization (Z-Score Normalization)\n",
    "    # Apply Z-score: (Data - Mean) / Std Dev\n",
    "    normalized_data = (data - sar_mean) / sar_std\n",
    "\n",
    "    # 2. Convert to PyTorch Tensor\n",
    "    # Reshape from (H, W) to (C, H, W) -> (1, H, W)\n",
    "    tensor = torch.from_numpy(normalized_data).unsqueeze(0)\n",
    "    \n",
    "    # Add a batch dimension, making the shape (1, C, H, W) -> (1, 1, H, W)\n",
    "    tensor = tensor.unsqueeze(0) \n",
    "\n",
    "    return tensor\n",
    "\n",
    "# =======================================================================\n",
    "# BATCH EXECUTION\n",
    "# =======================================================================\n",
    "\n",
    "print(f\"Starting batch pre-processing from: {ROOT_CHIPS_DIR}\")\n",
    "\n",
    "# Iterate through all district folders (Barpeta, Dhemaji, etc.)\n",
    "for district_name in os.listdir(ROOT_CHIPS_DIR):\n",
    "    district_path = os.path.join(ROOT_CHIPS_DIR, district_name)\n",
    "    \n",
    "    if not os.path.isdir(district_path):\n",
    "        continue\n",
    "\n",
    "    processed_tensors[district_name] = {'pre_flood': [], 'post_flood': []}\n",
    "    print(f\"\\n--- Processing District: {district_name} ---\")\n",
    "\n",
    "    # Iterate through 'pre_flood' and 'post_flood' folders\n",
    "    for phase in ['pre_flood', 'post_flood']:\n",
    "        phase_path = os.path.join(district_path, phase)\n",
    "        \n",
    "        if not os.path.isdir(phase_path):\n",
    "            continue\n",
    "\n",
    "        # Process all .tif files (image chips) in the phase folder\n",
    "        for chip_filename in os.listdir(phase_path):\n",
    "            if chip_filename.endswith('.tif'):\n",
    "                chip_file_path = os.path.join(phase_path, chip_filename)\n",
    "                \n",
    "                # Run the core pre-processing function\n",
    "                tensor = preprocess_sar_chip(\n",
    "                    chip_file_path, SAR_NORM_MEAN, SAR_NORM_STD\n",
    "                )\n",
    "                \n",
    "                if tensor is not None:\n",
    "                    # Store the resulting tensor\n",
    "                    processed_tensors[district_name][phase].append(tensor)\n",
    "                    # print(f\"    Processed: {chip_filename}\")\n",
    "\n",
    "# =======================================================================\n",
    "# FINAL CHECK\n",
    "# =======================================================================\n",
    "\n",
    "print(\"\\n=======================================================\")\n",
    "print(\"‚úÖ Batch Pre-processing Complete.\")\n",
    "print(\"=======================================================\")\n",
    "\n",
    "# Print the final structure for verification\n",
    "for district, phases in processed_tensors.items():\n",
    "    print(f\"District: {district}\")\n",
    "    for phase, tensors in phases.items():\n",
    "        if tensors:\n",
    "            # Check the shape of the first tensor in the list\n",
    "            print(f\"  {phase}: {len(tensors)} chips, each with shape {tensors[0].shape}\")\n",
    "        else:\n",
    "            print(f\"  {phase}: 0 chips found.\")\n",
    "\n",
    "# The 'processed_tensors' dictionary now holds all your data ready for the Prithvi model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3160305f",
   "metadata": {},
   "source": [
    "## **run the temporal AI inference and then stitch the predictions back together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87212e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_chip_pair(pre_path, post_path):\n",
    "    with rasterio.open(pre_path) as src_pre, rasterio.open(post_path) as src_post:\n",
    "        # Load data as NumPy arrays (assuming single band, VV polarization)\n",
    "        pre_chip = src_pre.read(1)\n",
    "        post_chip = src_post.read(1)\n",
    "        \n",
    "        # Stack them to create the temporal input (e.g., shape: 2, 512, 512)\n",
    "        temporal_input = np.stack([pre_chip, post_chip], axis=0)\n",
    "        \n",
    "        # Convert to PyTorch Tensor, add a batch dimension (1), and move to GPU (if available)\n",
    "        tensor_input = torch.from_numpy(temporal_input).float().unsqueeze(0)\n",
    "        \n",
    "        # Store the geospatial profile for later stitching\n",
    "        profile = src_pre.profile\n",
    "        \n",
    "    return tensor_input, profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3fb674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_inference_and_save(pre_path, post_path, output_mask_dir):\n",
    "    tensor_input, profile = prepare_chip_pair(pre_path, post_path)\n",
    "    \n",
    "    # 1. Run the prediction\n",
    "    # model.eval() is required for inference mode\n",
    "    with torch.no_grad():\n",
    "        # output is typically a logit map (e.g., shape: 1, num_classes, 512, 512)\n",
    "        output_logits = model(tensor_input) \n",
    "        \n",
    "    # 2. Get the final classification (0 or 1)\n",
    "    # This finds the class with the highest probability (e.g., 0=not-flood, 1=flood)\n",
    "    # Reshape and convert back to a NumPy array (shape: 512, 512)\n",
    "    predicted_mask_tensor = torch.argmax(output_logits, dim=1).squeeze().cpu()\n",
    "    predicted_mask_array = predicted_mask_tensor.numpy().astype(rasterio.uint8)\n",
    "    \n",
    "    # 3. Save the prediction mask\n",
    "    chip_filename = os.path.basename(pre_path).replace('PreFlood_Image', 'Flood_Mask')\n",
    "    output_path = os.path.join(output_mask_dir, chip_filename)\n",
    "    \n",
    "    # Update profile to reflect the new data type (binary mask)\n",
    "    profile.update(dtype=rasterio.uint8, count=1) \n",
    "    \n",
    "    with rasterio.open(output_path, 'w', **profile) as dst:\n",
    "        dst.write(predicted_mask_array, 1)\n",
    "    \n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4226179",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def stitch_masks(mask_dir, district_name, final_output_dir):\n",
    "    \"\"\"Stitches all predicted flood mask chips into a single GeoTIFF.\"\"\"\n",
    "    \n",
    "    mask_files = [os.path.join(mask_dir, f) for f in os.listdir(mask_dir) if f.endswith('.tif')]\n",
    "    \n",
    "    # Open all mask datasets\n",
    "    sources = [rasterio.open(f) for f in mask_files]\n",
    "    \n",
    "    # Use rasterio.merge to create a mosaic\n",
    "    stitched_array, out_transform = merge(sources)\n",
    "    \n",
    "    # Get the metadata from the first source file\n",
    "    out_meta = sources[0].profile.copy()\n",
    "    \n",
    "    # Update the metadata for the merged output\n",
    "    out_meta.update({\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"height\": stitched_array.shape[1],\n",
    "        \"width\": stitched_array.shape[2],\n",
    "        \"transform\": out_transform,\n",
    "        \"count\": 1,\n",
    "        \"dtype\": 'uint8'\n",
    "    })\n",
    "    \n",
    "    # Write the final stitched GeoTIFF\n",
    "    final_output_path = os.path.join(final_output_dir, f'{district_name}_Final_Flood_Mask.tif')\n",
    "    with rasterio.open(final_output_path, \"w\", **out_meta) as dest:\n",
    "        dest.write(stitched_array)\n",
    "        \n",
    "    # Close all source files\n",
    "    for src in sources:\n",
    "        src.close()\n",
    "        \n",
    "    print(f\"‚úÖ Final stitched mask saved to: {final_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e52637c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minor_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
